{
  "hash": "94774a1dc2fd221f701915662550ec3b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Support Vector Machines â€” optymalne granice decyzyjne\"\nformat:\n  html:\n    code-tools: true\n---\n\n## âš”ï¸ Czym jest Support Vector Machine (SVM)?\n\n**SVM** to algorytm, ktÃ³ry znajduje **optymalnÄ… granicÄ™ decyzyjnÄ…** miÄ™dzy klasami. Zamiast szukaÄ‡ \"jakiejkolwiek\" linii podziaÅ‚u, SVM znajduje tÄ…, ktÃ³ra **maksymalizuje margines** - odlegÅ‚oÅ›Ä‡ do najbliÅ¼szych punktÃ³w z kaÅ¼dej klasy.\n\n::: {.callout-note}\n## ğŸ’¡ Intuicja\nWyobraÅº sobie sÄ™dziego, ktÃ³ry musi wyznaczyÄ‡ granicÄ™ miÄ™dzy dwoma druÅ¼ynami. SVM nie tylko znajdzie granicÄ™, ale postawi jÄ… tak, Å¼eby byÅ‚a jak najdalej od najbliÅ¼szych zawodnikÃ³w z obu stron - dla maksymalnego bezpieczeÅ„stwa!\n:::\n\n---\n\n## ğŸ¯ Praktyczny przykÅ‚ad: klasyfikacja emaili spam/ham\n\nCzy email to spam czy prawdziwa wiadomoÅ›Ä‡?\n\n::: {#5bd700ef .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\n\n# Tworzenie realistycznych danych emaili\nnp.random.seed(42)\nn_emails = 2000\n\n# Generuj rÃ³Å¼ne typy emaili\nemail_types = np.random.choice(['spam', 'ham'], n_emails, p=[0.3, 0.7])\n\ndata = pd.DataFrame({\n    'dlugosc_tematu': np.random.randint(5, 100, n_emails),\n    'liczba_wielkich_liter': np.random.randint(0, 50, n_emails),\n    'liczba_wykrzyknikow': np.random.randint(0, 10, n_emails),\n    'liczba_linkow': np.random.randint(0, 20, n_emails),\n    'slowa_promocyjne': np.random.randint(0, 15, n_emails),\n    'dlugosc_wiadomosci': np.random.randint(50, 5000, n_emails),\n    'godzina_wyslania': np.random.randint(0, 24, n_emails),\n    'typ': email_types\n})\n\n# Realistyczne wzorce dla spam vs ham\nfor i, typ in enumerate(data['typ']):\n    if typ == 'spam':\n        # Spam charakterystyki\n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(20, 80)  # dÅ‚uÅ¼sze tematy\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(10, 50)  # DUÅ»O WIELKIMI\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(3, 10)  # wiÄ™cej !!!\n        data.loc[i, 'liczba_linkow'] = np.random.randint(5, 20)  # duÅ¼o linkÃ³w\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(5, 15)  # PROMOCJA, GRATIS\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(200, 1000)  # Å›rednie dÅ‚ugoÅ›ci\n        data.loc[i, 'godzina_wyslania'] = np.random.choice([2, 3, 22, 23, 1])  # dziwne godziny\n    else:\n        # Ham (prawdziwe) charakterystyki  \n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(5, 40)  # krÃ³tsze tematy\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(0, 8)  # normalne uÅ¼ywanie\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(0, 3)  # rzadko !\n        data.loc[i, 'liczba_linkow'] = np.random.randint(0, 3)  # maÅ‚o linkÃ³w\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(0, 2)  # prawie brak\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(50, 3000)  # rÃ³Å¼ne dÅ‚ugoÅ›ci\n        data.loc[i, 'godzina_wyslania'] = np.random.randint(8, 22)  # normalne godziny\n\n# Konwertuj target na binary\ndata['is_spam'] = (data['typ'] == 'spam').astype(int)\n\nprint(\"Statystyki emaili:\")\nprint(data.describe())\nprint(f\"\\nRozkÅ‚ad spam/ham:\")\nprint(data['typ'].value_counts())\nprint(f\"Procent spam: {data['is_spam'].mean():.1%}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## PokaÅ¼ statystyki emaili\n\n::: {#d3fc0cd7 .cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\nStatystyki emaili:\n       dlugosc_tematu  liczba_wielkich_liter  liczba_wykrzyknikow  \\\ncount     2000.000000            2000.000000           2000.00000   \nmean        30.482500              11.392500              2.60800   \nstd         18.078609              13.675451              2.69369   \nmin          5.000000               0.000000              0.00000   \n25%         17.000000               2.000000              1.00000   \n50%         27.000000               5.000000              2.00000   \n75%         38.000000              17.000000              4.00000   \nmax         79.000000              49.000000              9.00000   \n\n       liczba_linkow  slowa_promocyjne  dlugosc_wiadomosci  godzina_wyslania  \\\ncount    2000.000000       2000.000000         2000.000000       2000.000000   \nmean        4.360000          3.302500         1240.088000         13.031000   \nstd         5.642279          4.524183          834.469147          6.857445   \nmin         0.000000          0.000000           56.000000          1.000000   \n25%         1.000000          0.000000          539.000000          9.000000   \n50%         2.000000          1.000000          958.000000         14.000000   \n75%         7.000000          6.000000         1905.500000         19.000000   \nmax        19.000000         14.000000         2998.000000         23.000000   \n\n           is_spam  \ncount  2000.000000  \nmean      0.307500  \nstd       0.461574  \nmin       0.000000  \n25%       0.000000  \n50%       0.000000  \n75%       1.000000  \nmax       1.000000  \n\nRozkÅ‚ad spam/ham:\ntyp\nham     1385\nspam     615\nName: count, dtype: int64\nProcent spam: 30.8%\n```\n:::\n:::\n\n\n:::\n\n---\n\n## ğŸ”§ Budowanie modelu krok po kroku\n\n### 1) Przygotowanie danych\n\n::: {#cd5f6cdc .cell execution_count=3}\n``` {.python .cell-code}\n# Features do modelu\nfeatures = ['dlugosc_tematu', 'liczba_wielkich_liter', 'liczba_wykrzyknikow', \n           'liczba_linkow', 'slowa_promocyjne', 'dlugosc_wiadomosci', 'godzina_wyslania']\nX = data[features]\ny = data['is_spam']\n\n# PodziaÅ‚ train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standaryzacja - KLUCZOWA dla SVM!\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"Dane treningowe: {len(X_train)} emaili\")\nprint(f\"Dane testowe: {len(X_test)} emaili\")\nprint(f\"Spam w train: {y_train.mean():.1%}\")\nprint(f\"Spam w test: {y_test.mean():.1%}\")\n\nprint(\"\\nPrzykÅ‚ad standaryzacji:\")\nprint(\"Przed:\", X_train.iloc[0].values)\nprint(\"Po:\", X_train_scaled[0])\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## PokaÅ¼ podziaÅ‚ danych\n\n::: {#a180629e .cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\nDane treningowe: 1600 emaili\nDane testowe: 400 emaili\nSpam w train: 30.9%\nSpam w test: 30.0%\n\nPrzykÅ‚ad standaryzacji:\nPrzed: [   6    1    0    2    0 2057   21]\nPo: [-1.36651173 -0.75972497 -0.9773396  -0.41945283 -0.73519846  1.00688364\n  1.1681456 ]\n```\n:::\n:::\n\n\n:::\n\n### 2) Trenowanie rÃ³Å¼nych kerneli SVM\n\n::: {#58bff683 .cell execution_count=5}\n``` {.python .cell-code}\n# PorÃ³wnanie rÃ³Å¼nych kerneli\nkernels = ['linear', 'rbf', 'poly']\nsvm_results = {}\n\nfor kernel in kernels:\n    print(f\"\\nTrenowanie SVM z kernelem: {kernel}\")\n    \n    if kernel == 'poly':\n        svm = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm = SVC(kernel=kernel, random_state=42)\n    \n    # Trenowanie\n    svm.fit(X_train_scaled, y_train)\n    \n    # Predykcje\n    y_pred = svm.predict(X_test_scaled)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    svm_results[kernel] = {\n        'model': svm,\n        'accuracy': accuracy,\n        'predictions': y_pred\n    }\n    \n    print(f\"DokÅ‚adnoÅ›Ä‡: {accuracy:.1%}\")\n    print(f\"Liczba support vectors: {svm.n_support_}\")\n\n# ZnajdÅº najlepszy kernel\nbest_kernel = max(svm_results.keys(), key=lambda k: svm_results[k]['accuracy'])\nbest_model = svm_results[best_kernel]['model']\n\nprint(f\"\\nğŸ† Najlepszy kernel: {best_kernel}\")\nprint(f\"Najlepsza dokÅ‚adnoÅ›Ä‡: {svm_results[best_kernel]['accuracy']:.1%}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## PokaÅ¼ wyniki rÃ³Å¼nych kerneli\n\n::: {#4c4adb23 .cell execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nKernel linear:\nDokÅ‚adnoÅ›Ä‡: 100.0%\nLiczba support vectors: [2 3]\n\nKernel rbf:\nDokÅ‚adnoÅ›Ä‡: 100.0%\nLiczba support vectors: [11 26]\n\nKernel poly:\nDokÅ‚adnoÅ›Ä‡: 100.0%\nLiczba support vectors: [16 16]\n\nğŸ† Najlepszy kernel: linear\nNajlepsza dokÅ‚adnoÅ›Ä‡: 100.0%\n```\n:::\n:::\n\n\n:::\n\n### 3) Ewaluacja najlepszego modelu\n\n::: {#3e9500b8 .cell execution_count=7}\n``` {.python .cell-code}\n# SzczegÃ³Å‚owa analiza najlepszego modelu\nbest_predictions = svm_results[best_kernel]['predictions']\n\nprint(f\"SZCZEGÃ“ÅOWA ANALIZA - SVM {best_kernel.upper()}\")\nprint(\"=\" * 50)\n\n# Classification report\nprint(\"\\nRaport klasyfikacji:\")\nprint(classification_report(y_test, best_predictions, target_names=['Ham', 'Spam']))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, best_predictions)\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:     Ham    Spam\")\nprint(f\"Rzeczywiste Ham:  {cm[0,0]:3d}    {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Spam: {cm[1,0]:3d}    {cm[1,1]:3d}\")\n\n# Analiza bÅ‚Ä™dÃ³w\nfalse_positives = np.where((y_test == 0) & (best_predictions == 1))[0]\nfalse_negatives = np.where((y_test == 1) & (best_predictions == 0))[0]\n\nprint(f\"\\nAnaliza bÅ‚Ä™dÃ³w:\")\nprint(f\"False Positives (Ham â†’ Spam): {len(false_positives)}\")\nprint(f\"False Negatives (Spam â†’ Ham): {len(false_negatives)}\")\n\nif len(false_positives) > 0:\n    print(f\"\\nPrzykÅ‚ad bÅ‚Ä™dnie sklasyfikowanego Ham jako Spam:\")\n    fp_idx = false_positives[0]\n    print(f\"Email: {X_test.iloc[fp_idx].to_dict()}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## PokaÅ¼ szczegÃ³Å‚owÄ… analizÄ™\n\n::: {#8c4113a2 .cell execution_count=8}\n\n::: {.cell-output .cell-output-stdout}\n```\nSZCZEGÃ“ÅOWA ANALIZA - SVM LINEAR\n==================================================\n\nRaport klasyfikacji:\n              precision    recall  f1-score   support\n\n         Ham       1.00      1.00      1.00       280\n        Spam       1.00      1.00      1.00       120\n\n    accuracy                           1.00       400\n   macro avg       1.00      1.00      1.00       400\nweighted avg       1.00      1.00      1.00       400\n\n\nConfusion Matrix:\nPrzewidywane:     Ham    Spam\nRzeczywiste Ham:  280      0\nRzeczywiste Spam:   0    120\n\nAnaliza bÅ‚Ä™dÃ³w:\nFalse Positives (Ham â†’ Spam): 0\nFalse Negatives (Spam â†’ Ham): 0\n```\n:::\n:::\n\n\n:::\n\n---\n\n## ğŸ“Š Wizualizacja granic decyzyjnych\n\n::: {#74cc1b4c .cell execution_count=9}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Przygotuj dane (powtarzamy dla kompletnoÅ›ci)\nnp.random.seed(42)\nn_emails = 2000\n\nemail_types = np.random.choice(['spam', 'ham'], n_emails, p=[0.3, 0.7])\n\ndata = pd.DataFrame({\n    'dlugosc_tematu': np.random.randint(5, 100, n_emails),\n    'liczba_wielkich_liter': np.random.randint(0, 50, n_emails),\n    'liczba_wykrzyknikow': np.random.randint(0, 10, n_emails),\n    'liczba_linkow': np.random.randint(0, 20, n_emails),\n    'slowa_promocyjne': np.random.randint(0, 15, n_emails),\n    'dlugosc_wiadomosci': np.random.randint(50, 5000, n_emails),\n    'godzina_wyslania': np.random.randint(0, 24, n_emails),\n    'typ': email_types\n})\n\n# Popraw charakterystyki\nfor i, typ in enumerate(data['typ']):\n    if typ == 'spam':\n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(20, 80)\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(10, 50)\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(3, 10)\n        data.loc[i, 'liczba_linkow'] = np.random.randint(5, 20)\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(5, 15)\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(200, 1000)\n        data.loc[i, 'godzina_wyslania'] = np.random.choice([2, 3, 22, 23, 1])\n    else:\n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(5, 40)\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(0, 8)\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(0, 3)\n        data.loc[i, 'liczba_linkow'] = np.random.randint(0, 3)\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(0, 2)\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(50, 3000)\n        data.loc[i, 'godzina_wyslania'] = np.random.randint(8, 22)\n\ndata['is_spam'] = (data['typ'] == 'spam').astype(int)\n\n# Wybierz tylko 2 features dla wizualizacji 2D\nfeatures_2d = ['liczba_wielkich_liter', 'liczba_wykrzyknikow']\nX_2d = data[features_2d]\ny = data['is_spam']\n\nX_train_2d, X_test_2d, y_train, y_test = train_test_split(X_2d, y, test_size=0.2, random_state=42)\n\nscaler_2d = StandardScaler()\nX_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)\nX_test_2d_scaled = scaler_2d.transform(X_test_2d)\n\n# Trenuj modele z rÃ³Å¼nymi kernelami (na 2D)\nkernels = ['linear', 'rbf', 'poly']\nplt.figure(figsize=(15, 5))\n\nfor i, kernel in enumerate(kernels):\n    plt.subplot(1, 3, i+1)\n    \n    # Trenuj model\n    if kernel == 'poly':\n        svm = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm = SVC(kernel=kernel, random_state=42)\n    \n    svm.fit(X_train_2d_scaled, y_train)\n    \n    # StwÃ³rz mesh do wizualizacji granic\n    h = 0.02\n    x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1\n    y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    # Predykcje na mesh\n    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    # Narysuj granice i punkty\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    scatter = plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], \n                         c=y_train, cmap=plt.cm.RdYlBu, edgecolors='black')\n    \n    # PodÅ›wietl support vectors\n    if hasattr(svm, 'support_'):\n        plt.scatter(X_train_2d_scaled[svm.support_, 0], \n                   X_train_2d_scaled[svm.support_, 1],\n                   s=100, facecolors='none', edgecolors='black', linewidth=2)\n    \n    plt.xlabel('Liczba wielkich liter (scaled)')\n    plt.ylabel('Liczba wykrzyknikÃ³w (scaled)')\n    plt.title(f'SVM - {kernel.upper()} kernel')\n    \n    # DokÅ‚adnoÅ›Ä‡\n    accuracy = accuracy_score(y_test, svm.predict(X_test_2d_scaled))\n    plt.text(0.02, 0.98, f'Accuracy: {accuracy:.1%}', \n             transform=plt.gca().transAxes, \n             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n             verticalalignment='top')\n\nplt.tight_layout()\nplt.close()\n\n# Przygotuj dane dla peÅ‚nego modelu\nfeatures = ['dlugosc_tematu', 'liczba_wielkich_liter', 'liczba_wykrzyknikow', \n           'liczba_linkow', 'slowa_promocyjne', 'dlugosc_wiadomosci', 'godzina_wyslania']\nX_full = data[features]\nX_train_full, X_test_full, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42)\n\nscaler_full = StandardScaler()\nX_train_full_scaled = scaler_full.fit_transform(X_train_full)\nX_test_full_scaled = scaler_full.transform(X_test_full)\n\n# PorÃ³wnaj wyniki\nprint(\"PORÃ“WNANIE KERNELI (2D vs 7D features):\")\nprint(\"=\" * 50)\n\nfor kernel in kernels:\n    # 2D model\n    if kernel == 'poly':\n        svm_2d = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm_2d = SVC(kernel=kernel, random_state=42)\n    svm_2d.fit(X_train_2d_scaled, y_train)\n    acc_2d = accuracy_score(y_test, svm_2d.predict(X_test_2d_scaled))\n    \n    # 7D model\n    if kernel == 'poly':\n        svm_7d = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm_7d = SVC(kernel=kernel, random_state=42)\n    svm_7d.fit(X_train_full_scaled, y_train)\n    acc_7d = accuracy_score(y_test, svm_7d.predict(X_test_full_scaled))\n    \n    print(f\"{kernel.upper():6} - 2D: {acc_2d:.1%}, 7D: {acc_7d:.1%}, Poprawa: +{(acc_7d-acc_2d)*100:.1f}pp\")\n\n# OdtwÃ³rz wizualizacjÄ™\nplt.figure(figsize=(15, 5))\n\nfor i, kernel in enumerate(kernels):\n    plt.subplot(1, 3, i+1)\n    \n    if kernel == 'poly':\n        svm = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm = SVC(kernel=kernel, random_state=42)\n    \n    svm.fit(X_train_2d_scaled, y_train)\n    \n    h = 0.02\n    x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1\n    y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    \n    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    scatter = plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], \n                         c=y_train, cmap=plt.cm.RdYlBu, edgecolors='black')\n    \n    if hasattr(svm, 'support_'):\n        plt.scatter(X_train_2d_scaled[svm.support_, 0], \n                   X_train_2d_scaled[svm.support_, 1],\n                   s=100, facecolors='none', edgecolors='black', linewidth=2)\n    \n    plt.xlabel('Liczba wielkich liter (scaled)')\n    plt.ylabel('Liczba wykrzyknikÃ³w (scaled)')\n    plt.title(f'SVM - {kernel.upper()} kernel')\n    \n    accuracy = accuracy_score(y_test, svm.predict(X_test_2d_scaled))\n    plt.text(0.02, 0.98, f'Accuracy: {accuracy:.1%}', \n             transform=plt.gca().transAxes, \n             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n             verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## PokaÅ¼ wizualizacjÄ™ granic i porÃ³wnanie\n\n::: {#12d25932 .cell fig-height='5' fig-width='15' execution_count=10}\n\n::: {.cell-output .cell-output-stdout}\n```\nPORÃ“WNANIE KERNELI (2D vs 7D features):\n==================================================\nLINEAR - 2D: 100.0%, 7D: 100.0%, Poprawa: +0.0pp\nRBF    - 2D: 100.0%, 7D: 100.0%, Poprawa: +0.0pp\nPOLY   - 2D: 98.2%, 7D: 100.0%, Poprawa: +1.7pp\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Wizualizacja granic decyzyjnych SVM dla rÃ³Å¼nych kerneli](06-svm_files/figure-html/cell-11-output-2.png){width=1431 height=470}\n:::\n:::\n\n\n:::\n\n---\n\n## ğŸ¯ Rodzaje kerneli i ich zastosowania\n\n### 1) **Linear Kernel - proste granice**\n\n::: {#99efbdaa .cell execution_count=11}\n``` {.python .cell-code}\n# UÅ¼yj gdy:\n# - Dane sÄ… liniowo separowalne\n# - Masz duÅ¼o features (high-dimensional)\n# - Potrzebujesz szybkiego modelu\n# - Chcesz interpretowalnoÅ›Ä‡\n\nlinear_use_cases = [\n    \"Text classification (high-dimensional sparse data)\",\n    \"Gene expression analysis\", \n    \"Document categorization\",\n    \"Sentiment analysis z bag-of-words\"\n]\n\nprint(\"LINEAR KERNEL - najlepsze zastosowania:\")\nfor i, use_case in enumerate(linear_use_cases, 1):\n    print(f\"{i}. {use_case}\")\n\n# Demo: Linear SVM na prostych danych\nX_simple = np.random.randn(100, 2)\ny_simple = (X_simple[:, 0] + X_simple[:, 1] > 0).astype(int)\n\nlinear_svm = SVC(kernel='linear')\nlinear_svm.fit(X_simple, y_simple)\nprint(f\"\\nLinear SVM na prostych danych: {linear_svm.score(X_simple, y_simple):.1%}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## PokaÅ¼ zastosowania Linear Kernel\n\n::: {#f4cf2465 .cell execution_count=12}\n\n::: {.cell-output .cell-output-stdout}\n```\nLINEAR KERNEL - najlepsze zastosowania:\n1. Text classification (high-dimensional sparse data)\n2. Gene expression analysis\n3. Document categorization\n4. Sentiment analysis z bag-of-words\n\nLinear SVM na prostych danych: 98.0%\n```\n:::\n:::\n\n\n:::\n\n### 2) **RBF Kernel - uniwersalny wybÃ³r**\n\n::: {#9e317c34 .cell execution_count=13}\n``` {.python .cell-code}\n# RBF (Radial Basis Function) - najczÄ™Å›ciej uÅ¼ywany\n# MoÅ¼e aproksymowaÄ‡ dowolnÄ… granicÄ™ decyzyjnÄ…!\n\nrbf_use_cases = [\n    \"Image classification\",\n    \"Medical diagnosis\", \n    \"Fraud detection\",\n    \"Customer segmentation\",\n    \"General classification tasks\"\n]\n\nprint(\"RBF KERNEL - najlepsze zastosowania:\")\nfor i, use_case in enumerate(rbf_use_cases, 1):\n    print(f\"{i}. {use_case}\")\n\n# Demo: wpÅ‚yw parametru gamma\ngammas = [0.1, 1.0, 10.0]\nprint(f\"\\nWpÅ‚yw parametru gamma w RBF:\")\n\nfor gamma in gammas:\n    rbf_svm = SVC(kernel='rbf', gamma=gamma, random_state=42)\n    rbf_svm.fit(X_train_full_scaled, y_train)\n    accuracy = rbf_svm.score(X_test_full_scaled, y_test)\n    print(f\"Gamma {gamma:4.1f}: Accuracy = {accuracy:.1%}, Support Vectors = {rbf_svm.n_support_.sum()}\")\n\nprint(\"\\nğŸ’¡ Gamma wysoka = bardziej skomplikowane granice\")\nprint(\"ğŸ’¡ Gamma niska = gÅ‚adsze granice\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## PokaÅ¼ zastosowania RBF Kernel\n\n::: {#a8b97a9e .cell execution_count=14}\n\n::: {.cell-output .cell-output-stdout}\n```\nRBF KERNEL - najlepsze zastosowania:\n1. Image classification\n2. Medical diagnosis\n3. Fraud detection\n4. Customer segmentation\n5. General classification tasks\n\nWpÅ‚yw parametru gamma w RBF:\nGamma  0.1: Accuracy = 100.0%, Support Vectors = 26\nGamma  1.0: Accuracy = 100.0%, Support Vectors = 374\nGamma 10.0: Accuracy = 100.0%, Support Vectors = 1546\n\nğŸ’¡ Gamma wysoka = bardziej skomplikowane granice\nğŸ’¡ Gamma niska = gÅ‚adsze granice\n```\n:::\n:::\n\n\n:::\n\n### 3) **Polynomial Kernel - dla zÅ‚oÅ¼onych wzorcÃ³w**\n\n::: {#b6828a55 .cell execution_count=15}\n``` {.python .cell-code}\npoly_use_cases = [\n    \"Computer vision (shape recognition)\",\n    \"Bioinformatics (protein classification)\",\n    \"Natural language processing\",\n    \"Pattern recognition\"\n]\n\nprint(\"POLYNOMIAL KERNEL - najlepsze zastosowania:\")\nfor i, use_case in enumerate(poly_use_cases, 1):\n    print(f\"{i}. {use_case}\")\n\n# Demo: wpÅ‚yw stopnia wielomianu\ndegrees = [2, 3, 4, 5]\nprint(f\"\\nWpÅ‚yw stopnia wielomianu:\")\n\nfor degree in degrees:\n    poly_svm = SVC(kernel='poly', degree=degree, random_state=42)\n    poly_svm.fit(X_train_full_scaled, y_train)\n    accuracy = poly_svm.score(X_test_full_scaled, y_test)\n    print(f\"Degree {degree}: Accuracy = {accuracy:.1%}\")\n\nprint(\"\\nâš ï¸ Uwaga: wyÅ¼szie stopnie = ryzyko overfittingu!\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## PokaÅ¼ zastosowania Polynomial Kernel\n\n::: {#31520760 .cell execution_count=16}\n\n::: {.cell-output .cell-output-stdout}\n```\nPOLYNOMIAL KERNEL - najlepsze zastosowania:\n1. Computer vision (shape recognition)\n2. Bioinformatics (protein classification)\n3. Natural language processing\n4. Pattern recognition\n\nWpÅ‚yw stopnia wielomianu:\nDegree 2: Accuracy = 99.8%\nDegree 3: Accuracy = 100.0%\nDegree 4: Accuracy = 99.5%\nDegree 5: Accuracy = 100.0%\n\nâš ï¸ Uwaga: wyÅ¼szie stopnie = ryzyko overfittingu!\n```\n:::\n:::\n\n\n:::\n\n---\n\n## âš™ï¸ Tuning parametrÃ³w SVM\n\n::: {#13f9004d .cell execution_count=17}\n``` {.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\n\n# Grid Search dla RBF kernel\nparam_grid_rbf = {\n    'C': [0.1, 1, 10, 100],           # regularization strength\n    'gamma': [0.001, 0.01, 0.1, 1]   # kernel coefficient\n}\n\nprint(\"Rozpoczynam Grid Search dla RBF SVM...\")\ngrid_search = GridSearchCV(\n    SVC(kernel='rbf', random_state=42),\n    param_grid_rbf,\n    cv=3,  # 3-fold CV dla szybkoÅ›ci\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=0\n)\n\ngrid_search.fit(X_train_full_scaled, y_train)\n\nprint(\"Najlepsze parametry RBF:\")\nprint(grid_search.best_params_)\nprint(f\"Najlepsza dokÅ‚adnoÅ›Ä‡ CV: {grid_search.best_score_:.1%}\")\n\n# Test na zbiorze testowym\nbest_svm = grid_search.best_estimator_\ntest_accuracy = best_svm.score(X_test_full_scaled, y_test)\nprint(f\"DokÅ‚adnoÅ›Ä‡ na test set: {test_accuracy:.1%}\")\n\n# PorÃ³wnanie przed/po tuningu\nbaseline_svm = SVC(kernel='rbf', random_state=42)\nbaseline_svm.fit(X_train_full_scaled, y_train)\nbaseline_accuracy = baseline_svm.score(X_test_full_scaled, y_test)\n\nprint(f\"\\nPorÃ³wnanie:\")\nprint(f\"Baseline RBF:      {baseline_accuracy:.1%}\")\nprint(f\"Tuned RBF:         {test_accuracy:.1%}\")\nprint(f\"Poprawa:           +{(test_accuracy - baseline_accuracy)*100:.1f}pp\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## PokaÅ¼ wyniki tuningu\n\n::: {#5e878e48 .cell execution_count=18}\n\n::: {.cell-output .cell-output-stdout}\n```\nNajlepsze parametry RBF:\n{'C': 0.1, 'gamma': 0.01}\nNajlepsza dokÅ‚adnoÅ›Ä‡ CV: 100.0%\nDokÅ‚adnoÅ›Ä‡ na test set: 100.0%\n\nPorÃ³wnanie:\nBaseline RBF:      100.0%\nTuned RBF:         100.0%\nPoprawa:           +0.0pp\n```\n:::\n:::\n\n\n:::\n\n---\n\n## âš ï¸ PuÅ‚apki i rozwiÄ…zania\n\n### 1) **Brak standaryzacji**\n\n::: {#6a7be9f7 .cell execution_count=19}\n``` {.python .cell-code}\n# Problem: SVM jest bardzo wraÅ¼liwy na skale danych\nprint(\"DEMONSTRACJA: wpÅ‚yw standaryzacji\")\n\n# Bez standaryzacji\nsvm_unscaled = SVC(kernel='rbf', random_state=42)\nsvm_unscaled.fit(X_train_full, y_train)  # raw data!\nacc_unscaled = svm_unscaled.score(X_test_full, y_test)\n\n# Ze standaryzacjÄ…\nsvm_scaled = SVC(kernel='rbf', random_state=42)\nsvm_scaled.fit(X_train_full_scaled, y_train)  # scaled data!\nacc_scaled = svm_scaled.score(X_test_full_scaled, y_test)\n\nprint(f\"Bez standaryzacji: {acc_unscaled:.1%}\")\nprint(f\"Ze standaryzacjÄ…:  {acc_scaled:.1%}\")\nprint(f\"Poprawa:           +{(acc_scaled - acc_unscaled)*100:.1f}pp\")\nprint(\"\\nâœ… ZAWSZE standaryzuj dane przed SVM!\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## PokaÅ¼ wpÅ‚yw standaryzacji\n\n::: {#c6e220c6 .cell execution_count=20}\n\n::: {.cell-output .cell-output-stdout}\n```\nBez standaryzacji: 88.0%\nZe standaryzacjÄ…:  100.0%\nPoprawa:           +12.0pp\n\nâœ… ZAWSZE standaryzuj dane przed SVM!\n```\n:::\n:::\n\n\n:::\n\n### 2) **Niezbalansowane klasy**\n\n::: {#bd677440 .cell execution_count=21}\n``` {.python .cell-code}\n# Problem: gdy jedna klasa jest bardzo rzadka\nprint(\"PROBLEM: niezbalansowane klasy\")\n\n# SprawdÅº balans w naszych danych\nprint(f\"Balans klas: Spam {y_train.mean():.1%}, Ham {(1-y_train.mean()):.1%}\")\n\n# RozwiÄ…zanie 1: class_weight='balanced'\nsvm_balanced = SVC(kernel='rbf', class_weight='balanced', random_state=42)\nsvm_balanced.fit(X_train_full_scaled, y_train)\n\n# RozwiÄ…zanie 2: manual class weights\nspam_weight = len(y_train) / (2 * y_train.sum())  # 1/frequency\nham_weight = len(y_train) / (2 * (len(y_train) - y_train.sum()))\nclass_weights = {0: ham_weight, 1: spam_weight}\n\nsvm_manual = SVC(kernel='rbf', class_weight=class_weights, random_state=42)\nsvm_manual.fit(X_train_full_scaled, y_train)\n\n# PorÃ³wnanie\nsvm_normal = SVC(kernel='rbf', random_state=42)\nsvm_normal.fit(X_train_full_scaled, y_train)\n\nmodels = {\n    'Normal': svm_normal,\n    'Balanced': svm_balanced, \n    'Manual weights': svm_manual\n}\n\nprint(\"\\nPorÃ³wnanie podejÅ›Ä‡:\")\nfor name, model in models.items():\n    pred = model.predict(X_test_full_scaled)\n    accuracy = accuracy_score(y_test, pred)\n    \n    # SprawdÅº recall dla spam (waÅ¼ne!)\n    spam_indices = y_test == 1\n    spam_recall = np.mean(pred[spam_indices] == 1) if spam_indices.sum() > 0 else 0\n    \n    print(f\"{name:12}: Accuracy={accuracy:.1%}, Spam Recall={spam_recall:.1%}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## PokaÅ¼ rozwiÄ…zania dla niezbalansowanych klas\n\n::: {#3db49554 .cell execution_count=22}\n\n::: {.cell-output .cell-output-stdout}\n```\nBalans klas: Spam 30.9%, Ham 69.1%\n\nPorÃ³wnanie podejÅ›Ä‡:\nNormal      : Accuracy=100.0%, Spam Recall=100.0%\nBalanced    : Accuracy=100.0%, Spam Recall=100.0%\nManual weights: Accuracy=100.0%, Spam Recall=100.0%\n```\n:::\n:::\n\n\n:::\n\n### 3) **Overfitting z wysokim C**\n\n::: {#3c4a4c07 .cell execution_count=23}\n``` {.python .cell-code}\n# Problem: zbyt wysoka wartoÅ›Ä‡ C prowadzi do overfittingu\nC_values = [0.01, 0.1, 1, 10, 100, 1000]\nresults = []\n\nprint(\"WpÅ‚yw parametru C (regularization):\")\nprint(\"C      | Train Acc | Test Acc | Difference | Support Vectors\")\nprint(\"-\" * 60)\n\nfor C in C_values:\n    svm = SVC(kernel='rbf', C=C, random_state=42)\n    svm.fit(X_train_full_scaled, y_train)\n    \n    train_acc = svm.score(X_train_full_scaled, y_train)\n    test_acc = svm.score(X_test_full_scaled, y_test)\n    diff = train_acc - test_acc\n    n_sv = svm.n_support_.sum()\n    \n    print(f\"{C:6.2f} | {train_acc:8.1%} | {test_acc:7.1%} | {diff:9.1%} | {n_sv:14d}\")\n    results.append((C, train_acc, test_acc, diff, n_sv))\n\nprint(\"\\nğŸ’¡ Optimalne C: wysokie test accuracy, maÅ‚a rÃ³Å¼nica train-test\")\nprint(\"ğŸ’¡ Zbyt wysokie C: perfect train accuracy, sÅ‚abe test accuracy\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## PokaÅ¼ wpÅ‚yw parametru C\n\n::: {#16ed1524 .cell execution_count=24}\n\n::: {.cell-output .cell-output-stdout}\n```\nWpÅ‚yw parametru C (regularization):\nC      | Train Acc | Test Acc | Difference | Support Vectors\n------------------------------------------------------------\n  0.01 |   100.0% |  100.0% |      0.0% |            582\n  0.10 |   100.0% |  100.0% |      0.0% |             99\n  1.00 |   100.0% |  100.0% |      0.0% |             37\n 10.00 |   100.0% |  100.0% |      0.0% |             35\n100.00 |   100.0% |  100.0% |      0.0% |             35\n1000.00 |   100.0% |  100.0% |      0.0% |             35\n\nğŸ’¡ Optimalne C: wysokie test accuracy, maÅ‚a rÃ³Å¼nica train-test\nğŸ’¡ Zbyt wysokie C: perfect train accuracy, sÅ‚abe test accuracy\n```\n:::\n:::\n\n\n:::\n\n---\n\n## ğŸŒ Real-world przypadki uÅ¼ycia\n\n1. **Text Classification:** Spam detection, sentiment analysis, document categorization\n2. **Computer Vision:** Image classification, face recognition, medical image analysis\n3. **Finance:** Credit scoring, algorithmic trading, fraud detection\n4. **Healthcare:** Disease diagnosis, drug discovery, medical image analysis\n5. **Cybersecurity:** Intrusion detection, malware classification, network security\n\n::: {.callout-tip}\n## ğŸ’¡ Kiedy uÅ¼ywaÄ‡ SVM?\n\n**âœ… UÅ»YJ GDY:**\n\n- Potrzebujesz wysokiej dokÅ‚adnoÅ›ci klasyfikacji\n- Masz Å›redniej wielkoÅ›ci dataset (1K-100K samples)\n- Dane majÄ… clear margins miÄ™dzy klasami\n- Nie potrzebujesz probabilistic outputs\n- Memory efficiency jest waÅ¼na\n\n**âŒ NIE UÅ»YWAJ GDY:**\n\n- Masz bardzo duÅ¼e datasety (>100K samples - uÅ¼yj Neural Networks)\n- Potrzebujesz probabilistic predictions (uÅ¼yj Logistic Regression)\n- Dane majÄ… duÅ¼o noise'u (uÅ¼yj Random Forest)\n- Target jest continuous (uÅ¼yj SVR - Support Vector Regression)\n- Potrzebujesz bardzo szybkiego inference (uÅ¼yj Decision Tree)\n:::\n\n**Podsumowanie:** SVM to potÄ™Å¼ny algoritm do klasyfikacji z optymalnÄ… separacjÄ… klas. Wybierz kernel w zaleÅ¼noÅ›ci od zÅ‚oÅ¼onoÅ›ci danych: Linear dla prostych, RBF dla uniwersalnych, Polynomial dla specjalnych przypadkÃ³w. ğŸ¯\n\n",
    "supporting": [
      "06-svm_files"
    ],
    "filters": [],
    "includes": {}
  }
}