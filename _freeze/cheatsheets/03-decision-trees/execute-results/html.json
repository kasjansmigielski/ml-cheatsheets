{
  "hash": "6192ab8f822a53a73e8678342e0c2d93",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Decision Trees ‚Äî proste drzewa decyzyjne\"\nformat:\n  html:\n    code-tools: true\n---\n\n## üå≥ Czym sƒÖ Decision Trees?\n\n**Decision Trees** to jeden z najbardziej intuicyjnych algorytm√≥w ML, kt√≥ry **podejmuje decyzje jak cz≈Çowiek** - zadajƒÖc szereg pyta≈Ñ typu \"tak/nie\" i na podstawie odpowiedzi klasyfikuje lub przewiduje warto≈õci.\n\n::: {.callout-note}\n## üí° Intuicja\nWyobra≈∫ sobie lekarza, kt√≥ry diagnozuje chorobƒô: \"Czy ma gorƒÖczkƒô? TAK ‚Üí Czy boli gard≈Ço? TAK ‚Üí Czy ma katar? NIE ‚Üí Prawdopodobnie angina\". Decision Tree dzia≈Ça dok≈Çadnie tak samo!\n:::\n\n---\n\n## üéØ Praktyczny przyk≈Çad: klasyfikacja klient√≥w banku\n\nCzy klient we≈∫mie kredyt na podstawie jego profilu?\n\n::: {#477af6a1 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Tworzenie realistycznych danych klient√≥w banku\nnp.random.seed(42)\nn_clients = 2000\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_clients),\n    'dochod': np.random.lognormal(10, 0.5, n_clients),  # log-normal dla dochod√≥w\n    'historia_kredytowa': np.random.choice(['dobra', '≈õrednia', 's≈Çaba'], n_clients, p=[0.6, 0.3, 0.1]),\n    'zatrudnienie': np.random.choice(['sta≈Çe', 'tymczasowe', 'bezrobotny'], n_clients, p=[0.7, 0.25, 0.05]),\n    'oszczednosci': np.random.exponential(20000, n_clients),\n    'liczba_dzieci': np.random.randint(0, 4, n_clients)\n})\n\n# Realistyczna logika decyzyjna banku\ndef czy_kredyt(row):\n    score = 0\n    \n    # Wiek (30-50 lat = najlepsi klienci)\n    if 30 <= row['wiek'] <= 50:\n        score += 2\n    elif row['wiek'] < 25 or row['wiek'] > 60:\n        score -= 1\n    \n    # Doch√≥d\n    if row['dochod'] > 50000:\n        score += 3\n    elif row['dochod'] > 30000:\n        score += 1\n    else:\n        score -= 2\n    \n    # Historia kredytowa\n    if row['historia_kredytowa'] == 'dobra':\n        score += 2\n    elif row['historia_kredytowa'] == 's≈Çaba':\n        score -= 3\n    \n    # Zatrudnienie\n    if row['zatrudnienie'] == 'sta≈Çe':\n        score += 2\n    elif row['zatrudnienie'] == 'bezrobotny':\n        score -= 4\n    \n    # Oszczƒôdno≈õci\n    if row['oszczednosci'] > 50000:\n        score += 1\n    \n    # Dzieci (wiƒôcej dzieci = wiƒôksze ryzyko)\n    score -= row['liczba_dzieci'] * 0.5\n    \n    # Ko≈Ñcowa decyzja z odrobinƒÖ losowo≈õci\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 0.5)))\n    return probability > 0.5\n\ndata['kredyt_przyznany'] = data.apply(czy_kredyt, axis=1)\n\nprint(\"Statystyki klient√≥w:\")\nprint(data.describe())\nprint(f\"\\nOdsetek przyznanych kredyt√≥w: {data['kredyt_przyznany'].mean():.1%}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º statystyki i odsetki przyznanych kredyt√≥w\n\n::: {#8d40fd05 .cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\nStatystyki klient√≥w:\n              wiek         dochod   oszczednosci  liczba_dzieci\ncount  2000.000000    2000.000000    2000.000000    2000.000000\nmean     43.805500   25726.361013   19950.270464       1.523000\nstd      14.929203   14090.406882   20299.940268       1.130535\nmin      18.000000    4047.221838      27.128881       0.000000\n25%      31.000000   15922.761710    5923.833818       1.000000\n50%      44.000000   22782.391426   13844.909016       2.000000\n75%      56.000000   31922.003675   27566.091135       3.000000\nmax      69.000000  112447.929023  165194.684774       3.000000\n\nOdsetek przyznanych kredyt√≥w: 63.8%\n```\n:::\n:::\n\n\n:::\n\n---\n\n## üîß Budowanie modelu krok po kroku\n\n### 1) Przygotowanie danych\n\n::: {#ea573bd9 .cell execution_count=3}\n``` {.python .cell-code}\n# Encoding kategorycznych zmiennych\nfrom sklearn.preprocessing import LabelEncoder\n\ndata_encoded = data.copy()\nle_historia = LabelEncoder()\nle_zatrudnienie = LabelEncoder()\n\ndata_encoded['historia_kredytowa_num'] = le_historia.fit_transform(data['historia_kredytowa'])\ndata_encoded['zatrudnienie_num'] = le_zatrudnienie.fit_transform(data['zatrudnienie'])\n\n# Features do modelu\nfeature_columns = ['wiek', 'dochod', 'historia_kredytowa_num', 'zatrudnienie_num', 'oszczednosci', 'liczba_dzieci']\nX = data_encoded[feature_columns]\ny = data_encoded['kredyt_przyznany']\n\n# Podzia≈Ç train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Dane treningowe: {len(X_train)} klient√≥w\")\nprint(f\"Dane testowe: {len(X_test)} klient√≥w\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki dane treningowe i testowe\n\n::: {#764c66a0 .cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\nDane treningowe: 1600 klient√≥w\nDane testowe: 400 klient√≥w\n```\n:::\n:::\n\n\n:::\n\n### 2) Trenowanie Decision Tree\n\n::: {#38b23ee9 .cell execution_count=5}\n``` {.python .cell-code}\n# Tworzenie modelu z ograniczeniami (≈ºeby nie by≈Ç za g≈Çƒôboki)\ndt_model = DecisionTreeClassifier(\n    max_depth=5,        # maksymalna g≈Çƒôboko≈õƒá drzewa\n    min_samples_split=50,  # min. pr√≥bek do podzia≈Çu wƒôz≈Ça\n    min_samples_leaf=20,   # min. pr√≥bek w li≈õciu\n    random_state=42\n)\n\n# Trenowanie\ndt_model.fit(X_train, y_train)\n\nprint(\"Model wytrenowany!\")\nprint(f\"G≈Çƒôboko≈õƒá drzewa: {dt_model.tree_.max_depth}\")\nprint(f\"Liczba li≈õci: {dt_model.tree_.n_leaves}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º parametry wytrenowanego modelu\n\n::: {#3fd61c60 .cell execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\nModel wytrenowany!\nG≈Çƒôboko≈õƒá drzewa: 5\nLiczba li≈õci: 20\n```\n:::\n:::\n\n\n:::\n\n### 3) Ewaluacja modelu\n\n::: {#0a6b2ae0 .cell execution_count=7}\n``` {.python .cell-code}\n# Predykcje\ny_pred = dt_model.predict(X_test)\ny_pred_proba = dt_model.predict_proba(X_test)[:, 1]\n\n# Metryki\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nDok≈Çadno≈õƒá modelu: {accuracy:.1%}\")\n\n# Szczeg√≥≈Çowy raport\nprint(\"\\nRaport klasyfikacji:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:    Nie    Tak\")\nprint(f\"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki\n\n::: {#92116d1d .cell execution_count=8}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nConfusion Matrix:\nPrzewidywane:    Nie    Tak\nRzeczywiste Nie: 122    34\nRzeczywiste Tak:  27   217\n```\n:::\n:::\n\n\n:::\n\n---\n\n## üé® Wizualizacja drzewa decyzyjnego\n\n::: {#557fb42a .cell execution_count=9}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Tworzenie przyk≈Çadowych danych (kompletny przyk≈Çad)\nnp.random.seed(42)\nn_clients = 2000\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_clients),\n    'dochod': np.random.lognormal(10, 0.5, n_clients),\n    'historia_kredytowa': np.random.choice(['dobra', '≈õrednia', 's≈Çaba'], n_clients, p=[0.6, 0.3, 0.1]),\n    'zatrudnienie': np.random.choice(['sta≈Çe', 'tymczasowe', 'bezrobotny'], n_clients, p=[0.7, 0.25, 0.05]),\n    'oszczednosci': np.random.exponential(20000, n_clients),\n    'liczba_dzieci': np.random.randint(0, 4, n_clients)\n})\n\n# Prosta logika przyznawania kredytu\ndef czy_kredyt(row):\n    score = 0\n    if 30 <= row['wiek'] <= 50:\n        score += 2\n    if row['dochod'] > 50000:\n        score += 3\n    if row['historia_kredytowa'] == 'dobra':\n        score += 2\n    if row['zatrudnienie'] == 'sta≈Çe':\n        score += 2\n    if row['oszczednosci'] > 50000:\n        score += 1\n    score -= row['liczba_dzieci'] * 0.5\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 0.5)))\n    return probability > 0.5\n\ndata['kredyt_przyznany'] = data.apply(czy_kredyt, axis=1)\n\n# Encoding i model\ndata_encoded = data.copy()\nle_historia = LabelEncoder()\nle_zatrudnienie = LabelEncoder()\ndata_encoded['historia_kredytowa_num'] = le_historia.fit_transform(data['historia_kredytowa'])\ndata_encoded['zatrudnienie_num'] = le_zatrudnienie.fit_transform(data['zatrudnienie'])\n\nfeature_columns = ['wiek', 'dochod', 'historia_kredytowa_num', 'zatrudnienie_num', 'oszczednosci', 'liczba_dzieci']\nX = data_encoded[feature_columns]\ny = data_encoded['kredyt_przyznany']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie drzewa\ndt_model = DecisionTreeClassifier(max_depth=5, min_samples_split=50, min_samples_leaf=20, random_state=42)\ndt_model.fit(X_train, y_train)\n\n# Ewaluacja\ny_pred = dt_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\n# Wa≈ºno≈õƒá features\nfeature_names = ['wiek', 'doch√≥d', 'historia_kred.', 'zatrudnienie', 'oszczƒôdno≈õci', 'liczba_dzieci']\nimportance = dt_model.feature_importances_\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importance\n}).sort_values('importance', ascending=False)\n\n# Przygotuj wykres drzewa\nplt.figure(figsize=(20, 10))\ntree.plot_tree(dt_model, \n               feature_names=feature_names,\n               class_names=['Nie', 'Tak'],\n               filled=True,\n               rounded=True,\n               fontsize=10)\nplt.title(\"Drzewo decyzyjne - Przyznanie kredytu\")\nplt.close()\n\nprint(f\"Wyniki modelu Decision Tree:\")\nprint(f\"G≈Çƒôboko≈õƒá drzewa: {dt_model.tree_.max_depth}\")\nprint(f\"Liczba li≈õci: {dt_model.tree_.n_leaves}\")\nprint(f\"Dok≈Çadno≈õƒá modelu: {accuracy:.1%}\")\n\nprint(f\"\\nStatystyki danych:\")\nprint(f\"Dane treningowe: {len(X_train)} klient√≥w\")\nprint(f\"Dane testowe: {len(X_test)} klient√≥w\")\nprint(f\"Odsetek przyznanych kredyt√≥w: {data['kredyt_przyznany'].mean():.1%}\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:    Nie    Tak\")\nprint(f\"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}\")\n\nprint(\"\\nWa≈ºno≈õƒá cech:\")\nfor _, row in feature_importance.iterrows():\n    print(f\"{row['feature']}: {row['importance']:.3f}\")\n\n# Odtw√≥rz wykres drzewa\nplt.figure(figsize=(20, 10))\ntree.plot_tree(dt_model, \n               feature_names=feature_names,\n               class_names=['Nie', 'Tak'],\n               filled=True,\n               rounded=True,\n               fontsize=10)\nplt.title(\"Drzewo decyzyjne - Przyznanie kredytu\")\nplt.show()\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki i drzewo decyzyjne\n\n::: {#8b7c00be .cell fig-height='10' fig-width='20' execution_count=10}\n\n::: {.cell-output .cell-output-stdout}\n```\nWyniki modelu Decision Tree:\nG≈Çƒôboko≈õƒá drzewa: 5\nLiczba li≈õci: 13\nDok≈Çadno≈õƒá modelu: 95.0%\n\nStatystyki danych:\nDane treningowe: 1600 klient√≥w\nDane testowe: 400 klient√≥w\nOdsetek przyznanych kredyt√≥w: 93.2%\n\nConfusion Matrix:\nPrzewidywane:    Nie    Tak\nRzeczywiste Nie:  19    11\nRzeczywiste Tak:   9   361\n\nWa≈ºno≈õƒá cech:\nzatrudnienie: 0.441\nwiek: 0.317\nhistoria_kred.: 0.178\nliczba_dzieci: 0.055\noszczƒôdno≈õci: 0.008\ndoch√≥d: 0.001\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Wizualizacja drzewa decyzyjnego dla kredyt√≥w](03-decision-trees_files/figure-html/cell-11-output-2.png){width=1507 height=778}\n:::\n:::\n\n\n:::\n\n---\n\n## üîç Interpretacja decyzji dla konkretnego klienta\n\n::: {#66482235 .cell execution_count=11}\n``` {.python .cell-code}\n# Funkcja do interpretacji ≈õcie≈ºki decyzyjnej\ndef explain_decision(model, X_sample, feature_names):\n    # Pobierz ≈õcie≈ºkƒô w drzewie\n    leaf_id = model.decision_path(X_sample.reshape(1, -1)).toarray()[0]\n    feature = model.tree_.feature\n    threshold = model.tree_.threshold\n    \n    print(\"≈öcie≈ºka decyzyjna:\")\n    for node_id in range(len(leaf_id)):\n        if leaf_id[node_id] == 1:  # je≈õli wƒôze≈Ç jest na ≈õcie≈ºce\n            if feature[node_id] != -2:  # je≈õli nie jest li≈õciem\n                feature_name = feature_names[feature[node_id]]\n                threshold_val = threshold[node_id]\n                feature_val = X_sample[feature[node_id]]\n                \n                if feature_val <= threshold_val:\n                    condition = \"<=\"\n                else:\n                    condition = \">\"\n                    \n                print(f\"  {feature_name} ({feature_val:.0f}) {condition} {threshold_val:.0f}\")\n\n# Przyk≈Çad dla konkretnego klienta\nsample_client = X_test.iloc[0]\nprediction = dt_model.predict([sample_client])[0]\nprobability = dt_model.predict_proba([sample_client])[0]\n\nprint(f\"Klient testowy:\")\nprint(f\"Wiek: {sample_client['wiek']}\")\nprint(f\"Doch√≥d: {sample_client['dochod']:.0f}\")\nprint(f\"Oszczƒôdno≈õci: {sample_client['oszczednosci']:.0f}\")\nprint(f\"\\nDecyzja: {'KREDYT PRZYZNANY' if prediction else 'KREDYT ODRZUCONY'}\")\nprint(f\"Prawdopodobie≈Ñstwo: {probability[1]:.1%}\")\n\nexplain_decision(dt_model, sample_client.values, feature_names)\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º dane klienta testowego\n\n::: {#d34a8756 .cell execution_count=12}\n\n::: {.cell-output .cell-output-stdout}\n```\nKlient testowy:\nWiek: 56.0\nDoch√≥d: 7927\nOszczƒôdno≈õci: 32270\n\nDecyzja: KREDYT ODRZUCONY\nPrawdopodobie≈Ñstwo: 3.0%\n```\n:::\n:::\n\n\n:::\n\n---\n\n## üéØ R√≥≈ºne zastosowania Decision Trees\n\n### 1) **Medyczna diagnoza**\n\n::: {#867ac89e .cell execution_count=13}\n``` {.python .cell-code}\n# Przyk≈Çad klasyfikacji ryzyka chor√≥b serca\nmedical_features = ['wiek', 'cholesterol', 'ci≈õnienie', 'BMI', 'pali_papierosy']\n# Target: 'ryzyko_chor√≥b_serca' (wysokie/niskie)\n\nmedical_tree = DecisionTreeClassifier(max_depth=4)\n# Model automatycznie znajdzie progi: \"Je≈õli cholesterol > 240 I BMI > 30 ORAZ wiek > 50...\"\n```\n:::\n\n\n### 2) **Marketing - segmentacja klient√≥w**\n\n::: {#17e25259 .cell execution_count=14}\n``` {.python .cell-code}\n# Przewidywanie czy klient kupi produkt premium\nmarketing_features = ['doch√≥d_roczny', 'wiek', 'wykszta≈Çcenie', 'poprzednie_zakupy']\n# Target: 'kupi_premium' (tak/nie)\n\nmarketing_tree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=100)\n# Wynik: jasne regu≈Çy marketingowe do targetowania reklam\n```\n:::\n\n\n### 3) **HR - decyzje o zatrudnieniu**\n\n::: {#0e4095bc .cell execution_count=15}\n``` {.python .cell-code}\n# Przewidywanie sukcesu kandydata w rekrutacji\nhr_features = ['do≈õwiadczenie_lat', 'wykszta≈Çcenie', 'wynik_test√≥w', 'referencje']\n# Target: 'zatrudniony' (tak/nie)\n\nhr_tree = DecisionTreeClassifier(max_depth=5)\n# Wynik: automatyczne zasady rekrutacyjne\n```\n:::\n\n\n---\n\n## ‚öôÔ∏è Tuning parametr√≥w\n\n::: {#ca8c8c86 .cell execution_count=16}\n``` {.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\n\n# Najwa≈ºniejsze parametry do tuningu\nparam_grid = {\n    'max_depth': [3, 5, 7, 10, None],\n    'min_samples_split': [20, 50, 100],\n    'min_samples_leaf': [10, 20, 50],\n    'criterion': ['gini', 'entropy']\n}\n\n# Grid search z cross-validation\ngrid_search = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(\"Najlepsze parametry:\")\nprint(grid_search.best_params_)\nprint(f\"Najlepsza dok≈Çadno≈õƒá CV: {grid_search.best_score_:.1%}\")\n\n# Model z najlepszymi parametrami\nbest_model = grid_search.best_estimator_\nbest_accuracy = accuracy_score(y_test, best_model.predict(X_test))\nprint(f\"Dok≈Çadno≈õƒá na test set: {best_accuracy:.1%}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º najlepsze parametry\n\n::: {#dd0697e0 .cell execution_count=17}\n\n::: {.cell-output .cell-output-stdout}\n```\nNajlepsze parametry:\n{'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 20}\nNajlepsza dok≈Çadno≈õƒá CV: 96.5%\nDok≈Çadno≈õƒá na test set: 95.0%\n```\n:::\n:::\n\n\n:::\n\n---\n\n## ‚ö†Ô∏è Problemy i rozwiƒÖzania\n\n### 1) **Overfitting - drzewo za g≈Çƒôbokie**\n\n::: {#cc33161e .cell execution_count=18}\n``` {.python .cell-code}\n# Problem: drzewo \"pamiƒôta\" dane treningowe\noverfitted_tree = DecisionTreeClassifier()  # bez ogranicze≈Ñ!\noverfitted_tree.fit(X_train, y_train)\n\ntrain_acc = accuracy_score(y_train, overfitted_tree.predict(X_train))\ntest_acc = accuracy_score(y_test, overfitted_tree.predict(X_test))\n\nprint(f\"Overfitted model:\")\nprint(f\"Train accuracy: {train_acc:.1%}\")\nprint(f\"Test accuracy: {test_acc:.1%}\")\nprint(f\"R√≥≈ºnica: {train_acc - test_acc:.1%} - to overfitting!\")\n\n# RozwiƒÖzanie: ograniczenia\npruned_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=20)\npruned_tree.fit(X_train, y_train)\n\ntrain_acc_pruned = accuracy_score(y_train, pruned_tree.predict(X_train))\ntest_acc_pruned = accuracy_score(y_test, pruned_tree.predict(X_test))\n\nprint(f\"\\nPruned model:\")\nprint(f\"Train accuracy: {train_acc_pruned:.1%}\")\nprint(f\"Test accuracy: {test_acc_pruned:.1%}\")\nprint(f\"R√≥≈ºnica: {train_acc_pruned - test_acc_pruned:.1%} - znacznie lepiej!\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki\n\n::: {#bd03037e .cell execution_count=19}\n\n::: {.cell-output .cell-output-stdout}\n```\nOverfitted model:\nTrain accuracy: 100.0%\nTest accuracy: 94.8%\nR√≥≈ºnica: 5.2% - to overfitting!\n\nPruned model:\nTrain accuracy: 96.7%\nTest accuracy: 95.0%\nR√≥≈ºnica: 1.7% - znacznie lepiej!\n```\n:::\n:::\n\n\n:::\n\n### 2) **Brak stabilno≈õci - ma≈Çe zmiany = r√≥≈ºne drzewa**\n\n::: {#1039c84c .cell execution_count=20}\n``` {.python .cell-code}\n# Problem demonstracji\nresults = []\nfor i in range(10):\n    # R√≥≈ºne random_state = r√≥≈ºne drzewa\n    tree_test = DecisionTreeClassifier(random_state=i, max_depth=5)\n    tree_test.fit(X_train, y_train)\n    acc = accuracy_score(y_test, tree_test.predict(X_test))\n    results.append(acc)\n\nprint(f\"Dok≈Çadno≈õƒá r√≥≈ºnych drzew: {min(results):.1%} - {max(results):.1%}\")\nprint(f\"Rozrzut: {max(results) - min(results):.1%}\")\nprint(\"RozwiƒÖzanie: Random Forest (nastƒôpna ≈õciƒÖgawka!)\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki\n\n::: {#abf71be6 .cell execution_count=21}\n\n::: {.cell-output .cell-output-stdout}\n```\nDok≈Çadno≈õƒá r√≥≈ºnych drzew: 96.2% - 96.2%\nRozrzut: 0.0%\nRozwiƒÖzanie: Random Forest (nastƒôpna ≈õciƒÖgawka!)\n```\n:::\n:::\n\n\n:::\n\n---\n\n## üåç Real-world przypadki u≈ºycia\n\n1. **Bankowo≈õƒá:** Ocena ryzyka kredytowego, wykrywanie fraud√≥w\n2. **Medycyna:** Systemy wspomagania diagnostyki, triage pacjent√≥w  \n3. **E-commerce:** Rekomendacje produkt√≥w, ustalanie cen dynamicznych\n4. **HR:** Automatyzacja proces√≥w rekrutacyjnych\n5. **Marketing:** Segmentacja klient√≥w, personalizacja oferowania\n\n::: {.callout-tip}\n## üí° Kiedy u≈ºywaƒá Decision Trees?\n\n**‚úÖ U≈ªYJ GDY:**\n\n- Potrzebujesz interpretowalnego modelu\n- Dane majƒÖ kategoryczne zmienne  \n- Chcesz zrozumieƒá \"dlaczego\" model podjƒÖ≈Ç decyzjƒô\n- Masz nieliniowe zale≈ºno≈õci w danych\n- Brak√≥w w danych nie trzeba impute'owaƒá\n\n**‚ùå NIE U≈ªYWAJ GDY:**\n\n- Potrzebujesz najwy≈ºszej dok≈Çadno≈õci (u≈ºyj Random Forest/XGBoost)\n- Masz bardzo g≈Çƒôbokie wzorce w danych (u≈ºyj Neural Networks)\n- Dane sƒÖ bardzo ha≈Ça≈õliwe\n- Przewidujesz warto≈õci ciƒÖg≈Çe (lepiej Linear Regression)\n:::\n\n**Nastƒôpna ≈õciƒÖgawka:** Random Forest - zesp√≥≈Ç drzew (wkr√≥tce)! üå≤üå≤üå≤\n\n",
    "supporting": [
      "03-decision-trees_files"
    ],
    "filters": [],
    "includes": {}
  }
}