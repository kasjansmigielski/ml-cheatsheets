{
  "hash": "9b429c1e412617cac05e39c0d4015987",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Random Forest ‚Äî las drzew decyzyjnych\"\nformat:\n  html:\n    code-tools: true\n---\n\n## üå≤ Czym jest Random Forest?\n\n**Random Forest** to zesp√≥≈Ç wielu drzew decyzyjnych, kt√≥re **g≈ÇosujƒÖ razem** nad ko≈ÑcowƒÖ decyzjƒÖ. Jeden Decision Tree mo≈ºe siƒô myliƒá, ale gdy masz 100 drzew i wiƒôkszo≈õƒá m√≥wi \"TAK\" - to prawdopodobnie dobra odpowied≈∫!\n\n::: {.callout-note}\n## üí° Intuicja\nWyobra≈∫ sobie konsylium lekarskim: jeden lekarz mo≈ºe siƒô pomyliƒá w diagnozie, ale gdy 7 z 10 ekspert√≥w zgadza siƒô - diagnoza jest znacznie bardziej pewna. Random Forest dzia≈Ça dok≈Çadnie tak samo!\n:::\n\n---\n\n## üéØ Praktyczny przyk≈Çad: przewidywanie sukcesu produktu\n\nCzy nowy produkt odniesie sukces na rynku?\n\n::: {#6e2b1d1b .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Tworzenie realistycznych danych produkt√≥w\nnp.random.seed(42)\nn_products = 2000\n\ndata = pd.DataFrame({\n    'cena': np.random.lognormal(6, 1, n_products),  # log-normal dla cen\n    'jakosc': np.random.randint(1, 11, n_products),  # 1-10 skala jako≈õci\n    'marketing_budget': np.random.exponential(50000, n_products),\n    'konkurencja': np.random.randint(1, 21, n_products),  # liczba konkurent√≥w\n    'sezon': np.random.choice(['wiosna', 'lato', 'jesien', 'zima'], n_products),\n    'kategoria': np.random.choice(['elektronika', 'moda', 'dom', 'sport', 'ksiazki'], n_products),\n    'ocena_testowa': np.random.normal(7, 2, n_products),  # oceny focus group\n    'innowacyjnosc': np.random.randint(1, 11, n_products),\n    'marka_znana': np.random.choice([0, 1], n_products, p=[0.3, 0.7])  # czy znana marka\n})\n\n# Realistyczna logika sukcesu produktu\ndef czy_sukces(row):\n    score = 0\n    \n    # Cena (sweet spot 50-500)\n    if 50 <= row['cena'] <= 500:\n        score += 2\n    elif row['cena'] > 1000:\n        score -= 2\n    \n    # Jako≈õƒá (najwa≈ºniejsze!)\n    score += row['jakosc'] * 0.8\n    \n    # Marketing\n    if row['marketing_budget'] > 100000:\n        score += 3\n    elif row['marketing_budget'] > 50000:\n        score += 1\n    \n    # Konkurencja (mniej = lepiej)\n    score -= row['konkurencja'] * 0.2\n    \n    # Sezon (lato i zima lepsze)\n    if row['sezon'] in ['lato', 'zima']:\n        score += 1\n    \n    # Kategoria\n    if row['kategoria'] == 'elektronika':\n        score += 1\n    elif row['kategoria'] == 'ksiazki':\n        score -= 1\n    \n    # Ocena testowa\n    score += (row['ocena_testowa'] - 5) * 0.5\n    \n    # Innowacyjno≈õƒá\n    score += row['innowacyjnosc'] * 0.3\n    \n    # Znana marka\n    if row['marka_znana']:\n        score += 2\n    \n    # Ko≈Ñcowa decyzja z odrobinƒÖ losowo≈õci\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 1)))\n    return probability > 0.5\n\ndata['sukces'] = data.apply(czy_sukces, axis=1)\n\nprint(\"Statystyki produkt√≥w:\")\nprint(data.describe())\nprint(f\"\\nOdsetek udanych produkt√≥w: {data['sukces'].mean():.1%}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º statystyki produkt√≥w\n\n::: {#c99cc829 .cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\nStatystyki produkt√≥w:\n               cena       jakosc  marketing_budget  konkurencja  \\\ncount   2000.000000  2000.000000       2000.000000   2000.00000   \nmean     692.530569     5.534500      49376.934612     10.53950   \nstd      929.701850     2.871922      49876.166534      5.76594   \nmin       15.779832     1.000000         11.353200      1.00000   \n25%      216.445352     3.000000      13957.783468      6.00000   \n50%      421.867820     6.000000      33526.684882     11.00000   \n75%      798.695019     8.000000      67692.822351     16.00000   \nmax    19010.210160    10.000000     376260.171802     20.00000   \n\n       ocena_testowa  innowacyjnosc  marka_znana  \ncount    2000.000000    2000.000000  2000.000000  \nmean        7.011461       5.500000     0.711000  \nstd         1.957420       2.909679     0.453411  \nmin        -0.844801       1.000000     0.000000  \n25%         5.700781       3.000000     0.000000  \n50%         7.041344       6.000000     1.000000  \n75%         8.307413       8.000000     1.000000  \nmax        13.754766      10.000000     1.000000  \n\nOdsetek udanych produkt√≥w: 99.2%\n```\n:::\n:::\n\n\n:::\n\n---\n\n## üîß Budowanie modelu krok po kroku\n\n### 1) Przygotowanie danych\n\n::: {#9ea7be7e .cell execution_count=3}\n``` {.python .cell-code}\n# Encoding kategorycznych zmiennych\nfrom sklearn.preprocessing import LabelEncoder\n\ndata_encoded = data.copy()\nle_sezon = LabelEncoder()\nle_kategoria = LabelEncoder()\n\ndata_encoded['sezon_num'] = le_sezon.fit_transform(data['sezon'])\ndata_encoded['kategoria_num'] = le_kategoria.fit_transform(data['kategoria'])\n\n# Features do modelu\nfeature_columns = ['cena', 'jakosc', 'marketing_budget', 'konkurencja', \n                  'sezon_num', 'kategoria_num', 'ocena_testowa', \n                  'innowacyjnosc', 'marka_znana']\nX = data_encoded[feature_columns]\ny = data_encoded['sukces']\n\n# Podzia≈Ç train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Dane treningowe: {len(X_train)} produkt√≥w\")\nprint(f\"Dane testowe: {len(X_test)} produkt√≥w\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º podzia≈Ç danych\n\n::: {#1c75e19f .cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\nDane treningowe: 1600 produkt√≥w\nDane testowe: 400 produkt√≥w\n```\n:::\n:::\n\n\n:::\n\n### 2) Trenowanie Random Forest\n\n::: {#06798f5a .cell execution_count=5}\n``` {.python .cell-code}\n# Tworzenie modelu Random Forest\nrf_model = RandomForestClassifier(\n    n_estimators=100,       # liczba drzew w lesie\n    max_depth=10,          # maksymalna g≈Çƒôboko≈õƒá ka≈ºdego drzewa\n    min_samples_split=20,   # min. pr√≥bek do podzia≈Çu\n    min_samples_leaf=10,    # min. pr√≥bek w li≈õciu\n    random_state=42,\n    n_jobs=-1              # u≈ºyj wszystkie rdzenie CPU\n)\n\n# Trenowanie\nrf_model.fit(X_train, y_train)\n\nprint(\"Random Forest wytrenowany!\")\nprint(f\"Liczba drzew: {rf_model.n_estimators}\")\nprint(f\"≈örednia g≈Çƒôboko≈õƒá drzew: {np.mean([tree.tree_.max_depth for tree in rf_model.estimators_]):.1f}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º parametry wytrenowanego modelu\n\n::: {#af33619f .cell execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest wytrenowany!\nLiczba drzew: 100\n≈örednia g≈Çƒôboko≈õƒá drzew: 5.2\n```\n:::\n:::\n\n\n:::\n\n### 3) Ewaluacja modelu\n\n::: {#dea7a50a .cell execution_count=7}\n``` {.python .cell-code}\n# Predykcje\ny_pred = rf_model.predict(X_test)\ny_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n\n# Metryki\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nDok≈Çadno≈õƒá Random Forest: {accuracy:.1%}\")\n\n# Szczeg√≥≈Çowy raport\nprint(\"\\nRaport klasyfikacji:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:     Pora≈ºka  Sukces\")\nprint(f\"Rzeczywiste Pora≈ºka: {cm[0,0]:3d}     {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Sukces:  {cm[1,0]:3d}     {cm[1,1]:3d}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki Random Forest\n\n::: {#dd0a5a0d .cell execution_count=8}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nDok≈Çadno≈õƒá Random Forest: 99.8%\n\nConfusion Matrix:\nPrzewidywane:     Pora≈ºka  Sukces\nRzeczywiste Pora≈ºka:   0       1\nRzeczywiste Sukces:    0     399\n```\n:::\n:::\n\n\n:::\n\n---\n\n## üìä Analiza wa≈ºno≈õci cech (Feature Importance)\n\n::: {#d6fedd95 .cell execution_count=9}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Przygotuj dane (powtarzamy dla kompletno≈õci)\nnp.random.seed(42)\nn_products = 2000\n\ndata = pd.DataFrame({\n    'cena': np.random.lognormal(6, 1, n_products),\n    'jakosc': np.random.randint(1, 11, n_products),\n    'marketing_budget': np.random.exponential(50000, n_products),\n    'konkurencja': np.random.randint(1, 21, n_products),\n    'sezon': np.random.choice(['wiosna', 'lato', 'jesien', 'zima'], n_products),\n    'kategoria': np.random.choice(['elektronika', 'moda', 'dom', 'sport', 'ksiazki'], n_products),\n    'ocena_testowa': np.random.normal(7, 2, n_products),\n    'innowacyjnosc': np.random.randint(1, 11, n_products),\n    'marka_znana': np.random.choice([0, 1], n_products, p=[0.3, 0.7])\n})\n\ndef czy_sukces(row):\n    score = 0\n    if 50 <= row['cena'] <= 500:\n        score += 2\n    elif row['cena'] > 1000:\n        score -= 2\n    score += row['jakosc'] * 0.8\n    if row['marketing_budget'] > 100000:\n        score += 3\n    elif row['marketing_budget'] > 50000:\n        score += 1\n    score -= row['konkurencja'] * 0.2\n    if row['sezon'] in ['lato', 'zima']:\n        score += 1\n    if row['kategoria'] == 'elektronika':\n        score += 1\n    elif row['kategoria'] == 'ksiazki':\n        score -= 1\n    score += (row['ocena_testowa'] - 5) * 0.5\n    score += row['innowacyjnosc'] * 0.3\n    if row['marka_znana']:\n        score += 2\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 1)))\n    return probability > 0.5\n\ndata['sukces'] = data.apply(czy_sukces, axis=1)\n\n# Encoding i model\ndata_encoded = data.copy()\nle_sezon = LabelEncoder()\nle_kategoria = LabelEncoder()\ndata_encoded['sezon_num'] = le_sezon.fit_transform(data['sezon'])\ndata_encoded['kategoria_num'] = le_kategoria.fit_transform(data['kategoria'])\n\nfeature_columns = ['cena', 'jakosc', 'marketing_budget', 'konkurencja', \n                  'sezon_num', 'kategoria_num', 'ocena_testowa', \n                  'innowacyjnosc', 'marka_znana']\nX = data_encoded[feature_columns]\ny = data_encoded['sukces']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenuj model\nrf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\nrf_model.fit(X_train, y_train)\n\n# Ewaluacja\ny_pred = rf_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\n# Wa≈ºno≈õƒá cech\nfeature_names = ['cena', 'jako≈õƒá', 'marketing_budget', 'konkurencja', \n                'sezon', 'kategoria', 'ocena_testowa', 'innowacyjno≈õƒá', 'marka_znana']\nimportance = rf_model.feature_importances_\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importance\n}).sort_values('importance', ascending=False)\n\n# Wykres wa≈ºno≈õci cech\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance['feature'], feature_importance['importance'])\nplt.xlabel('Wa≈ºno≈õƒá cechy')\nplt.title('Wa≈ºno≈õƒá cech w przewidywaniu sukcesu produktu')\nplt.gca().invert_yaxis()\nplt.close()\n\nprint(f\"Wyniki Random Forest:\")\nprint(f\"Dok≈Çadno≈õƒá modelu: {accuracy:.1%}\")\nprint(f\"Liczba drzew: {rf_model.n_estimators}\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:     Pora≈ºka  Sukces\")\nprint(f\"Rzeczywiste Pora≈ºka: {cm[0,0]:3d}     {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Sukces:  {cm[1,0]:3d}     {cm[1,1]:3d}\")\n\nprint(\"\\nWa≈ºno≈õƒá cech:\")\nfor _, row in feature_importance.iterrows():\n    print(f\"{row['feature']}: {row['importance']:.3f}\")\n\n# Odtw√≥rz wykres\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance['feature'], feature_importance['importance'])\nplt.xlabel('Wa≈ºno≈õƒá cechy')\nplt.title('Wa≈ºno≈õƒá cech w przewidywaniu sukcesu produktu')\nplt.gca().invert_yaxis()\nplt.show()\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki i wykres wa≈ºno≈õci cech\n\n::: {#7b28af4d .cell fig-height='6' fig-width='10' execution_count=10}\n\n::: {.cell-output .cell-output-stdout}\n```\nWyniki Random Forest:\nDok≈Çadno≈õƒá modelu: 99.8%\nLiczba drzew: 100\n\nConfusion Matrix:\nPrzewidywane:     Pora≈ºka  Sukces\nRzeczywiste Pora≈ºka:   0       1\nRzeczywiste Sukces:    0     399\n\nWa≈ºno≈õƒá cech:\ncena: 0.294\nmarketing_budget: 0.168\nocena_testowa: 0.143\njako≈õƒá: 0.117\nkonkurencja: 0.105\ninnowacyjno≈õƒá: 0.069\nsezon: 0.042\nkategoria: 0.033\nmarka_znana: 0.028\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Wa≈ºno≈õƒá cech w Random Forest](04-random-forest_files/figure-html/cell-11-output-2.png){width=894 height=526}\n:::\n:::\n\n\n:::\n\n---\n\n## üîç Random Forest vs Decision Tree - por√≥wnanie\n\n::: {#6d0d087d .cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Por√≥wnanie z pojedynczym Decision Tree\ndt_model = DecisionTreeClassifier(max_depth=10, random_state=42)\ndt_model.fit(X_train, y_train)\n\ndt_pred = dt_model.predict(X_test)\ndt_accuracy = accuracy_score(y_test, dt_pred)\n\nprint(\"POR√ìWNANIE MODELI:\")\nprint(f\"Decision Tree:    {dt_accuracy:.1%}\")\nprint(f\"Random Forest:    {accuracy:.1%}\")\nprint(f\"Poprawa:          +{(accuracy - dt_accuracy)*100:.1f} punkt√≥w procentowych\")\n\n# Test stabilno≈õci - r√≥≈ºne random_state\ndt_results = []\nrf_results = []\n\nfor rs in range(10):\n    # Decision Tree\n    dt_temp = DecisionTreeClassifier(max_depth=10, random_state=rs)\n    dt_temp.fit(X_train, y_train)\n    dt_results.append(accuracy_score(y_test, dt_temp.predict(X_test)))\n    \n    # Random Forest  \n    rf_temp = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=rs)\n    rf_temp.fit(X_train, y_train)\n    rf_results.append(accuracy_score(y_test, rf_temp.predict(X_test)))\n\nprint(f\"\\nSTABILNO≈öƒÜ (10 r√≥≈ºnych random_state):\")\nprint(f\"Decision Tree rozrzut: {max(dt_results)-min(dt_results):.1%}\")\nprint(f\"Random Forest rozrzut: {max(rf_results)-min(rf_results):.1%}\")\nprint(\"Random Forest jest znacznie bardziej stabilny!\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º por√≥wnanie modeli\n\n::: {#452617e9 .cell execution_count=12}\n\n::: {.cell-output .cell-output-stdout}\n```\nPOR√ìWNANIE MODELI:\nDecision Tree:    98.5%\nRandom Forest:    99.8%\nPoprawa:          +1.3 punkt√≥w procentowych\n\nSTABILNO≈öƒÜ (10 r√≥≈ºnych random_state):\nDecision Tree rozrzut: 1.0%\nRandom Forest rozrzut: 0.0%\nRandom Forest jest znacznie bardziej stabilny!\n```\n:::\n:::\n\n\n:::\n\n---\n\n## üéØ Zastosowania Random Forest\n\n### 1) **E-commerce - rekomendacje produkt√≥w**\n\n::: {#d150b61b .cell execution_count=13}\n``` {.python .cell-code}\n# Przewidywanie czy u≈ºytkownik kupi produkt\necommerce_features = ['historia_zakup√≥w', 'czas_na_stronie', 'kategoria_preferowana', \n                     'cena_produktu', 'oceny_produktu', 'sezon']\n# Target: 'kupi_produkt' (tak/nie)\n\necommerce_rf = RandomForestClassifier(n_estimators=200)\n# Wynik: system rekomendacji uwzglƒôdniajƒÖcy kompleksowe zachowania u≈ºytkownik√≥w\n```\n:::\n\n\n### 2) **Finanse - wykrywanie fraud√≥w**\n\n::: {#f9be4fe5 .cell execution_count=14}\n``` {.python .cell-code}\n# Identyfikacja podejrzanych transakcji\nfraud_features = ['kwota', 'godzina', 'lokalizacja', 'typ_karty', \n                 'historia_klienta', 'czƒôstotliwo≈õƒá_transakcji']\n# Target: 'fraud' (tak/nie)\n\nfraud_rf = RandomForestClassifier(n_estimators=500, class_weight='balanced')\n# Wynik: system antyfaudowy z wysokƒÖ dok≈Çadno≈õciƒÖ\n```\n:::\n\n\n### 3) **HR - przewidywanie odej≈õƒá pracownik√≥w**\n\n::: {#9fafd7ca .cell execution_count=15}\n``` {.python .cell-code}\n# Employee churn prediction\nhr_features = ['satisfaction', 'last_evaluation', 'projects', 'salary', \n              'time_company', 'work_accident', 'promotion']\n# Target: 'left_company' (tak/nie)\n\nhr_rf = RandomForestClassifier(n_estimators=300)\n# Wynik: wczesne ostrze≈ºenia przed odej≈õciami kluczowych pracownik√≥w\n```\n:::\n\n\n---\n\n## ‚öôÔ∏è Tuning parametr√≥w Random Forest\n\n::: {#3c7cac07 .cell execution_count=16}\n``` {.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\n\n# Najwa≈ºniejsze parametry do tuningu\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [5, 10, 15, None],\n    'min_samples_split': [10, 20, 50],\n    'min_samples_leaf': [5, 10, 20],\n    'max_features': ['sqrt', 'log2', None]\n}\n\n# Grid search z cross-validation (uwaga: mo≈ºe potrwaƒá!)\nprint(\"Rozpoczynam Grid Search - mo≈ºe potrwaƒá kilka minut...\")\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_grid,\n    cv=3,  # redukcja CV dla szybko≈õci\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(\"Najlepsze parametry:\")\nprint(grid_search.best_params_)\nprint(f\"Najlepsza dok≈Çadno≈õƒá CV: {grid_search.best_score_:.1%}\")\n\n# Model z najlepszymi parametrami\nbest_rf = grid_search.best_estimator_\nbest_accuracy = accuracy_score(y_test, best_rf.predict(X_test))\nprint(f\"Dok≈Çadno≈õƒá na test set: {best_accuracy:.1%}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º najlepsze parametry\n\n::: {#7c8bb1e3 .cell execution_count=17}\n\n::: {.cell-output .cell-output-stdout}\n```\nNajlepsze parametry:\n{'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 50}\nNajlepsza dok≈Çadno≈õƒá CV: 99.0%\nDok≈Çadno≈õƒá na test set: 99.8%\n```\n:::\n:::\n\n\n:::\n\n---\n\n## ‚ö†Ô∏è Pu≈Çapki i rozwiƒÖzania\n\n### 1) **Overfitting mimo ensemble**\n\n::: {#d0361db2 .cell execution_count=18}\n``` {.python .cell-code}\n# Problem: za g≈Çƒôbokie drzewa mogƒÖ nadal prowadziƒá do overfittingu\noverfitted_rf = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=None,    # bez limitu g≈Çƒôboko≈õci!\n    min_samples_leaf=1  # pozw√≥l na li≈õcie z 1 pr√≥bkƒÖ!\n)\noverfitted_rf.fit(X_train, y_train)\n\ntrain_acc_over = accuracy_score(y_train, overfitted_rf.predict(X_train))\ntest_acc_over = accuracy_score(y_test, overfitted_rf.predict(X_test))\n\nprint(f\"Overfitted Random Forest:\")\nprint(f\"Train accuracy: {train_acc_over:.1%}\")\nprint(f\"Test accuracy: {test_acc_over:.1%}\")\nprint(f\"R√≥≈ºnica: {train_acc_over - test_acc_over:.1%}\")\n\n# RozwiƒÖzanie: odpowiednie parametry\nbalanced_rf = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,      # ogranicz g≈Çƒôboko≈õƒá\n    min_samples_leaf=10  # wymu≈õ wiƒôcej pr√≥bek w li≈õciach\n)\nbalanced_rf.fit(X_train, y_train)\n\ntrain_acc_bal = accuracy_score(y_train, balanced_rf.predict(X_train))\ntest_acc_bal = accuracy_score(y_test, balanced_rf.predict(X_test))\n\nprint(f\"\\nBalanced Random Forest:\")\nprint(f\"Train accuracy: {train_acc_bal:.1%}\")\nprint(f\"Test accuracy: {test_acc_bal:.1%}\")\nprint(f\"R√≥≈ºnica: {train_acc_bal - test_acc_bal:.1%} - znacznie lepiej!\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki overfittingu\n\n::: {#4c04fcee .cell execution_count=19}\n\n::: {.cell-output .cell-output-stdout}\n```\nOverfitted Random Forest:\nTrain accuracy: 100.0%\nTest accuracy: 99.8%\nR√≥≈ºnica: 0.2%\n\nBalanced Random Forest:\nTrain accuracy: 99.0%\nTest accuracy: 99.8%\nR√≥≈ºnica: -0.8% - znacznie lepiej!\n```\n:::\n:::\n\n\n:::\n\n### 2) **Niezbalansowane klasy**\n\n::: {#8186f9d5 .cell execution_count=20}\n``` {.python .cell-code}\n# Problem: gdy jedna klasa jest rzadka (np. 5% fraud√≥w, 95% normalnych transakcji)\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Sprawd≈∫ balans klas w naszych danych\nprint(f\"Rozk≈Çad klas:\")\nprint(f\"Pora≈ºka: {(~y_train).sum()} ({(~y_train).mean():.1%})\")\nprint(f\"Sukces: {y_train.sum()} ({y_train.mean():.1%})\")\n\n# RozwiƒÖzanie 1: class_weight='balanced'\nbalanced_class_rf = RandomForestClassifier(\n    n_estimators=100,\n    class_weight='balanced',  # automatyczne wa≈ºenie klas\n    random_state=42\n)\nbalanced_class_rf.fit(X_train, y_train)\n\n# RozwiƒÖzanie 2: bootstrap sampling\nbalanced_rf = RandomForestClassifier(\n    n_estimators=100,\n    max_samples=0.8,  # u≈ºyj tylko 80% pr√≥bek do ka≈ºdego drzewa\n    random_state=42\n)\nbalanced_rf.fit(X_train, y_train)\n\nprint(\"RozwiƒÖzania zosta≈Çy zaimplementowane!\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º rozk≈Çad klas\n\n::: {#731234ab .cell execution_count=21}\n\n::: {.cell-output .cell-output-stdout}\n```\nRozk≈Çad klas:\nPora≈ºka: 16 (1.0%)\nSukces: 1584 (99.0%)\nRozwiƒÖzania zosta≈Çy zaimplementowane!\n```\n:::\n:::\n\n\n:::\n\n---\n\n## üåç Real-world przypadki u≈ºycia\n\n1. **Finanse:** Credit scoring, wykrywanie prania pieniƒôdzy, trading algorytmiczny\n2. **E-commerce:** Systemy rekomendacji, dynamic pricing, churn prediction\n3. **Healthcare:** Diagnoza medyczna, drug discovery, analiza obraz√≥w medycznych\n4. **Marketing:** Customer segmentation, campaign optimization, A/B testing\n5. **Cybersecurity:** Intrusion detection, malware classification, anomaly detection\n\n::: {.callout-tip}\n## üí° Kiedy u≈ºywaƒá Random Forest?\n\n**‚úÖ U≈ªYJ GDY:**\n\n- Potrzebujesz wysokiej dok≈Çadno≈õci z interpretowalnym modelem\n- Masz mixed features (liczbowe + kategoryczne)\n- Dane zawierajƒÖ missing values (RF radzi sobie z nimi dobrze)\n- Chcesz feature importance bez complex feature engineering\n- Potrzebujesz stabilnego modelu (ma≈Ça wariancja)\n\n**‚ùå NIE U≈ªYWAJ GDY:**\n\n- Masz bardzo du≈ºe datasety (u≈ºyj XGBoost/LightGBM)\n- Potrzebujesz bardzo prostego modelu (u≈ºyj Decision Tree)\n- Target jest continuous i potrzebujesz liniowej interpretacji (u≈ºyj Linear Regression)\n- Masz bardzo wysokowymiarowe dane (u≈ºyj SVM lub Neural Networks)\n:::\n\n**Nastƒôpna ≈õciƒÖgawka:** [K-Means Clustering - grupowanie bez etykiet](05-kmeans.qmd)! üéØ\n\n",
    "supporting": [
      "04-random-forest_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}