{
  "hash": "0942f10d36a539825cb51cf8b53bb694",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Wprowadzenie do Machine Learning — fundament\"\nformat:\n  html:\n    code-tools: true\n---\n\n## 🤖 Czym jest Machine Learning?\n\n**Machine Learning (ML)** to dziedzina informatyki, która pozwala komputerom **uczyć się i podejmować decyzje na podstawie danych**, bez konieczności programowania każdej reguły z góry.\n\n::: {.callout-note}\n## 💡 Intuicja\nZamiast pisać kod \"jeśli temperatura > 25°C, to będzie słonecznie\", **ML pozwala algorytmowi samemu odkryć** te zależności z historycznych danych pogodowych.\n:::\n\n---\n\n## 📊 Główne rodzaje ML\n\n### 1) **Supervised Learning** (Uczenie nadzorowane)\n**Mamy dane + znamy prawidłowe odpowiedzi**\n\n::: {#supervised-example .cell execution_count=1}\n``` {.python .cell-code}\n# Przykład: predykcja ceny domu\n# Dane wejściowe: powierzchnia, lokalizacja, rok budowy\n# Cel: przewidzieć cenę\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Przykładowe dane\ndata = pd.DataFrame({\n    'powierzchnia': [50, 75, 100, 120, 150],\n    'rok_budowy': [1990, 2000, 2010, 2015, 2020],\n    'cena': [300000, 400000, 550000, 650000, 800000]  # znamy prawdziwe ceny!\n})\n\nprint(\"Dane treningowe:\")\nprint(data)\n\n# Trenowanie modelu\nmodel = LinearRegression()\nX = data[['powierzchnia', 'rok_budowy']]\ny = data['cena']\nmodel.fit(X, y)\n\n# Predykcja dla nowego domu\nnowy_dom = [[90, 2005]]\nprzewidywana_cena = model.predict(nowy_dom)\nprint(f\"\\nPredykcja dla domu 90m², rok 2005:\")\nprint(f\"Przewidywana cena: {przewidywana_cena[0]:.0f} zł\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDane treningowe:\n   powierzchnia  rok_budowy    cena\n0            50        1990  300000\n1            75        2000  400000\n2           100        2010  550000\n3           120        2015  650000\n4           150        2020  800000\n\nPredykcja dla domu 90m², rok 2005:\nPrzewidywana cena: 493819 zł\n```\n:::\n:::\n\n\n**Real-world zastosowania:**\n- Predykcja cen akcji/nieruchomości\n- Diagnoza medyczna (klasyfikacja chorób)\n- Filtrowanie spamu w emailach\n- Rozpoznawanie mowy/obrazów\n\n---\n\n### 2) **Unsupervised Learning** (Uczenie nienadzorowane)  \n**Mamy tylko dane, szukamy ukrytych wzorców**\n\n::: {#unsupervised-example .cell execution_count=2}\n``` {.python .cell-code}\n# Przykład: segmentacja klientów sklepu\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Dane klientów: wiek i wydatki miesięczne\nklienci = pd.DataFrame({\n    'wiek': [25, 30, 35, 22, 28, 45, 50, 55, 60, 65],\n    'wydatki': [2000, 2500, 3000, 1800, 2200, 4000, 4500, 3500, 3000, 2800]\n})\n\nprint(\"Dane klientów:\")\nprint(klienci.head())\n\n# Grupowanie klientów w 3 segmenty\nkmeans = KMeans(n_clusters=3, random_state=42)\nklienci['segment'] = kmeans.fit_predict(klienci[['wiek', 'wydatki']])\n\nprint(\"\\nSegmenty klientów:\")\nfor i in range(3):\n    segment = klienci[klienci['segment'] == i]\n    print(f\"Segment {i}: średni wiek {segment['wiek'].mean():.0f}, \"\n          f\"średnie wydatki {segment['wydatki'].mean():.0f}\")\n\nprint(f\"\\nPrzykład: klient 30 lat, wydaje 2500zł -> segment {kmeans.predict([[30, 2500]])[0]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDane klientów:\n   wiek  wydatki\n0    25     2000\n1    30     2500\n2    35     3000\n3    22     1800\n4    28     2200\n\nSegmenty klientów:\nSegment 0: średni wiek 25, średnie wydatki 2000\nSegment 1: średni wiek 50, średnie wydatki 4000\nSegment 2: średni wiek 48, średnie wydatki 2825\n\nPrzykład: klient 30 lat, wydaje 2500zł -> segment 2\n```\n:::\n:::\n\n\n**Real-world zastosowania:**\n- Segmentacja klientów (marketing)\n- Wykrywanie anomalii (cyberbezpieczeństwo)\n- Analiza koszykowa (co kupują razem)\n- Kompresja danych\n\n---\n\n### 3) **Reinforcement Learning** (Uczenie ze wzmocnieniem)\n**Agent uczy się przez interakcję i nagrody/kary**\n\n::: {#reinforcement-example .cell execution_count=3}\n``` {.python .cell-code}\n# Przykład koncepcyjny: optymalizacja reklam\nclass SimpleAgent:\n    def __init__(self):\n        # Jakie reklamy pokazywać: [\"sportowe\", \"technologiczne\", \"modowe\"]\n        self.ad_types = [\"sportowe\", \"technologiczne\", \"modowe\"]\n        self.rewards = [0, 0, 0]  # nagrody za każdy typ\n        self.counts = [0, 0, 0]   # ile razy pokazane\n    \n    def choose_ad(self):\n        # Wybierz reklamę z najwyższą średnią nagrodą\n        avg_rewards = [r/max(c,1) for r, c in zip(self.rewards, self.counts)]\n        return avg_rewards.index(max(avg_rewards))\n    \n    def update_reward(self, ad_type, clicked):\n        # Aktualizuj nagrody na podstawie kliknięć\n        self.counts[ad_type] += 1\n        if clicked:\n            self.rewards[ad_type] += 1\n    \n    def get_stats(self):\n        for i, ad_type in enumerate(self.ad_types):\n            rate = self.rewards[i] / max(self.counts[i], 1) * 100\n            print(f\"{ad_type}: {self.counts[i]} pokazań, {self.rewards[i]} kliknięć ({rate:.1f}%)\")\n\n# Symulacja\nimport numpy as np\nnp.random.seed(42)\n\nagent = SimpleAgent()\nprint(\"🎯 Symulacja optymalizacji reklam:\")\nprint(\"Agent uczy się, które reklamy działają najlepiej...\\n\")\n\nfor day in range(10):\n    ad = agent.choose_ad()\n    # Różne prawdopodobieństwa kliknięć dla różnych typów reklam\n    click_probs = [0.1, 0.3, 0.2]  # technologiczne najlepsze\n    clicked = np.random.random() < click_probs[ad]\n    agent.update_reward(ad, clicked)\n    print(f\"Dzień {day+1}: pokazano {agent.ad_types[ad]}, kliknięta: {clicked}\")\n\nprint(\"\\n📊 Końcowe statystyki:\")\nagent.get_stats()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n🎯 Symulacja optymalizacji reklam:\nAgent uczy się, które reklamy działają najlepiej...\n\nDzień 1: pokazano sportowe, kliknięta: False\nDzień 2: pokazano sportowe, kliknięta: False\nDzień 3: pokazano sportowe, kliknięta: False\nDzień 4: pokazano sportowe, kliknięta: False\nDzień 5: pokazano sportowe, kliknięta: False\nDzień 6: pokazano sportowe, kliknięta: False\nDzień 7: pokazano sportowe, kliknięta: True\nDzień 8: pokazano sportowe, kliknięta: False\nDzień 9: pokazano sportowe, kliknięta: False\nDzień 10: pokazano sportowe, kliknięta: False\n\n📊 Końcowe statystyki:\nsportowe: 10 pokazań, 1 kliknięć (10.0%)\ntechnologiczne: 0 pokazań, 0 kliknięć (0.0%)\nmodowe: 0 pokazań, 0 kliknięć (0.0%)\n```\n:::\n:::\n\n\n**Real-world zastosowania:**\n- Gry komputerowe (AI graczy)\n- Autonomiczne pojazdy\n- Optymalizacja reklam online\n- Roboty przemysłowe\n\n---\n\n## 🎯 Jak wybrać odpowiedni typ ML?\n\n| **Sytuacja** | **Typ ML** | **Przykład** |\n|-------------|------------|--------------|\n| Masz dane z prawidłowymi odpowiedziami | **Supervised** | Spam/nie-spam w emailach |\n| Chcesz znaleźć ukryte grupы | **Unsupervised** | Segmentacja klientów |\n| System ma się uczyć przez trial & error | **Reinforcement** | Bot do gier |\n\n---\n\n## 🔧 Podstawowe kroki projektu ML\n\n::: {#ml-workflow-example .cell execution_count=4}\n``` {.python .cell-code}\n# Kompletny workflow ML na przykładzie klasyfikacji iris\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\nprint(\"🔬 Kompletny workflow projektu ML\")\nprint(\"=\" * 40)\n\n# 1. Załaduj i poznaj dane\niris = load_iris()\ndata = pd.DataFrame(iris.data, columns=iris.feature_names)\ndata['target'] = iris.target\ntarget_names = iris.target_names\n\nprint(\"1️⃣ Dane załadowane:\")\nprint(f\"Kształt: {data.shape}\")\nprint(f\"Klasy: {target_names}\")\nprint(data.head(3))\n\n# 2. Przygotuj dane (czyszczenie, encoding)\nX = data.drop('target', axis=1)\ny = data['target']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nprint(f\"\\n2️⃣ Dane przeskalowane (pierwsze 3 cechy pierwszej próbki):\")\nprint(f\"Przed: {X.iloc[0, :3].values}\")\nprint(f\"Po: {X_scaled[0, :3]}\")\n\n# 3. Podziel na train/test\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\nprint(f\"\\n3️⃣ Podział danych:\")\nprint(f\"Train: {len(X_train)} próbek\")\nprint(f\"Test: {len(X_test)} próbek\")\n\n# 4. Wybierz i wytrenuj model\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\nprint(f\"\\n4️⃣ Model wytrenowany: RandomForestClassifier\")\n\n# 5. Oceń wyniki\npredictions = model.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"\\n5️⃣ Wyniki:\")\nprint(f\"Dokładność: {accuracy:.2%}\")\n\n# Przykład predykcji\nsample_flower = X_test[0:1]\npredicted_class = model.predict(sample_flower)[0]\npredicted_name = target_names[predicted_class]\nactual_name = target_names[y_test.iloc[0]]\nprint(f\"\\nPrzykład predykcji:\")\nprint(f\"Przewidywana klasa: {predicted_name}\")\nprint(f\"Rzeczywista klasa: {actual_name}\")\nprint(\"✅ Poprawnie!\" if predicted_name == actual_name else \"❌ Błąd\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n🔬 Kompletny workflow projektu ML\n========================================\n1️⃣ Dane załadowane:\nKształt: (150, 5)\nKlasy: ['setosa' 'versicolor' 'virginica']\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                5.1               3.5                1.4               0.2   \n1                4.9               3.0                1.4               0.2   \n2                4.7               3.2                1.3               0.2   \n\n   target  \n0       0  \n1       0  \n2       0  \n\n2️⃣ Dane przeskalowane (pierwsze 3 cechy pierwszej próbki):\nPrzed: [5.1 3.5 1.4]\nPo: [-0.90068117  1.01900435 -1.34022653]\n\n3️⃣ Podział danych:\nTrain: 120 próbek\nTest: 30 próbek\n\n4️⃣ Model wytrenowany: RandomForestClassifier\n\n5️⃣ Wyniki:\nDokładność: 100.00%\n\nPrzykład predykcji:\nPrzewidywana klasa: versicolor\nRzeczywista klasa: versicolor\n✅ Poprawnie!\n```\n:::\n:::\n\n\n---\n\n## 💡 Najważniejsze biblioteki Python dla ML\n\n```python\n# Podstawowe przetwarzanie danych\nimport pandas as pd      # DataFrames, CSV, analiza\nimport numpy as np       # obliczenia numeryczne\n\n# Machine Learning\nfrom sklearn import *    # algorytmy ML, preprocessing, metryki\nimport xgboost as xgb   # zaawansowane drzewa decyzyjne\n\n# Wizualizacja\nimport matplotlib.pyplot as plt  # wykresy\nimport seaborn as sns           # piękne wykresy statystyczne\n\n# Deep Learning\nimport tensorflow as tf  # sieci neuronowe (Google)\nimport torch            # sieci neuronowe (Facebook)\n```\n\n---\n\n::: {.callout-tip}\n## 🎯 Pro tips dla początkujących\n\n1. **Zacznij od prostych algorytmów** - Linear Regression, Decision Trees\n2. **80% czasu to przygotowanie danych** - czyszczenie, eksploracja, feature engineering\n3. **Zawsze sprawdź czy model nie jest overfitted** - użyj validation set\n4. **Rozumiej swoje dane** przed wyborem algorytmu\n5. **Praktyka > teoria** - rób dużo projektów na różnych danych!\n:::\n\n**Następna ściągawka:** [Linear Regression w praktyce](02-linear-regression.qmd) 🚀\n\n",
    "supporting": [
      "01-intro-ml_files"
    ],
    "filters": [],
    "includes": {}
  }
}