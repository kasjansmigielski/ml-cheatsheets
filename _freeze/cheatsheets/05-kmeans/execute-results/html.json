{
  "hash": "852ec2821855e45d4c0d426494e902b7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"K-Means ‚Äî grupowanie klient√≥w bez etykiet\"\nformat:\n  html:\n    code-tools: true\n---\n\n## üéØ Czym jest K-Means?\n\n**K-Means** to algorytm **uczenia nienadzorowanego**, kt√≥ry automatycznie **grupuje podobne dane** w klastry. Nie potrzebujesz etykiet - algorytm sam znajduje wzorce i dzieli dane na K grup.\n\n::: {.callout-note}\n## üí° Intuicja\nWyobra≈∫ sobie imprezƒô, gdzie ludzie sami grupujƒÖ siƒô w krƒôgi na podstawie wsp√≥lnych zainteresowa≈Ñ. K-Means dzia≈Ça podobnie - znajduje \"centra grup\" i przydziela ka≈ºdy punkt do najbli≈ºszego centrum.\n:::\n\n---\n\n## üõçÔ∏è Praktyczny przyk≈Çad: segmentacja klient√≥w sklepu\n\nJak pogrupowaƒá klient√≥w do targetowanych kampanii marketingowych?\n\n::: {#f384ec99 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Tworzenie realistycznych danych klient√≥w e-commerce\nnp.random.seed(42)\nn_customers = 1000\n\n# Generujemy r√≥≈ºne typy klient√≥w z wyra≈∫nymi wzorcami\ncustomer_types = np.random.choice(['premium', 'average', 'bargain', 'sporadic'], n_customers, \n                                 p=[0.15, 0.35, 0.35, 0.15])\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_customers),\n    'roczny_dochod': np.random.normal(50000, 20000, n_customers),\n    'wydatki_roczne': np.random.normal(25000, 15000, n_customers),\n    'liczba_zakupow': np.random.randint(1, 50, n_customers),\n    'sredni_koszt_zakupu': np.random.normal(200, 100, n_customers),\n    'lata_jako_klient': np.random.randint(0, 10, n_customers),\n    'typ_klienta': customer_types\n})\n\n# Realistyczne korelacje miƒôdzy zmiennymi\nfor i, typ in enumerate(data['typ_klienta']):\n    if typ == 'premium':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(80000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(45000, 10000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(20, 50)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(400, 100)\n    elif typ == 'bargain':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(30000, 10000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(8000, 3000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(15, 40)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(80, 30)\n    elif typ == 'sporadic':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(45000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(5000, 2000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(1, 8)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(300, 150)\n\n# Czy≈õƒá dane - usu≈Ñ warto≈õci ujemne\ndata['roczny_dochod'] = np.maximum(data['roczny_dochod'], 15000)\ndata['wydatki_roczne'] = np.maximum(data['wydatki_roczne'], 1000)\ndata['sredni_koszt_zakupu'] = np.maximum(data['sredni_koszt_zakupu'], 20)\n\nprint(\"Statystyki klient√≥w:\")\nprint(data[['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', 'sredni_koszt_zakupu']].describe())\nprint(f\"\\nRozk≈Çad typ√≥w klient√≥w:\")\nprint(data['typ_klienta'].value_counts())\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º statystyki klient√≥w\n\n::: {#1d61ac1a .cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\nStatystyki klient√≥w:\n              wiek  roczny_dochod  wydatki_roczne  liczba_zakupow  \\\ncount  1000.000000    1000.000000     1000.000000      1000.00000   \nmean     43.359000   46988.933946    19816.768355        24.00700   \nstd      14.817276   22175.621844    17169.303009        13.41898   \nmin      18.000000   15000.000000     1000.000000         1.00000   \n25%      30.000000   29854.699281     6462.783773        15.00000   \n50%      43.000000   42533.564242    11041.836565        25.00000   \n75%      56.000000   62388.416902    33188.043332        35.00000   \nmax      69.000000  122502.434178    74817.934938        49.00000   \n\n       sredni_koszt_zakupu  \ncount          1000.000000  \nmean            203.883750  \nstd             143.706490  \nmin              20.000000  \n25%              83.839117  \n50%             162.389656  \n75%             303.607754  \nmax             763.483441  \n\nRozk≈Çad typ√≥w klient√≥w:\ntyp_klienta\nbargain     344\naverage     337\npremium     166\nsporadic    153\nName: count, dtype: int64\n```\n:::\n:::\n\n\n:::\n\n---\n\n## üîß Budowanie modelu krok po kroku\n\n### 1) Przygotowanie danych\n\n::: {#3f99054e .cell execution_count=3}\n``` {.python .cell-code}\n# Wybierz features do clusteringu (bez typ_klienta - to chcemy odkryƒá!)\nfeatures = ['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', \n           'sredni_koszt_zakupu', 'lata_jako_klient']\nX = data[features]\n\n# Standaryzacja - BARDZO WA≈ªNE w K-Means!\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(\"Przed standaryzacjƒÖ:\")\nprint(X.describe())\n\nprint(\"\\nPo standaryzacji (pierwsze 5 wierszy):\")\nX_scaled_df = pd.DataFrame(X_scaled, columns=features)\nprint(X_scaled_df.head())\nprint(\"\\n≈örednie po standaryzacji:\", X_scaled_df.mean().round(3))\nprint(\"Odchylenia po standaryzacji:\", X_scaled_df.std().round(3))\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki standaryzacji\n\n::: {#d6ab438d .cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\nPrzed standaryzacjƒÖ:\n              wiek  roczny_dochod  wydatki_roczne  liczba_zakupow  \\\ncount  1000.000000    1000.000000     1000.000000      1000.00000   \nmean     43.359000   46988.933946    19816.768355        24.00700   \nstd      14.817276   22175.621844    17169.303009        13.41898   \nmin      18.000000   15000.000000     1000.000000         1.00000   \n25%      30.000000   29854.699281     6462.783773        15.00000   \n50%      43.000000   42533.564242    11041.836565        25.00000   \n75%      56.000000   62388.416902    33188.043332        35.00000   \nmax      69.000000  122502.434178    74817.934938        49.00000   \n\n       sredni_koszt_zakupu  lata_jako_klient  \ncount          1000.000000       1000.000000  \nmean            203.883750          4.550000  \nstd             143.706490          2.914789  \nmin              20.000000          0.000000  \n25%              83.839117          2.000000  \n50%             162.389656          5.000000  \n75%             303.607754          7.000000  \nmax             763.483441          9.000000  \n\nPo standaryzacji (pierwsze 5 wierszy):\n       wiek  roczny_dochod  wydatki_roczne  liczba_zakupow  \\\n0  1.393733       0.346806        0.136732        0.894181   \n1 -0.969556       0.499661       -0.675331       -1.342577   \n2 -0.699466      -1.432630       -0.608789       -0.224198   \n3 -0.159286      -1.443249       -0.634140        0.223154   \n4 -0.496898      -1.443249       -0.966417        0.894181   \n\n   sredni_koszt_zakupu  lata_jako_klient  \n0             0.053442          1.184211  \n1             1.809673         -1.561786  \n2            -0.914825          0.154462  \n3            -1.101092         -1.561786  \n4            -0.652771         -0.875287  \n\n≈örednie po standaryzacji: wiek                  -0.0\nroczny_dochod         -0.0\nwydatki_roczne         0.0\nliczba_zakupow        -0.0\nsredni_koszt_zakupu    0.0\nlata_jako_klient       0.0\ndtype: float64\nOdchylenia po standaryzacji: wiek                   1.001\nroczny_dochod          1.001\nwydatki_roczne         1.001\nliczba_zakupow         1.001\nsredni_koszt_zakupu    1.001\nlata_jako_klient       1.001\ndtype: float64\n```\n:::\n:::\n\n\n:::\n\n### 2) Znajdowanie optymalnej liczby klastr√≥w\n\n::: {#74efb10f .cell execution_count=5}\n``` {.python .cell-code}\n# Metoda ≈Çokcia (Elbow Method)\ninertias = []\nsilhouette_scores = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n\n# Znajd≈∫ optymalne K\nbest_k = K_range[np.argmax(silhouette_scores)]\nprint(f\"Optymalna liczba klastr√≥w (Silhouette): {best_k}\")\n\nprint(\"\\nWyniki dla r√≥≈ºnych K:\")\nfor k, inertia, sil_score in zip(K_range, inertias, silhouette_scores):\n    print(f\"K={k}: Inertia={inertia:.0f}, Silhouette={sil_score:.3f}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º optymalne K\n\n::: {#e584fe25 .cell execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\nOptymalna liczba klastr√≥w (Silhouette): 2\n\nWyniki dla r√≥≈ºnych K:\nK=2: Inertia=4416, Silhouette=0.281\nK=3: Inertia=3691, Silhouette=0.232\nK=4: Inertia=3296, Silhouette=0.200\nK=5: Inertia=3018, Silhouette=0.196\nK=6: Inertia=2828, Silhouette=0.195\nK=7: Inertia=2665, Silhouette=0.176\nK=8: Inertia=2526, Silhouette=0.177\nK=9: Inertia=2403, Silhouette=0.181\nK=10: Inertia=2286, Silhouette=0.176\n```\n:::\n:::\n\n\n:::\n\n### 3) Trenowanie finalnego modelu\n\n::: {#4527ec01 .cell execution_count=7}\n``` {.python .cell-code}\n# Trenuj model z optymalnym K\nfinal_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\ncluster_labels = final_kmeans.fit_predict(X_scaled)\n\n# Dodaj etykiety klastr√≥w do danych\ndata['klaster'] = cluster_labels\n\nprint(f\"Model K-Means wytrenowany z K={best_k}\")\nprint(f\"Silhouette Score: {silhouette_score(X_scaled, cluster_labels):.3f}\")\n\n# Analiza klastr√≥w\nprint(\"\\nRozmiary klastr√≥w:\")\ncluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\nfor i, count in enumerate(cluster_counts):\n    print(f\"Klaster {i}: {count} klient√≥w ({count/len(data)*100:.1f}%)\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki finalnego modelu\n\n::: {#53fb7b7f .cell execution_count=8}\n\n::: {.cell-output .cell-output-stdout}\n```\nModel K-Means wytrenowany z K=2\nSilhouette Score: 0.281\n\nRozmiary klastr√≥w:\nKlaster 0: 275 klient√≥w (27.5%)\nKlaster 1: 725 klient√≥w (72.5%)\n```\n:::\n:::\n\n\n:::\n\n---\n\n## üìä Analiza i interpretacja klastr√≥w\n\n::: {#aadee708 .cell execution_count=9}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Przygotuj dane (powtarzamy dla kompletno≈õci)\nnp.random.seed(42)\nn_customers = 1000\n\ncustomer_types = np.random.choice(['premium', 'average', 'bargain', 'sporadic'], n_customers, \n                                 p=[0.15, 0.35, 0.35, 0.15])\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_customers),\n    'roczny_dochod': np.random.normal(50000, 20000, n_customers),\n    'wydatki_roczne': np.random.normal(25000, 15000, n_customers),\n    'liczba_zakupow': np.random.randint(1, 50, n_customers),\n    'sredni_koszt_zakupu': np.random.normal(200, 100, n_customers),\n    'lata_jako_klient': np.random.randint(0, 10, n_customers),\n    'typ_klienta': customer_types\n})\n\n# Popraw dane wed≈Çug typ√≥w\nfor i, typ in enumerate(data['typ_klienta']):\n    if typ == 'premium':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(80000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(45000, 10000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(20, 50)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(400, 100)\n    elif typ == 'bargain':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(30000, 10000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(8000, 3000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(15, 40)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(80, 30)\n    elif typ == 'sporadic':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(45000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(5000, 2000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(1, 8)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(300, 150)\n\ndata['roczny_dochod'] = np.maximum(data['roczny_dochod'], 15000)\ndata['wydatki_roczne'] = np.maximum(data['wydatki_roczne'], 1000)\ndata['sredni_koszt_zakupu'] = np.maximum(data['sredni_koszt_zakupu'], 20)\n\n# Przygotuj model\nfeatures = ['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', \n           'sredni_koszt_zakupu', 'lata_jako_klient']\nX = data[features]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Znajd≈∫ optymalne K\nsilhouette_scores = []\nK_range = range(2, 11)\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n\nbest_k = K_range[np.argmax(silhouette_scores)]\n\n# Trenuj finalny model\nfinal_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\ncluster_labels = final_kmeans.fit_predict(X_scaled)\ndata['klaster'] = cluster_labels\n\n# Analiza charakterystyk klastr√≥w\ncluster_analysis = data.groupby('klaster')[features].mean()\n\nprint(\"CHARAKTERYSTYKI KLASTR√ìW:\")\nprint(\"=\" * 50)\n\nfor cluster_id in range(best_k):\n    print(f\"\\nKLASTER {cluster_id}:\")\n    cluster_data = data[data['klaster'] == cluster_id]\n    \n    print(f\"Liczba klient√≥w: {len(cluster_data)} ({len(cluster_data)/len(data)*100:.1f}%)\")\n    print(f\"≈öredni wiek: {cluster_data['wiek'].mean():.0f} lat\")\n    print(f\"≈öredni doch√≥d: {cluster_data['roczny_dochod'].mean():.0f} z≈Ç\")\n    print(f\"≈örednie wydatki: {cluster_data['wydatki_roczne'].mean():.0f} z≈Ç\")\n    print(f\"≈örednia liczba zakup√≥w: {cluster_data['liczba_zakupow'].mean():.0f}\")\n    print(f\"≈öredni koszt zakupu: {cluster_data['sredni_koszt_zakupu'].mean():.0f} z≈Ç\")\n    \n    # Nadaj nazwƒô klastrowi na podstawie charakterystyk\n    avg_income = cluster_data['roczny_dochod'].mean()\n    avg_spending = cluster_data['wydatki_roczne'].mean()\n    avg_frequency = cluster_data['liczba_zakupow'].mean()\n    \n    if avg_income > 60000 and avg_spending > 30000:\n        cluster_name = \"üåü PREMIUM CUSTOMERS\"\n    elif avg_frequency < 10 and avg_spending < 10000:\n        cluster_name = \"üò¥ SPORADYCZNI KLIENCI\"\n    elif avg_spending / avg_income < 0.3 and avg_frequency > 20:\n        cluster_name = \"üí∞ BARGAIN HUNTERS\"\n    else:\n        cluster_name = \"üîÑ ≈öREDNI KLIENCI\"\n    \n    print(f\"Typ: {cluster_name}\")\n\n# Przygotuj wizualizacjƒô\nplt.figure(figsize=(12, 8))\n\n# Wykres 1: Doch√≥d vs Wydatki\nplt.subplot(2, 2, 1)\nscatter = plt.scatter(data['roczny_dochod'], data['wydatki_roczne'], \n                     c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Roczny doch√≥d')\nplt.ylabel('Wydatki roczne')\nplt.title('Doch√≥d vs Wydatki')\nplt.colorbar(scatter)\n\n# Wykres 2: Wiek vs Liczba zakup√≥w\nplt.subplot(2, 2, 2)\nscatter2 = plt.scatter(data['wiek'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Wiek')\nplt.ylabel('Liczba zakup√≥w')\nplt.title('Wiek vs Liczba zakup√≥w')\nplt.colorbar(scatter2)\n\n# Wykres 3: ≈öredni koszt vs Liczba zakup√≥w\nplt.subplot(2, 2, 3)\nscatter3 = plt.scatter(data['sredni_koszt_zakupu'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('≈öredni koszt zakupu')\nplt.ylabel('Liczba zakup√≥w')\nplt.title('Koszt vs Czƒôstotliwo≈õƒá')\nplt.colorbar(scatter3)\n\n# Wykres 4: Rozk≈Çad klastr√≥w\nplt.subplot(2, 2, 4)\ncluster_counts = data['klaster'].value_counts().sort_index()\nplt.bar(range(len(cluster_counts)), cluster_counts.values, \n        color=plt.cm.viridis(np.linspace(0, 1, len(cluster_counts))))\nplt.xlabel('Klaster')\nplt.ylabel('Liczba klient√≥w')\nplt.title('Rozmiary klastr√≥w')\nplt.xticks(range(len(cluster_counts)))\n\nplt.tight_layout()\nplt.close()\n\n# Por√≥wnanie z rzeczywistymi typami klient√≥w\nif 'typ_klienta' in data.columns:\n    print(\"\\n\" + \"=\"*50)\n    print(\"POR√ìWNANIE Z RZECZYWISTYMI TYPAMI:\")\n    comparison = pd.crosstab(data['klaster'], data['typ_klienta'])\n    print(comparison)\n\n# Odtw√≥rz wykresy\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nscatter = plt.scatter(data['roczny_dochod'], data['wydatki_roczne'], \n                     c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Roczny doch√≥d')\nplt.ylabel('Wydatki roczne')\nplt.title('Doch√≥d vs Wydatki')\nplt.colorbar(scatter)\n\nplt.subplot(2, 2, 2)\nscatter2 = plt.scatter(data['wiek'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Wiek')\nplt.ylabel('Liczba zakup√≥w')\nplt.title('Wiek vs Liczba zakup√≥w')\nplt.colorbar(scatter2)\n\nplt.subplot(2, 2, 3)\nscatter3 = plt.scatter(data['sredni_koszt_zakupu'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('≈öredni koszt zakupu')\nplt.ylabel('Liczba zakup√≥w')\nplt.title('Koszt vs Czƒôstotliwo≈õƒá')\nplt.colorbar(scatter3)\n\nplt.subplot(2, 2, 4)\ncluster_counts = data['klaster'].value_counts().sort_index()\nplt.bar(range(len(cluster_counts)), cluster_counts.values, \n        color=plt.cm.viridis(np.linspace(0, 1, len(cluster_counts))))\nplt.xlabel('Klaster')\nplt.ylabel('Liczba klient√≥w')\nplt.title('Rozmiary klastr√≥w')\nplt.xticks(range(len(cluster_counts)))\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º analizƒô klastr√≥w i wizualizacje\n\n::: {#5fd950d8 .cell fig-height='8' fig-width='12' execution_count=10}\n\n::: {.cell-output .cell-output-stdout}\n```\nCHARAKTERYSTYKI KLASTR√ìW:\n==================================================\n\nKLASTER 0:\nLiczba klient√≥w: 275 (27.5%)\n≈öredni wiek: 43 lat\n≈öredni doch√≥d: 71194 z≈Ç\n≈örednie wydatki: 41779 z≈Ç\n≈örednia liczba zakup√≥w: 34\n≈öredni koszt zakupu: 332 z≈Ç\nTyp: üåü PREMIUM CUSTOMERS\n\nKLASTER 1:\nLiczba klient√≥w: 725 (72.5%)\n≈öredni wiek: 44 lat\n≈öredni doch√≥d: 37808 z≈Ç\n≈örednie wydatki: 11486 z≈Ç\n≈örednia liczba zakup√≥w: 20\n≈öredni koszt zakupu: 155 z≈Ç\nTyp: üîÑ ≈öREDNI KLIENCI\n\n==================================================\nPOR√ìWNANIE Z RZECZYWISTYMI TYPAMI:\ntyp_klienta  average  bargain  premium  sporadic\nklaster                                         \n0                109        0      166         0\n1                228      344        0       153\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Analiza klastr√≥w klient√≥w](05-kmeans_files/figure-html/cell-11-output-2.png){width=1142 height=759}\n:::\n:::\n\n\n:::\n\n---\n\n## üéØ Praktyczne zastosowania K-Means\n\n### 1) **Marketing - personalizacja kampanii**\n\n::: {#b2e5c25c .cell execution_count=11}\n``` {.python .cell-code}\n# Strategia marketingowa na podstawie klastr√≥w\ndef marketing_strategy(cluster_id, cluster_data):\n    avg_income = cluster_data['roczny_dochod'].mean()\n    avg_spending = cluster_data['wydatki_roczne'].mean()\n    avg_frequency = cluster_data['liczba_zakupow'].mean()\n    \n    if avg_income > 60000 and avg_spending > 30000:\n        return {\n            'segment': 'Premium Customers',\n            'strategy': 'Produkty luksusowe, VIP program, personal shopping',\n            'budget_allocation': '40%',\n            'channels': 'Email premium, personal calls, exclusive events'\n        }\n    elif avg_frequency < 10 and avg_spending < 10000:\n        return {\n            'segment': 'Sporadic Customers', \n            'strategy': 'Reaktywacja, oferty specjalne, przypomnienia',\n            'budget_allocation': '15%',\n            'channels': 'SMS, push notifications, retargeting ads'\n        }\n    elif avg_spending / avg_income < 0.3 and avg_frequency > 20:\n        return {\n            'segment': 'Bargain Hunters',\n            'strategy': 'Promocje, wyprzeda≈ºe, programy lojalno≈õciowe',\n            'budget_allocation': '25%', \n            'channels': 'Newsletter z promocjami, social media deals'\n        }\n    else:\n        return {\n            'segment': 'Average Customers',\n            'strategy': 'Standardowe produkty, cross-selling, up-selling',\n            'budget_allocation': '20%',\n            'channels': 'Email marketing, social media, display ads'\n        }\n\nprint(\"STRATEGIA MARKETINGOWA DLA KA≈ªDEGO KLASTRA:\")\nprint(\"=\" * 60)\n\nfor cluster_id in range(best_k):\n    cluster_data = data[data['klaster'] == cluster_id]\n    strategy = marketing_strategy(cluster_id, cluster_data)\n    \n    print(f\"\\nKLASTER {cluster_id}: {strategy['segment']}\")\n    print(f\"Strategia: {strategy['strategy']}\")\n    print(f\"Bud≈ºet: {strategy['budget_allocation']}\")\n    print(f\"Kana≈Çy: {strategy['channels']}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º strategiƒô marketingowƒÖ\n\n::: {#d185501d .cell execution_count=12}\n\n::: {.cell-output .cell-output-stdout}\n```\nSTRATEGIA MARKETINGOWA DLA KA≈ªDEGO KLASTRA:\n============================================================\n\nKLASTER 0: Premium Customers\nStrategia: Produkty luksusowe, VIP program, personal shopping\nBud≈ºet: 40%\nKana≈Çy: Email premium, personal calls, exclusive events\n\nKLASTER 1: Average Customers\nStrategia: Standardowe produkty, cross-selling, up-selling\nBud≈ºet: 20%\nKana≈Çy: Email marketing, social media, display ads\n```\n:::\n:::\n\n\n:::\n\n### 2) **Inne bran≈ºe**\n\n::: {#13d34618 .cell execution_count=13}\n``` {.python .cell-code}\n# Healthcare - grupowanie pacjent√≥w\nhealthcare_features = ['wiek', 'BMI', 'ci≈õnienie', 'cholesterol', 'aktywno≈õƒá_fizyczna']\n# Wynik: programy profilaktyczne dostosowane do grup ryzyka\n\n# Finanse - portfolio management  \nfinance_features = ['doch√≥d', 'tolerancja_ryzyka', 'horyzont_inwestycji', 'do≈õwiadczenie']\n# Wynik: personalizowane porady inwestycyjne\n\n# Retail - optymalizacja sklep√≥w\nretail_features = ['lokalizacja', 'demografia', 'konkurencja', 'ruch_pieszy']  \n# Wynik: optymalne rozmieszczenie produkt√≥w w r√≥≈ºnych lokalizacjach\n```\n:::\n\n\n---\n\n## ‚öôÔ∏è Tuning parametr√≥w K-Means\n\n::: {#ed94c82f .cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\n\n# 1) R√≥≈ºne metody inicjalizacji\ninit_methods = ['k-means++', 'random']\nresults = {}\n\nfor init_method in init_methods:\n    kmeans = KMeans(n_clusters=best_k, init=init_method, n_init=10, random_state=42)\n    labels = kmeans.fit_predict(X_scaled)\n    sil_score = silhouette_score(X_scaled, labels)\n    results[init_method] = sil_score\n\nprint(\"Por√≥wnanie metod inicjalizacji:\")\nfor method, score in results.items():\n    print(f\"{method}: Silhouette = {score:.3f}\")\n\n# 2) Wp≈Çyw liczby inicjalizacji\nn_init_values = [1, 5, 10, 20]\nfor n_init in n_init_values:\n    kmeans = KMeans(n_clusters=best_k, n_init=n_init, random_state=42)\n    start_time = pd.Timestamp.now()\n    kmeans.fit(X_scaled)\n    end_time = pd.Timestamp.now()\n    duration = (end_time - start_time).total_seconds()\n    sil_score = silhouette_score(X_scaled, kmeans.labels_)\n    print(f\"n_init={n_init}: Silhouette={sil_score:.3f}, Czas={duration:.2f}s\")\n\n# 3) R√≥≈ºne algorytmy\nalgorithms = ['lloyd', 'elkan']  # 'auto' wybiera automatycznie\nfor algorithm in algorithms:\n    try:\n        kmeans = KMeans(n_clusters=best_k, algorithm=algorithm, random_state=42)\n        start_time = pd.Timestamp.now()\n        kmeans.fit(X_scaled)\n        end_time = pd.Timestamp.now()\n        duration = (end_time - start_time).total_seconds()\n        print(f\"Algorytm {algorithm}: Czas={duration:.2f}s\")\n    except:\n        print(f\"Algorytm {algorithm}: Niedostƒôpny w tej wersji\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki tuningu\n\n::: {#8b57bc89 .cell execution_count=15}\n\n::: {.cell-output .cell-output-stdout}\n```\nPor√≥wnanie metod inicjalizacji:\nk-means++: Silhouette = 0.281\nrandom: Silhouette = 0.281\nn_init=1: Silhouette=0.280, Czas=0.00s\nn_init=5: Silhouette=0.281, Czas=0.00s\nn_init=10: Silhouette=0.281, Czas=0.01s\nn_init=20: Silhouette=0.281, Czas=0.03s\n```\n:::\n:::\n\n\n:::\n\n---\n\n## ‚ö†Ô∏è Pu≈Çapki i rozwiƒÖzania\n\n### 1) **Brak standaryzacji danych**\n\n::: {#87d78fd5 .cell execution_count=16}\n``` {.python .cell-code}\n# Problem: r√≥≈ºne skale zmiennych\nprint(\"PROBLEM: Bez standaryzacji\")\nprint(\"Doch√≥d (tysiƒÖce): 20-80\")  \nprint(\"Wiek (lata): 18-70\")\nprint(\"‚Üí K-Means bƒôdzie skupiaƒá siƒô g≈Ç√≥wnie na dochodzie!\")\n\n# Demonstracja\nkmeans_unscaled = KMeans(n_clusters=3, random_state=42)\nlabels_unscaled = kmeans_unscaled.fit_predict(X[['roczny_dochod', 'wiek']])\n\nkmeans_scaled = KMeans(n_clusters=3, random_state=42)  \nX_subset_scaled = scaler.fit_transform(X[['roczny_dochod', 'wiek']])\nlabels_scaled = kmeans_scaled.fit_predict(X_subset_scaled)\n\nsil_unscaled = silhouette_score(X[['roczny_dochod', 'wiek']], labels_unscaled)\nsil_scaled = silhouette_score(X_subset_scaled, labels_scaled)\n\nprint(f\"\\nWyniki:\")\nprint(f\"Bez standaryzacji: Silhouette = {sil_unscaled:.3f}\")\nprint(f\"Ze standaryzacjƒÖ: Silhouette = {sil_scaled:.3f}\")\nprint(\"‚úÖ Standaryzacja ZAWSZE poprawia wyniki!\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º problem standaryzacji\n\n::: {#a91ca490 .cell execution_count=17}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nWyniki:\nBez standaryzacji: Silhouette = 0.562\nZe standaryzacjƒÖ: Silhouette = 0.406\n‚úÖ Standaryzacja ZAWSZE poprawia wyniki!\n```\n:::\n:::\n\n\n:::\n\n### 2) **Outliers - warto≈õci odstajƒÖce**\n\n::: {#afa5448f .cell execution_count=18}\n``` {.python .cell-code}\n# Problem: outliers zak≈Ç√≥cajƒÖ centroidy klastr√≥w\nfrom sklearn.preprocessing import RobustScaler\n\n# Dodaj kilka outlier√≥w\ndata_with_outliers = data.copy()\ndata_with_outliers.loc[0, 'roczny_dochod'] = 500000  # milioner!\ndata_with_outliers.loc[1, 'wydatki_roczne'] = 200000  # mega wydatki\n\nX_outliers = data_with_outliers[features]\n\n# Por√≥wnaj r√≥≈ºne skalery\nscalers = {\n    'StandardScaler': StandardScaler(),\n    'RobustScaler': RobustScaler()  # odporny na outliers\n}\n\nprint(\"Wp≈Çyw outliers:\")\nfor scaler_name, scaler_obj in scalers.items():\n    X_scaled_comp = scaler_obj.fit_transform(X_outliers)\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    labels = kmeans.fit_predict(X_scaled_comp)\n    sil_score = silhouette_score(X_scaled_comp, labels)\n    print(f\"{scaler_name}: Silhouette = {sil_score:.3f}\")\n\nprint(\"\\n‚úÖ RobustScaler lepiej radzi sobie z outliers!\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wp≈Çyw outliers\n\n::: {#7be8f44f .cell execution_count=19}\n\n::: {.cell-output .cell-output-stdout}\n```\nWp≈Çyw outliers:\nStandardScaler: Silhouette = 0.193\nRobustScaler: Silhouette = 0.246\n\n‚úÖ RobustScaler lepiej radzi sobie z outliers!\n```\n:::\n:::\n\n\n:::\n\n### 3) **Curse of dimensionality**\n\n::: {#4495b44e .cell execution_count=20}\n``` {.python .cell-code}\n# Problem: za du≈ºo wymiar√≥w\nfrom sklearn.decomposition import PCA\n\nprint(\"Problem wielu wymiar√≥w:\")\ndimensions = [2, 5, 10, 20, 50]\n\nfor n_dims in dimensions:\n    if n_dims <= X_scaled.shape[1]:\n        X_subset = X_scaled[:, :n_dims]\n    else:\n        # Dodaj sztuczne wymiary\n        extra_dims = np.random.randn(X_scaled.shape[0], n_dims - X_scaled.shape[1])\n        X_subset = np.hstack([X_scaled, extra_dims])\n    \n    kmeans = KMeans(n_clusters=3, random_state=42)\n    labels = kmeans.fit_predict(X_subset)\n    \n    if len(np.unique(labels)) > 1:  # sprawd≈∫ czy sƒÖ r√≥≈ºne klastry\n        sil_score = silhouette_score(X_subset, labels)\n        print(f\"{n_dims} wymiar√≥w: Silhouette = {sil_score:.3f}\")\n    else:\n        print(f\"{n_dims} wymiar√≥w: Wszystkie punkty w jednym klastrze!\")\n\nprint(\"\\nRozwiƒÖzanie: PCA dimensionality reduction\")\npca = PCA(n_components=0.95)  # zachowaj 95% wariancji\nX_pca = pca.fit_transform(X_scaled)\nprint(f\"Redukcja z {X_scaled.shape[1]} do {X_pca.shape[1]} wymiar√≥w\")\n\nkmeans_pca = KMeans(n_clusters=3, random_state=42)\nlabels_pca = kmeans_pca.fit_predict(X_pca)\nsil_pca = silhouette_score(X_pca, labels_pca)\nprint(f\"Po PCA: Silhouette = {sil_pca:.3f}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º problem wymiarowo≈õci\n\n::: {#27298a09 .cell execution_count=21}\n\n::: {.cell-output .cell-output-stdout}\n```\nProblem wielu wymiar√≥w:\n2 wymiar√≥w: Silhouette = 0.406\n5 wymiar√≥w: Silhouette = 0.285\n10 wymiar√≥w: Silhouette = 0.133\n20 wymiar√≥w: Silhouette = 0.055\n50 wymiar√≥w: Silhouette = 0.024\n\nRozwiƒÖzanie: PCA dimensionality reduction\nRedukcja z 6 do 6 wymiar√≥w\nPo PCA: Silhouette = 0.233\n```\n:::\n:::\n\n\n:::\n\n---\n\n## üåç Real-world przypadki u≈ºycia\n\n1. **E-commerce:** Segmentacja klient√≥w, personalizacja, dynamic pricing\n2. **Marketing:** Customer personas, campaign optimization, market research  \n3. **Finance:** Portfolio optimization, risk assessment, fraud detection\n4. **Healthcare:** Patient stratification, treatment personalization, drug discovery\n5. **Operations:** Supply chain optimization, demand forecasting, quality control\n\n::: {.callout-tip}\n## üí° Kiedy u≈ºywaƒá K-Means?\n\n**‚úÖ U≈ªYJ GDY:**\n\n- Chcesz odkryƒá ukryte grupy w danych bez etykiet\n- Dane majƒÖ podobne gƒôsto≈õci i sƒÖ \"okrƒÖg≈Çe\" (sferyczne klastry)\n- Potrzebujesz szybkiego i skalowalnego algorytmu\n- Wiesz w przybli≈ºeniu ile mo≈ºe byƒá grup (K)\n- Chcesz segmentowaƒá klient√≥w, produkty, rynki\n\n**‚ùå NIE U≈ªYWAJ GDY:**\n\n- Klastry majƒÖ r√≥≈ºne rozmiary lub gƒôsto≈õci (u≈ºyj DBSCAN)\n- Klastry majƒÖ nieregularne kszta≈Çty (u≈ºyj Hierarchical Clustering)\n- Nie wiesz wcale ile mo≈ºe byƒá grup (u≈ºyj DBSCAN/HDBSCAN)\n- Masz kategoryczne zmienne (u≈ºyj K-Modes)\n- Dane majƒÖ du≈ºo outliers (u≈ºyj DBSCAN)\n:::\n\n**Nastƒôpna ≈õciƒÖgawka:** Support Vector Machines - znajdowanie optymalnych granic! üéØ\n\n",
    "supporting": [
      "05-kmeans_files"
    ],
    "filters": [],
    "includes": {}
  }
}