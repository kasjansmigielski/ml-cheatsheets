{
  "hash": "b9e40ab3e2439b54c8e4ff88931d08b5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Linear Regression ‚Äî przewiduj warto≈õci liczbowe\"\nformat:\n  html:\n    code-tools: true\n---\n\n## üìà Czym jest Linear Regression?\n\n**Linear Regression** to jeden z najprostszych i najczƒô≈õciej u≈ºywanych algorytm√≥w ML do **przewidywania warto≈õci liczbowych** (np. ceny, temperatury, sprzeda≈ºy). Szuka najlepszej linii prostej, kt√≥ra opisuje zale≈ºno≈õƒá miƒôdzy zmiennymi.\n\n::: {.callout-note}\n## üí° Intuicja matematyczna\nSzukamy takiej linii `y = ax + b`, kt√≥ra najlepiej \"przechodzi\" przez nasze punkty danych. Algorytm automatycznie dobiera optymalne warto≈õci `a` (slope) i `b` (intercept).\n:::\n\n---\n\n## üè† Praktyczny przyk≈Çad: predykcja cen mieszka≈Ñ\n\n::: {#d393d578 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Tworzenie przyk≈Çadowych danych mieszka≈Ñ\nnp.random.seed(42)\nn_mieszkan = 1000\n\ndata = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_mieszkan),\n    'liczba_pokoi': np.random.randint(1, 5, n_mieszkan),\n    'wiek_budynku': np.random.randint(0, 50, n_mieszkan),\n    'odleglosc_centrum': np.random.exponential(5, n_mieszkan)\n})\n\n# Realistyczna formu≈Ça ceny (z szumem)\ndata['cena'] = (\n    data['powierzchnia'] * 8000 +           # 8k za m¬≤\n    data['liczba_pokoi'] * 15000 +          # 15k za pok√≥j\n    (50 - data['wiek_budynku']) * 2000 +    # starsze = ta≈Ñsze\n    data['odleglosc_centrum'] * (-3000) +   # dalej = ta≈Ñsze\n    np.random.normal(0, 50000, n_mieszkan)  # szum losowy\n)\n\nprint(\"Podstawowe statystyki:\")\nprint(data.describe())\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º statystyki\n\n::: {#da759dc5 .cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\nPodstawowe statystyki:\n       powierzchnia  liczba_pokoi  wiek_budynku  odleglosc_centrum  \\\ncount   1000.000000   1000.000000   1000.000000        1000.000000   \nmean      70.483301      2.473000     25.049000           4.971457   \nstd       24.480398      1.128958     14.384001           4.883716   \nmin      -11.031684      1.000000      0.000000           0.000058   \n25%       53.810242      1.000000     13.000000           1.459988   \n50%       70.632515      2.000000     25.000000           3.505541   \n75%       86.198597      4.000000     38.000000           6.954361   \nmax      166.318287      4.000000     49.000000          34.028756   \n\n               cena  \ncount  1.000000e+03  \nmean   6.363187e+05  \nstd    2.004394e+05  \nmin    1.686504e+04  \n25%    5.049745e+05  \n50%    6.356690e+05  \n75%    7.647808e+05  \nmax    1.440259e+06  \n```\n:::\n:::\n\n\n:::\n\n---\n\n## üîß Trenowanie modelu krok po kroku\n\n### 1) Przygotowanie danych\n\n::: {#90689e66 .cell execution_count=3}\n``` {.python .cell-code}\n# Sprawdzenie korelacji - kt√≥re zmienne sƒÖ najwa≈ºniejsze?\ncorrelation = data.corr()['cena'].sort_values(ascending=False)\n\n# Podzia≈Ç na features (X) i target (y)\nX = data[['powierzchnia', 'liczba_pokoi', 'wiek_budynku', 'odleglosc_centrum']]\ny = data['cena']\n\n# Podzia≈Ç train/test (80/20)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"Korelacja z cenƒÖ:\")\nprint(correlation)\n\nprint(f\"Dane treningowe: {X_train.shape[0]} mieszka≈Ñ\")\nprint(f\"Dane testowe: {X_test.shape[0]} mieszka≈Ñ\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º korelacje oraz dane treningowe\n\n::: {#3bb9e4e4 .cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\nKorelacja z cenƒÖ:\ncena                 1.000000\npowierzchnia         0.949080\nliczba_pokoi         0.025094\nodleglosc_centrum   -0.042040\nwiek_budynku        -0.115566\nName: cena, dtype: float64\nDane treningowe: 800 mieszka≈Ñ\nDane testowe: 200 mieszka≈Ñ\n```\n:::\n:::\n\n\n:::\n\n### 2) Trenowanie modelu\n\n::: {#213f5f9b .cell execution_count=5}\n``` {.python .cell-code}\n# Utworzenie i trenowanie modelu\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Sprawdzenie wsp√≥≈Çczynnik√≥w\nprint(\"Wsp√≥≈Çczynniki modelu:\")\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"{feature}: {coef:.0f} z≈Ç\")\nprint(f\"Intercept (sta≈Ça): {model.intercept_:.0f} z≈Ç\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wsp√≥≈Çczynniki\n\n::: {#ba48d79b .cell execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\nWsp√≥≈Çczynniki modelu:\npowierzchnia: 7924 z≈Ç\nliczba_pokoi: 15811 z≈Ç\nwiek_budynku: -2108 z≈Ç\nodleglosc_centrum: -2698 z≈Ç\nIntercept (sta≈Ça): 104042 z≈Ç\n```\n:::\n:::\n\n\n:::\n\n### 3) Ewaluacja wynik√≥w\n\n::: {#151e9b92 .cell execution_count=7}\n``` {.python .cell-code}\n# Predykcje na zbiorze testowym\ny_pred = model.predict(X_test)\n\n# Metryki\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"\\nWyniki modelu:\")\nprint(f\"Mean Absolute Error: {mae:.0f} z≈Ç\")\nprint(f\"R¬≤ Score: {r2:.3f}\")\nprint(f\"≈öredni b≈ÇƒÖd to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania\")\n\n# Przyk≈Çadowe predykcje\nfor i in range(5):\n    rzeczywista = y_test.iloc[i]\n    przewidywana = y_pred[i]\n    b≈ÇƒÖd = abs(rzeczywista - przewidywana)\n    print(f\"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}z≈Ç, \"\n          f\"przewidywana={przewidywana:.0f}z≈Ç, b≈ÇƒÖd={b≈ÇƒÖd:.0f}z≈Ç\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wsp√≥≈Çczynniki\n\n::: {#9b3a4596 .cell execution_count=8}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nWyniki modelu:\nMean Absolute Error: 38912 z≈Ç\nR¬≤ Score: 0.934\n≈öredni b≈ÇƒÖd to 6.1% ceny mieszkania\nMieszkanie 1: rzeczywista=702313z≈Ç, przewidywana=771489z≈Ç, b≈ÇƒÖd=69177z≈Ç\nMieszkanie 2: rzeczywista=791174z≈Ç, przewidywana=816756z≈Ç, b≈ÇƒÖd=25582z≈Ç\nMieszkanie 3: rzeczywista=326702z≈Ç, przewidywana=274297z≈Ç, b≈ÇƒÖd=52405z≈Ç\nMieszkanie 4: rzeczywista=441380z≈Ç, przewidywana=456054z≈Ç, b≈ÇƒÖd=14674z≈Ç\nMieszkanie 5: rzeczywista=445427z≈Ç, przewidywana=383199z≈Ç, b≈ÇƒÖd=62228z≈Ç\n```\n:::\n:::\n\n\n:::\n\n---\n\n## üìä Wizualizacja wynik√≥w\n\n::: {#c87e70ed .cell execution_count=9}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Przyk≈Çadowe dane mieszka≈Ñ (powtarzamy dla demonstracji)\nnp.random.seed(42)\nn_mieszkan = 1000\n\ndata = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_mieszkan),\n    'liczba_pokoi': np.random.randint(1, 5, n_mieszkan),\n    'wiek_budynku': np.random.randint(0, 50, n_mieszkan),\n    'odleglosc_centrum': np.random.exponential(5, n_mieszkan)\n})\n\ndata['cena'] = (\n    data['powierzchnia'] * 8000 +\n    data['liczba_pokoi'] * 15000 +\n    (50 - data['wiek_budynku']) * 2000 +\n    data['odleglosc_centrum'] * (-3000) +\n    np.random.normal(0, 50000, n_mieszkan)\n)\n\n# Przygotowanie i trenowanie modelu\nX = data[['powierzchnia', 'liczba_pokoi', 'wiek_budynku', 'odleglosc_centrum']]\ny = data['cena']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Oblicz metryki\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Przygotuj wykresy\nplt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Rzeczywiste ceny')\nplt.ylabel('Przewidywane ceny')\nplt.title('Rzeczywiste vs Przewidywane')\n\nplt.subplot(1, 2, 2)\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Przewidywane ceny')\nplt.ylabel('Residuals (b≈Çƒôdy)')\nplt.title('Analiza residuals')\n\nplt.tight_layout()\n\n# Zapisz wykres do zmiennej\nimport io\nimport base64\nbuf = io.BytesIO()\nplt.savefig(buf, format='png', dpi=100, bbox_inches='tight')\nbuf.seek(0)\nplt.close()\n\n# Przygotuj przyk≈Çadowe predykcje\nprzykladowe_predykcje = []\nfor i in range(3):\n    rzeczywista = y_test.iloc[i]\n    przewidywana = y_pred[i]\n    blad = abs(rzeczywista - przewidywana)\n    przykladowe_predykcje.append(f\"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}z≈Ç, przewidywana={przewidywana:.0f}z≈Ç, b≈ÇƒÖd={blad:.0f}z≈Ç\")\n\nprint(f\"Wyniki modelu Linear Regression:\")\nprint(f\"Mean Absolute Error: {mae:.0f} z≈Ç\")\nprint(f\"R¬≤ Score: {r2:.3f}\")\nprint(f\"≈öredni b≈ÇƒÖd to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania\")\nprint(f\"\\nPrzyk≈Çadowe predykcje:\")\nfor pred in przykladowe_predykcje:\n    print(pred)\n\n# Odtw√≥rz wykres\nplt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Rzeczywiste ceny')\nplt.ylabel('Przewidywane ceny')\nplt.title('Rzeczywiste vs Przewidywane')\n\nplt.subplot(1, 2, 2)\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Przewidywane ceny')\nplt.ylabel('Residuals (b≈Çƒôdy)')\nplt.title('Analiza residuals')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki i wykresy analizy\n\n::: {#d160629f .cell fig-height='6' fig-width='10' execution_count=10}\n\n::: {.cell-output .cell-output-stdout}\n```\nWyniki modelu Linear Regression:\nMean Absolute Error: 38912 z≈Ç\nR¬≤ Score: 0.934\n≈öredni b≈ÇƒÖd to 6.1% ceny mieszkania\n\nPrzyk≈Çadowe predykcje:\nMieszkanie 1: rzeczywista=702313z≈Ç, przewidywana=771489z≈Ç, b≈ÇƒÖd=69177z≈Ç\nMieszkanie 2: rzeczywista=791174z≈Ç, przewidywana=816756z≈Ç, b≈ÇƒÖd=25582z≈Ç\nMieszkanie 3: rzeczywista=326702z≈Ç, przewidywana=274297z≈Ç, b≈ÇƒÖd=52405z≈Ç\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Analiza wynik√≥w modelu Linear Regression](02-linear-regression_files/figure-html/cell-11-output-2.png){width=950 height=566}\n:::\n:::\n\n\n:::\n\n---\n\n## üéØ Praktyczne zastosowania Linear Regression\n\n### 1) **Biznesowe**\n\n::: {#08d7105e .cell execution_count=11}\n``` {.python .cell-code}\n# Przewidywanie sprzeda≈ºy na podstawie bud≈ºetu marketingowego\nsales_data = pd.DataFrame({\n    'marketing_budget': [10000, 15000, 20000, 25000, 30000],\n    'sales': [100000, 140000, 180000, 220000, 260000]\n})\n\nmodel_sales = LinearRegression()\nmodel_sales.fit(sales_data[['marketing_budget']], sales_data['sales'])\n\n# \"Je≈õli zwiƒôkszƒô bud≈ºet do 35k, sprzeda≈º wzro≈õnie do:\"\nnew_sales = model_sales.predict([[35000]])\nprint(f\"Przewidywana sprzeda≈º przy bud≈ºecie 35k: {new_sales[0]:.0f}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º przewidywanƒÖ sprzeda≈º\n\n::: {#7d8c3e31 .cell execution_count=12}\n\n::: {.cell-output .cell-output-stdout}\n```\nPrzewidywana sprzeda≈º przy bud≈ºecie 35k: 300000\n```\n:::\n:::\n\n\n:::\n\n### 2) **Analizy finansowe**\n\n::: {#020cf24d .cell execution_count=13}\n``` {.python .cell-code}\n# Relacja miƒôdzy PKB a konsumpcjƒÖ\neconomics_data = pd.DataFrame({\n    'pkb_per_capita': [25000, 30000, 35000, 40000, 45000],\n    'konsumpcja': [18000, 21000, 24000, 27000, 30000]\n})\n\nmodel_econ = LinearRegression()\nmodel_econ.fit(economics_data[['pkb_per_capita']], economics_data['konsumpcja'])\n\n# Wsp√≥≈Çczynnik sk≈Çonno≈õci do konsumpcji\nprint(f\"Na ka≈ºde dodatkowe 1000z≈Ç PKB, konsumpcja ro≈õnie o: {model_econ.coef_[0]:.0f}z≈Ç\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wsp√≥≈Çczynnik sk≈Çonno≈õci do konsumpcji\n\n::: {#cdf1a029 .cell execution_count=14}\n\n::: {.cell-output .cell-output-stdout}\n```\nNa ka≈ºde dodatkowe 1000z≈Ç PKB, konsumpcja ro≈õnie o: 1z≈Ç\n```\n:::\n:::\n\n\n:::\n\n---\n\n## ‚ö†Ô∏è Najczƒôstsze pu≈Çapki i jak ich unikaƒá\n\n### 1) **Overfitting z wieloma zmiennymi**\n\n::: {#fef82d42 .cell execution_count=15}\n``` {.python .cell-code}\n# Z≈ÅO: za du≈ºo features wzglƒôdem danych\n# Je≈õli masz 100 mieszka≈Ñ, nie u≈ºywaj 50 features!\n\n# DOBRZE: zasada kciuka\ndef check_features_ratio(X, y):\n    ratio = len(y) / X.shape[1]\n    if ratio < 10:\n        print(f\"‚ö†Ô∏è Uwaga: masz tylko {ratio:.1f} obserwacji na feature!\")\n        print(\"Rozwa≈º: wiƒôcej danych lub mniej features\")\n    else:\n        print(f\"‚úÖ OK: {ratio:.1f} obserwacji na feature\")\n\ncheck_features_ratio(X_train, y_train)\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki\n\n::: {#1f419359 .cell execution_count=16}\n\n::: {.cell-output .cell-output-stdout}\n```\n‚úÖ OK: 200.0 obserwacji na feature\n```\n:::\n:::\n\n\n:::\n\n### 2) **Sprawdzanie za≈Ço≈ºe≈Ñ linearno≈õci**\n\n::: {#0fed6089 .cell execution_count=17}\n``` {.python .cell-code}\n# Sprawd≈∫ czy zale≈ºno≈õci sƒÖ rzeczywi≈õcie liniowe\nfrom scipy import stats\n\nfor col in X.columns:\n    correlation, p_value = stats.pearsonr(data[col], data['cena'])\n    print(f\"{col}: korelacja={correlation:.3f}, p-value={p_value:.3f}\")\n    \n    if abs(correlation) < 0.1:\n        print(f\"‚ö†Ô∏è {col} ma s≈ÇabƒÖ korelacjƒô - mo≈ºe nie byƒá przydatna\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki korelacji\n\n::: {#22296916 .cell execution_count=18}\n\n::: {.cell-output .cell-output-stdout}\n```\npowierzchnia: korelacja=0.949, p-value=0.000\nliczba_pokoi: korelacja=0.025, p-value=0.428\n‚ö†Ô∏è liczba_pokoi ma s≈ÇabƒÖ korelacjƒô - mo≈ºe nie byƒá przydatna\nwiek_budynku: korelacja=-0.116, p-value=0.000\nodleglosc_centrum: korelacja=-0.042, p-value=0.184\n‚ö†Ô∏è odleglosc_centrum ma s≈ÇabƒÖ korelacjƒô - mo≈ºe nie byƒá przydatna\n```\n:::\n:::\n\n\n:::\n\n### 3) **Wielokoliniowo≈õƒá** (features korelujƒÖ miƒôdzy sobƒÖ)\n\n::: {#4a8003d3 .cell execution_count=19}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Przyk≈Çadowe dane features (demonstracja)\nnp.random.seed(42)\nn_samples = 1000\n\nX = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_samples),\n    'liczba_pokoi': np.random.randint(1, 5, n_samples),\n    'wiek_budynku': np.random.randint(0, 50, n_samples),\n    'odleglosc_centrum': np.random.exponential(5, n_samples)\n})\n\n# Oblicz korelacje miƒôdzy features\ncorrelation_matrix = X.corr()\n\n# Przygotuj wykres\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Korelacje miƒôdzy features')\nplt.close()\n\nprint(\"Macierz korelacji miƒôdzy zmiennymi:\")\nprint(correlation_matrix)\nprint(\"\\nInterpretacja:\")\nprint(\"‚Ä¢ Warto≈õci blisko 1.0 = silna korelacja pozytywna\")\nprint(\"‚Ä¢ Warto≈õci blisko -1.0 = silna korelacja negatywna\") \nprint(\"‚Ä¢ Warto≈õci blisko 0.0 = brak korelacji\")\nprint(\"\\nJe≈õli korelacja miƒôdzy features > 0.8, usu≈Ñ jednƒÖ z nich (multicollinearity)!\")\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Korelacje miƒôdzy features')\nplt.show()\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º mapƒô korelacji\n\n::: {#c456b5dd .cell fig-height='6' fig-width='8' execution_count=20}\n\n::: {.cell-output .cell-output-stdout}\n```\nMacierz korelacji miƒôdzy zmiennymi:\n                   powierzchnia  liczba_pokoi  wiek_budynku  odleglosc_centrum\npowierzchnia           1.000000     -0.064763      0.033920           0.029856\nliczba_pokoi          -0.064763      1.000000     -0.003894          -0.057555\nwiek_budynku           0.033920     -0.003894      1.000000          -0.037242\nodleglosc_centrum      0.029856     -0.057555     -0.037242           1.000000\n\nInterpretacja:\n‚Ä¢ Warto≈õci blisko 1.0 = silna korelacja pozytywna\n‚Ä¢ Warto≈õci blisko -1.0 = silna korelacja negatywna\n‚Ä¢ Warto≈õci blisko 0.0 = brak korelacji\n\nJe≈õli korelacja miƒôdzy features > 0.8, usu≈Ñ jednƒÖ z nich (multicollinearity)!\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Mapa korelacji miƒôdzy zmiennymi](02-linear-regression_files/figure-html/cell-21-output-2.png){width=713 height=505}\n:::\n:::\n\n\n:::\n\n# Je≈õli korelacja miƒôdzy features > 0.8, usu≈Ñ jednƒÖ z nich!\n\n---\n\n## üîß Parametry do tuningu\n\n::: {#3c70f817 .cell execution_count=21}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n# 1) Standardization - gdy features majƒÖ r√≥≈ºne skale\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train)\n\n# 2) Polynomial Features - dla nieliniowych zale≈ºno≈õci\npoly_model = Pipeline([\n    ('poly', PolynomialFeatures(degree=2)),\n    ('linear', LinearRegression())\n])\npoly_model.fit(X_train, y_train)\n\npoly_pred = poly_model.predict(X_test)\npoly_r2 = r2_score(y_test, poly_pred)\nprint(f\"Polynomial Regression R¬≤: {poly_r2:.3f}\")\n\n# 3) Regularization - Ridge i Lasso\nfrom sklearn.linear_model import Ridge, Lasso\n\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\nridge_r2 = r2_score(y_test, ridge.predict(X_test))\n\nlasso = Lasso(alpha=1.0)\nlasso.fit(X_train, y_train)\nlasso_r2 = r2_score(y_test, lasso.predict(X_test))\n\nprint(f\"Ridge R¬≤: {ridge_r2:.3f}\")\nprint(f\"Lasso R¬≤: {lasso_r2:.3f}\")\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Poka≈º wyniki predykcji cen\n\n::: {#e3b76714 .cell execution_count=22}\n\n::: {.cell-output .cell-output-stdout}\n```\nPolynomial Regression R¬≤: 0.933\nRidge R¬≤: 0.934\nLasso R¬≤: 0.934\n```\n:::\n:::\n\n\n:::\n\n---\n\n## üåç Real-world przyk≈Çady zastosowa≈Ñ\n\n1. **E-commerce:** Przewidywanie warto≈õci ≈ºyciowej klienta (LTV)\n2. **Nieruchomo≈õci:** Automatyczna wycena dom√≥w (Zillow)\n3. **Finanse:** Scoring kredytowy, przewidywanie cen akcji\n4. **Marketing:** ROI kampanii reklamowych\n5. **Supply Chain:** Prognozowanie popytu na produkty\n\n::: {.callout-tip}\n## üí° Kiedy u≈ºywaƒá Linear Regression?\n\n**‚úÖ U≈ªYJ GDY:**\n\n- Przewidujesz warto≈õci liczbowe (continuous target)\n- Zale≈ºno≈õci wydajƒÖ siƒô liniowe\n- Chcesz interpretowalny model\n- Masz stosunkowo ma≈Ço features\n\n**‚ùå NIE U≈ªYWAJ GDY:**\n\n- Target jest kategoryczny (u≈ºyj klasyfikacji)\n- Zale≈ºno≈õci sƒÖ bardzo nieliniowe (u≈ºyj Random Forest, XGBoost)\n- Masz tysiƒÖce features (u≈ºyj regularization)\n:::\n\n**Nastƒôpna ≈õciƒÖgawka:** [Decision Trees - klasyfikacja](03-decision-trees.qmd) üå≥\n\n",
    "supporting": [
      "02-linear-regression_files"
    ],
    "filters": [],
    "includes": {}
  }
}