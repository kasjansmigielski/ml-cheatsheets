[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Witaj w Å›wiecie Machine Learning!",
    "section": "",
    "text": "Witaj w praktycznej kolekcji Å›ciÄ…g z Machine Learning! Znajdziesz tutaj algorytmy, teoriÄ™ i real-world przykÅ‚ady przedstawione w przystÄ™pny sposÃ³b dla kursantÃ³w Data Science. KaÅ¼da Å›ciÄ…gawka to kompaktowa porcja wiedzy gotowa do zastosowania.\n\n\n\nWyjaÅ›nienie algorytmu - jak dziaÅ‚a i dlaczego\nPraktyczne przykÅ‚ady kodu gotowe do skopiowania i uruchomienia\nReal-world zastosowania - kiedy i gdzie uÅ¼yÄ‡\nKluczowe parametry - co moÅ¼na dostroiÄ‡ i jak\nTipsy od praktykÃ³w - na co uwaÅ¼aÄ‡ w rzeczywistych projektach\n\n\n\n\nNajlepsza metoda nauki:\n\nPrzeczytaj teoriÄ™ - zrozum podstawy algorytmu\nPrzeanalizuj przykÅ‚ad - zobacz jak teoria przekÅ‚ada siÄ™ na kod\n\nSkopiuj snippety - uÅ¼yj przyciska ğŸ“‹ Copy w prawym gÃ³rnym rogu\nTestuj w swoim Å›rodowisku - Jupyter, Colab, VS Code\nEksperymentuj z parametrami - zmieÅ„ wartoÅ›ci i obserwuj efekty!\n\n\n\n\n\n\n\nğŸ’¡ Pro tip dla kursantÃ³w\n\n\n\nNie tylko kopiuj kod - zrozum co robi kaÅ¼da linia. SprÃ³buj zmieniÄ‡ dane wejÅ›ciowe, parametry algorytmu lub dodaÄ‡ wÅ‚asne modyfikacje. To najlepszy sposÃ³b na naukÄ™ ML!\n\n\n\n\n\n\n\n\nWprowadzenie do ML - Czym jest ML i jakie sÄ… gÅ‚Ã³wne rodzaje algorytmÃ³w\n\n\n\n\n\nLinear Regression - Predykcja wartoÅ›ci liczbowych\n\n\n\n\n\nDecision Trees - Proste drzewa decyzyjne\n\n\n\n\n\n\n\nğŸ”¥ Kolejne Å›ciÄ…gi w przygotowaniu!\n\n\n\n\nK-Means Clustering (segmentacja klientÃ³w)\nRandom Forest (ensembles)\nLogistic Regression (klasyfikacja binarna)\nFeature Engineering (przygotowanie danych)\nModel Evaluation (metryki i walidacja)\n\nÅšledÅº aktualizacje!\n\n\n\n\n\n\n\nZacznij od podstaw - Wprowadzenie do ML\nWybierz algorytm ktÃ³ry CiÄ™ interesuje z menu ML Cheatsheets\n\nPrzeczytaj teoriÄ™ - zrozum jak dziaÅ‚a algorytm\nSkopiuj kod (ikona ğŸ“‹ Copy w prawym gÃ³rnym rogu bloku)\nTestuj w praktyce - uruchom w Jupyter/Colab i eksperymentuj!\n\n\nPowodzenia w nauce Machine Learning! ğŸš€ğŸ¤–"
  },
  {
    "objectID": "index.html#co-znajdziesz-w-kaÅ¼dej-Å›ciÄ…gawce",
    "href": "index.html#co-znajdziesz-w-kaÅ¼dej-Å›ciÄ…gawce",
    "title": "Witaj w Å›wiecie Machine Learning!",
    "section": "",
    "text": "WyjaÅ›nienie algorytmu - jak dziaÅ‚a i dlaczego\nPraktyczne przykÅ‚ady kodu gotowe do skopiowania i uruchomienia\nReal-world zastosowania - kiedy i gdzie uÅ¼yÄ‡\nKluczowe parametry - co moÅ¼na dostroiÄ‡ i jak\nTipsy od praktykÃ³w - na co uwaÅ¼aÄ‡ w rzeczywistych projektach"
  },
  {
    "objectID": "index.html#jak-korzystaÄ‡-z-ml-cheatsheets",
    "href": "index.html#jak-korzystaÄ‡-z-ml-cheatsheets",
    "title": "Witaj w Å›wiecie Machine Learning!",
    "section": "",
    "text": "Najlepsza metoda nauki:\n\nPrzeczytaj teoriÄ™ - zrozum podstawy algorytmu\nPrzeanalizuj przykÅ‚ad - zobacz jak teoria przekÅ‚ada siÄ™ na kod\n\nSkopiuj snippety - uÅ¼yj przyciska ğŸ“‹ Copy w prawym gÃ³rnym rogu\nTestuj w swoim Å›rodowisku - Jupyter, Colab, VS Code\nEksperymentuj z parametrami - zmieÅ„ wartoÅ›ci i obserwuj efekty!\n\n\n\n\n\n\n\nğŸ’¡ Pro tip dla kursantÃ³w\n\n\n\nNie tylko kopiuj kod - zrozum co robi kaÅ¼da linia. SprÃ³buj zmieniÄ‡ dane wejÅ›ciowe, parametry algorytmu lub dodaÄ‡ wÅ‚asne modyfikacje. To najlepszy sposÃ³b na naukÄ™ ML!"
  },
  {
    "objectID": "index.html#dostÄ™pne-Å›ciÄ…gi-ml",
    "href": "index.html#dostÄ™pne-Å›ciÄ…gi-ml",
    "title": "Witaj w Å›wiecie Machine Learning!",
    "section": "",
    "text": "Wprowadzenie do ML - Czym jest ML i jakie sÄ… gÅ‚Ã³wne rodzaje algorytmÃ³w\n\n\n\n\n\nLinear Regression - Predykcja wartoÅ›ci liczbowych\n\n\n\n\n\nDecision Trees - Proste drzewa decyzyjne\n\n\n\n\n\n\n\nğŸ”¥ Kolejne Å›ciÄ…gi w przygotowaniu!\n\n\n\n\nK-Means Clustering (segmentacja klientÃ³w)\nRandom Forest (ensembles)\nLogistic Regression (klasyfikacja binarna)\nFeature Engineering (przygotowanie danych)\nModel Evaluation (metryki i walidacja)\n\nÅšledÅº aktualizacje!"
  },
  {
    "objectID": "index.html#szybki-start",
    "href": "index.html#szybki-start",
    "title": "Witaj w Å›wiecie Machine Learning!",
    "section": "",
    "text": "Zacznij od podstaw - Wprowadzenie do ML\nWybierz algorytm ktÃ³ry CiÄ™ interesuje z menu ML Cheatsheets\n\nPrzeczytaj teoriÄ™ - zrozum jak dziaÅ‚a algorytm\nSkopiuj kod (ikona ğŸ“‹ Copy w prawym gÃ³rnym rogu bloku)\nTestuj w praktyce - uruchom w Jupyter/Colab i eksperymentuj!\n\n\nPowodzenia w nauce Machine Learning! ğŸš€ğŸ¤–"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html",
    "href": "cheatsheets/02-linear-regression.html",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "",
    "text": "Linear Regression to jeden z najprostszych i najczÄ™Å›ciej uÅ¼ywanych algorytmÃ³w ML do przewidywania wartoÅ›ci liczbowych (np. ceny, temperatury, sprzedaÅ¼y). Szuka najlepszej linii prostej, ktÃ³ra opisuje zaleÅ¼noÅ›Ä‡ miÄ™dzy zmiennymi.\n\n\n\n\n\n\nğŸ’¡ Intuicja matematyczna\n\n\n\nSzukamy takiej linii y = ax + b, ktÃ³ra najlepiej â€œprzechodziâ€ przez nasze punkty danych. Algorytm automatycznie dobiera optymalne wartoÅ›ci a (slope) i b (intercept)."
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#czym-jest-linear-regression",
    "href": "cheatsheets/02-linear-regression.html#czym-jest-linear-regression",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "",
    "text": "Linear Regression to jeden z najprostszych i najczÄ™Å›ciej uÅ¼ywanych algorytmÃ³w ML do przewidywania wartoÅ›ci liczbowych (np. ceny, temperatury, sprzedaÅ¼y). Szuka najlepszej linii prostej, ktÃ³ra opisuje zaleÅ¼noÅ›Ä‡ miÄ™dzy zmiennymi.\n\n\n\n\n\n\nğŸ’¡ Intuicja matematyczna\n\n\n\nSzukamy takiej linii y = ax + b, ktÃ³ra najlepiej â€œprzechodziâ€ przez nasze punkty danych. Algorytm automatycznie dobiera optymalne wartoÅ›ci a (slope) i b (intercept)."
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#praktyczny-przykÅ‚ad-predykcja-cen-mieszkaÅ„",
    "href": "cheatsheets/02-linear-regression.html#praktyczny-przykÅ‚ad-predykcja-cen-mieszkaÅ„",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "ğŸ  Praktyczny przykÅ‚ad: predykcja cen mieszkaÅ„",
    "text": "ğŸ  Praktyczny przykÅ‚ad: predykcja cen mieszkaÅ„\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Tworzenie przykÅ‚adowych danych mieszkaÅ„\nnp.random.seed(42)\nn_mieszkan = 1000\n\ndata = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_mieszkan),\n    'liczba_pokoi': np.random.randint(1, 5, n_mieszkan),\n    'wiek_budynku': np.random.randint(0, 50, n_mieszkan),\n    'odleglosc_centrum': np.random.exponential(5, n_mieszkan)\n})\n\n# Realistyczna formuÅ‚a ceny (z szumem)\ndata['cena'] = (\n    data['powierzchnia'] * 8000 +           # 8k za mÂ²\n    data['liczba_pokoi'] * 15000 +          # 15k za pokÃ³j\n    (50 - data['wiek_budynku']) * 2000 +    # starsze = taÅ„sze\n    data['odleglosc_centrum'] * (-3000) +   # dalej = taÅ„sze\n    np.random.normal(0, 50000, n_mieszkan)  # szum losowy\n)\n\nprint(\"Podstawowe statystyki:\")\nprint(data.describe())\n\n\n\n\n\n\n\nPokaÅ¼ statystyki\n\n\n\n\n\n\n\nPodstawowe statystyki:\n       powierzchnia  liczba_pokoi  wiek_budynku  odleglosc_centrum  \\\ncount   1000.000000   1000.000000   1000.000000        1000.000000   \nmean      70.483301      2.473000     25.049000           4.971457   \nstd       24.480398      1.128958     14.384001           4.883716   \nmin      -11.031684      1.000000      0.000000           0.000058   \n25%       53.810242      1.000000     13.000000           1.459988   \n50%       70.632515      2.000000     25.000000           3.505541   \n75%       86.198597      4.000000     38.000000           6.954361   \nmax      166.318287      4.000000     49.000000          34.028756   \n\n               cena  \ncount  1.000000e+03  \nmean   6.363187e+05  \nstd    2.004394e+05  \nmin    1.686504e+04  \n25%    5.049745e+05  \n50%    6.356690e+05  \n75%    7.647808e+05  \nmax    1.440259e+06"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#trenowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/02-linear-regression.html#trenowanie-modelu-krok-po-kroku",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "ğŸ”§ Trenowanie modelu krok po kroku",
    "text": "ğŸ”§ Trenowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Sprawdzenie korelacji - ktÃ³re zmienne sÄ… najwaÅ¼niejsze?\ncorrelation = data.corr()['cena'].sort_values(ascending=False)\n\n# PodziaÅ‚ na features (X) i target (y)\nX = data[['powierzchnia', 'liczba_pokoi', 'wiek_budynku', 'odleglosc_centrum']]\ny = data['cena']\n\n# PodziaÅ‚ train/test (80/20)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"Korelacja z cenÄ…:\")\nprint(correlation)\n\nprint(f\"Dane treningowe: {X_train.shape[0]} mieszkaÅ„\")\nprint(f\"Dane testowe: {X_test.shape[0]} mieszkaÅ„\")\n\n\n\n\n\n\n\nPokaÅ¼ korelacje oraz dane treningowe\n\n\n\n\n\n\n\nKorelacja z cenÄ…:\ncena                 1.000000\npowierzchnia         0.949080\nliczba_pokoi         0.025094\nodleglosc_centrum   -0.042040\nwiek_budynku        -0.115566\nName: cena, dtype: float64\nDane treningowe: 800 mieszkaÅ„\nDane testowe: 200 mieszkaÅ„\n\n\n\n\n\n\n\n2) Trenowanie modelu\n\n# Utworzenie i trenowanie modelu\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Sprawdzenie wspÃ³Å‚czynnikÃ³w\nprint(\"WspÃ³Å‚czynniki modelu:\")\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"{feature}: {coef:.0f} zÅ‚\")\nprint(f\"Intercept (staÅ‚a): {model.intercept_:.0f} zÅ‚\")\n\n\n\n\n\n\n\nPokaÅ¼ wspÃ³Å‚czynniki\n\n\n\n\n\n\n\nWspÃ³Å‚czynniki modelu:\npowierzchnia: 7924 zÅ‚\nliczba_pokoi: 15811 zÅ‚\nwiek_budynku: -2108 zÅ‚\nodleglosc_centrum: -2698 zÅ‚\nIntercept (staÅ‚a): 104042 zÅ‚\n\n\n\n\n\n\n\n3) Ewaluacja wynikÃ³w\n\n# Predykcje na zbiorze testowym\ny_pred = model.predict(X_test)\n\n# Metryki\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"\\nWyniki modelu:\")\nprint(f\"Mean Absolute Error: {mae:.0f} zÅ‚\")\nprint(f\"RÂ² Score: {r2:.3f}\")\nprint(f\"Åšredni bÅ‚Ä…d to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania\")\n\n# PrzykÅ‚adowe predykcje\nfor i in range(5):\n    rzeczywista = y_test.iloc[i]\n    przewidywana = y_pred[i]\n    bÅ‚Ä…d = abs(rzeczywista - przewidywana)\n    print(f\"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}zÅ‚, \"\n          f\"przewidywana={przewidywana:.0f}zÅ‚, bÅ‚Ä…d={bÅ‚Ä…d:.0f}zÅ‚\")\n\n\n\n\n\n\n\nPokaÅ¼ wspÃ³Å‚czynniki\n\n\n\n\n\n\n\n\nWyniki modelu:\nMean Absolute Error: 38912 zÅ‚\nRÂ² Score: 0.934\nÅšredni bÅ‚Ä…d to 6.1% ceny mieszkania\nMieszkanie 1: rzeczywista=702313zÅ‚, przewidywana=771489zÅ‚, bÅ‚Ä…d=69177zÅ‚\nMieszkanie 2: rzeczywista=791174zÅ‚, przewidywana=816756zÅ‚, bÅ‚Ä…d=25582zÅ‚\nMieszkanie 3: rzeczywista=326702zÅ‚, przewidywana=274297zÅ‚, bÅ‚Ä…d=52405zÅ‚\nMieszkanie 4: rzeczywista=441380zÅ‚, przewidywana=456054zÅ‚, bÅ‚Ä…d=14674zÅ‚\nMieszkanie 5: rzeczywista=445427zÅ‚, przewidywana=383199zÅ‚, bÅ‚Ä…d=62228zÅ‚"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#wizualizacja-wynikÃ³w",
    "href": "cheatsheets/02-linear-regression.html#wizualizacja-wynikÃ³w",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "ğŸ“Š Wizualizacja wynikÃ³w",
    "text": "ğŸ“Š Wizualizacja wynikÃ³w\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\n\n# PrzykÅ‚adowe dane mieszkaÅ„ (powtarzamy dla demonstracji)\nnp.random.seed(42)\nn_mieszkan = 1000\n\ndata = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_mieszkan),\n    'liczba_pokoi': np.random.randint(1, 5, n_mieszkan),\n    'wiek_budynku': np.random.randint(0, 50, n_mieszkan),\n    'odleglosc_centrum': np.random.exponential(5, n_mieszkan)\n})\n\ndata['cena'] = (\n    data['powierzchnia'] * 8000 +\n    data['liczba_pokoi'] * 15000 +\n    (50 - data['wiek_budynku']) * 2000 +\n    data['odleglosc_centrum'] * (-3000) +\n    np.random.normal(0, 50000, n_mieszkan)\n)\n\n# Przygotowanie i trenowanie modelu\nX = data[['powierzchnia', 'liczba_pokoi', 'wiek_budynku', 'odleglosc_centrum']]\ny = data['cena']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Oblicz metryki\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Przygotuj wykresy\nplt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Rzeczywiste ceny')\nplt.ylabel('Przewidywane ceny')\nplt.title('Rzeczywiste vs Przewidywane')\n\nplt.subplot(1, 2, 2)\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Przewidywane ceny')\nplt.ylabel('Residuals (bÅ‚Ä™dy)')\nplt.title('Analiza residuals')\n\nplt.tight_layout()\n\n# Zapisz wykres do zmiennej\nimport io\nimport base64\nbuf = io.BytesIO()\nplt.savefig(buf, format='png', dpi=100, bbox_inches='tight')\nbuf.seek(0)\nplt.close()\n\n# Przygotuj przykÅ‚adowe predykcje\nprzykladowe_predykcje = []\nfor i in range(3):\n    rzeczywista = y_test.iloc[i]\n    przewidywana = y_pred[i]\n    blad = abs(rzeczywista - przewidywana)\n    przykladowe_predykcje.append(f\"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}zÅ‚, przewidywana={przewidywana:.0f}zÅ‚, bÅ‚Ä…d={blad:.0f}zÅ‚\")\n\nprint(f\"Wyniki modelu Linear Regression:\")\nprint(f\"Mean Absolute Error: {mae:.0f} zÅ‚\")\nprint(f\"RÂ² Score: {r2:.3f}\")\nprint(f\"Åšredni bÅ‚Ä…d to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania\")\nprint(f\"\\nPrzykÅ‚adowe predykcje:\")\nfor pred in przykladowe_predykcje:\n    print(pred)\n\n# OdtwÃ³rz wykres\nplt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Rzeczywiste ceny')\nplt.ylabel('Przewidywane ceny')\nplt.title('Rzeczywiste vs Przewidywane')\n\nplt.subplot(1, 2, 2)\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Przewidywane ceny')\nplt.ylabel('Residuals (bÅ‚Ä™dy)')\nplt.title('Analiza residuals')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nPokaÅ¼ wyniki i wykresy analizy\n\n\n\n\n\n\n\nWyniki modelu Linear Regression:\nMean Absolute Error: 38912 zÅ‚\nRÂ² Score: 0.934\nÅšredni bÅ‚Ä…d to 6.1% ceny mieszkania\n\nPrzykÅ‚adowe predykcje:\nMieszkanie 1: rzeczywista=702313zÅ‚, przewidywana=771489zÅ‚, bÅ‚Ä…d=69177zÅ‚\nMieszkanie 2: rzeczywista=791174zÅ‚, przewidywana=816756zÅ‚, bÅ‚Ä…d=25582zÅ‚\nMieszkanie 3: rzeczywista=326702zÅ‚, przewidywana=274297zÅ‚, bÅ‚Ä…d=52405zÅ‚\n\n\n\n\n\nAnaliza wynikÃ³w modelu Linear Regression"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#praktyczne-zastosowania-linear-regression",
    "href": "cheatsheets/02-linear-regression.html#praktyczne-zastosowania-linear-regression",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "ğŸ¯ Praktyczne zastosowania Linear Regression",
    "text": "ğŸ¯ Praktyczne zastosowania Linear Regression\n\n1) Biznesowe\n\n# Przewidywanie sprzedaÅ¼y na podstawie budÅ¼etu marketingowego\nsales_data = pd.DataFrame({\n    'marketing_budget': [10000, 15000, 20000, 25000, 30000],\n    'sales': [100000, 140000, 180000, 220000, 260000]\n})\n\nmodel_sales = LinearRegression()\nmodel_sales.fit(sales_data[['marketing_budget']], sales_data['sales'])\n\n# \"JeÅ›li zwiÄ™kszÄ™ budÅ¼et do 35k, sprzedaÅ¼ wzroÅ›nie do:\"\nnew_sales = model_sales.predict([[35000]])\nprint(f\"Przewidywana sprzedaÅ¼ przy budÅ¼ecie 35k: {new_sales[0]:.0f}\")\n\n\n\n\n\n\n\nPokaÅ¼ przewidywanÄ… sprzedaÅ¼\n\n\n\n\n\n\n\nPrzewidywana sprzedaÅ¼ przy budÅ¼ecie 35k: 300000\n\n\n\n\n\n\n\n2) Analizy finansowe\n\n# Relacja miÄ™dzy PKB a konsumpcjÄ…\neconomics_data = pd.DataFrame({\n    'pkb_per_capita': [25000, 30000, 35000, 40000, 45000],\n    'konsumpcja': [18000, 21000, 24000, 27000, 30000]\n})\n\nmodel_econ = LinearRegression()\nmodel_econ.fit(economics_data[['pkb_per_capita']], economics_data['konsumpcja'])\n\n# WspÃ³Å‚czynnik skÅ‚onnoÅ›ci do konsumpcji\nprint(f\"Na kaÅ¼de dodatkowe 1000zÅ‚ PKB, konsumpcja roÅ›nie o: {model_econ.coef_[0]:.0f}zÅ‚\")\n\n\n\n\n\n\n\nPokaÅ¼ wspÃ³Å‚czynnik skÅ‚onnoÅ›ci do konsumpcji\n\n\n\n\n\n\n\nNa kaÅ¼de dodatkowe 1000zÅ‚ PKB, konsumpcja roÅ›nie o: 1zÅ‚"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#najczÄ™stsze-puÅ‚apki-i-jak-ich-unikaÄ‡",
    "href": "cheatsheets/02-linear-regression.html#najczÄ™stsze-puÅ‚apki-i-jak-ich-unikaÄ‡",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "âš ï¸ NajczÄ™stsze puÅ‚apki i jak ich unikaÄ‡",
    "text": "âš ï¸ NajczÄ™stsze puÅ‚apki i jak ich unikaÄ‡\n\n1) Overfitting z wieloma zmiennymi\n\n# ZÅO: za duÅ¼o features wzglÄ™dem danych\n# JeÅ›li masz 100 mieszkaÅ„, nie uÅ¼ywaj 50 features!\n\n# DOBRZE: zasada kciuka\ndef check_features_ratio(X, y):\n    ratio = len(y) / X.shape[1]\n    if ratio &lt; 10:\n        print(f\"âš ï¸ Uwaga: masz tylko {ratio:.1f} obserwacji na feature!\")\n        print(\"RozwaÅ¼: wiÄ™cej danych lub mniej features\")\n    else:\n        print(f\"âœ… OK: {ratio:.1f} obserwacji na feature\")\n\ncheck_features_ratio(X_train, y_train)\n\n\n\n\n\n\n\nPokaÅ¼ wyniki\n\n\n\n\n\n\n\nâœ… OK: 200.0 obserwacji na feature\n\n\n\n\n\n\n\n2) Sprawdzanie zaÅ‚oÅ¼eÅ„ linearnoÅ›ci\n\n# SprawdÅº czy zaleÅ¼noÅ›ci sÄ… rzeczywiÅ›cie liniowe\nfrom scipy import stats\n\nfor col in X.columns:\n    correlation, p_value = stats.pearsonr(data[col], data['cena'])\n    print(f\"{col}: korelacja={correlation:.3f}, p-value={p_value:.3f}\")\n    \n    if abs(correlation) &lt; 0.1:\n        print(f\"âš ï¸ {col} ma sÅ‚abÄ… korelacjÄ™ - moÅ¼e nie byÄ‡ przydatna\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki korelacji\n\n\n\n\n\n\n\npowierzchnia: korelacja=0.949, p-value=0.000\nliczba_pokoi: korelacja=0.025, p-value=0.428\nâš ï¸ liczba_pokoi ma sÅ‚abÄ… korelacjÄ™ - moÅ¼e nie byÄ‡ przydatna\nwiek_budynku: korelacja=-0.116, p-value=0.000\nodleglosc_centrum: korelacja=-0.042, p-value=0.184\nâš ï¸ odleglosc_centrum ma sÅ‚abÄ… korelacjÄ™ - moÅ¼e nie byÄ‡ przydatna\n\n\n\n\n\n\n\n3) WielokoliniowoÅ›Ä‡ (features korelujÄ… miÄ™dzy sobÄ…)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# PrzykÅ‚adowe dane features (demonstracja)\nnp.random.seed(42)\nn_samples = 1000\n\nX = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_samples),\n    'liczba_pokoi': np.random.randint(1, 5, n_samples),\n    'wiek_budynku': np.random.randint(0, 50, n_samples),\n    'odleglosc_centrum': np.random.exponential(5, n_samples)\n})\n\n# Oblicz korelacje miÄ™dzy features\ncorrelation_matrix = X.corr()\n\n# Przygotuj wykres\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Korelacje miÄ™dzy features')\nplt.close()\n\nprint(\"Macierz korelacji miÄ™dzy zmiennymi:\")\nprint(correlation_matrix)\nprint(\"\\nInterpretacja:\")\nprint(\"â€¢ WartoÅ›ci blisko 1.0 = silna korelacja pozytywna\")\nprint(\"â€¢ WartoÅ›ci blisko -1.0 = silna korelacja negatywna\") \nprint(\"â€¢ WartoÅ›ci blisko 0.0 = brak korelacji\")\nprint(\"\\nJeÅ›li korelacja miÄ™dzy features &gt; 0.8, usuÅ„ jednÄ… z nich (multicollinearity)!\")\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Korelacje miÄ™dzy features')\nplt.show()\n\n\n\n\n\n\n\nPokaÅ¼ mapÄ™ korelacji\n\n\n\n\n\n\n\nMacierz korelacji miÄ™dzy zmiennymi:\n                   powierzchnia  liczba_pokoi  wiek_budynku  odleglosc_centrum\npowierzchnia           1.000000     -0.064763      0.033920           0.029856\nliczba_pokoi          -0.064763      1.000000     -0.003894          -0.057555\nwiek_budynku           0.033920     -0.003894      1.000000          -0.037242\nodleglosc_centrum      0.029856     -0.057555     -0.037242           1.000000\n\nInterpretacja:\nâ€¢ WartoÅ›ci blisko 1.0 = silna korelacja pozytywna\nâ€¢ WartoÅ›ci blisko -1.0 = silna korelacja negatywna\nâ€¢ WartoÅ›ci blisko 0.0 = brak korelacji\n\nJeÅ›li korelacja miÄ™dzy features &gt; 0.8, usuÅ„ jednÄ… z nich (multicollinearity)!\n\n\n\n\n\nMapa korelacji miÄ™dzy zmiennymi"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#parametry-do-tuningu",
    "href": "cheatsheets/02-linear-regression.html#parametry-do-tuningu",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "ğŸ”§ Parametry do tuningu",
    "text": "ğŸ”§ Parametry do tuningu\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n# 1) Standardization - gdy features majÄ… rÃ³Å¼ne skale\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train)\n\n# 2) Polynomial Features - dla nieliniowych zaleÅ¼noÅ›ci\npoly_model = Pipeline([\n    ('poly', PolynomialFeatures(degree=2)),\n    ('linear', LinearRegression())\n])\npoly_model.fit(X_train, y_train)\n\npoly_pred = poly_model.predict(X_test)\npoly_r2 = r2_score(y_test, poly_pred)\nprint(f\"Polynomial Regression RÂ²: {poly_r2:.3f}\")\n\n# 3) Regularization - Ridge i Lasso\nfrom sklearn.linear_model import Ridge, Lasso\n\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\nridge_r2 = r2_score(y_test, ridge.predict(X_test))\n\nlasso = Lasso(alpha=1.0)\nlasso.fit(X_train, y_train)\nlasso_r2 = r2_score(y_test, lasso.predict(X_test))\n\nprint(f\"Ridge RÂ²: {ridge_r2:.3f}\")\nprint(f\"Lasso RÂ²: {lasso_r2:.3f}\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki predykcji cen\n\n\n\n\n\n\n\nPolynomial Regression RÂ²: 0.933\nRidge RÂ²: 0.934\nLasso RÂ²: 0.934"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#real-world-przykÅ‚ady-zastosowaÅ„",
    "href": "cheatsheets/02-linear-regression.html#real-world-przykÅ‚ady-zastosowaÅ„",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "ğŸŒ Real-world przykÅ‚ady zastosowaÅ„",
    "text": "ğŸŒ Real-world przykÅ‚ady zastosowaÅ„\n\nE-commerce: Przewidywanie wartoÅ›ci Å¼yciowej klienta (LTV)\nNieruchomoÅ›ci: Automatyczna wycena domÃ³w (Zillow)\nFinanse: Scoring kredytowy, przewidywanie cen akcji\nMarketing: ROI kampanii reklamowych\nSupply Chain: Prognozowanie popytu na produkty\n\n\n\n\n\n\n\nğŸ’¡ Kiedy uÅ¼ywaÄ‡ Linear Regression?\n\n\n\nâœ… UÅ»YJ GDY:\n\nPrzewidujesz wartoÅ›ci liczbowe (continuous target)\nZaleÅ¼noÅ›ci wydajÄ… siÄ™ liniowe\nChcesz interpretowalny model\nMasz stosunkowo maÅ‚o features\n\nâŒ NIE UÅ»YWAJ GDY:\n\nTarget jest kategoryczny (uÅ¼yj klasyfikacji)\nZaleÅ¼noÅ›ci sÄ… bardzo nieliniowe (uÅ¼yj Random Forest, XGBoost)\nMasz tysiÄ…ce features (uÅ¼yj regularization)\n\n\n\nNastÄ™pna Å›ciÄ…gawka: Decision Trees - klasyfikacja ğŸŒ³"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html",
    "href": "cheatsheets/01-intro-ml.html",
    "title": "Wprowadzenie do Machine Learning â€” fundament",
    "section": "",
    "text": "Machine Learning (ML) to dziedzina informatyki, ktÃ³ra pozwala komputerom uczyÄ‡ siÄ™ i podejmowaÄ‡ decyzje na podstawie danych, bez koniecznoÅ›ci programowania kaÅ¼dej reguÅ‚y z gÃ³ry.\n\n\n\n\n\n\nğŸ’¡ Intuicja\n\n\n\nZamiast pisaÄ‡ kod â€œjeÅ›li temperatura &gt; 25Â°C, to bÄ™dzie sÅ‚onecznieâ€, ML pozwala algorytmowi samemu odkryÄ‡ te zaleÅ¼noÅ›ci z historycznych danych pogodowych."
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#czym-jest-machine-learning",
    "href": "cheatsheets/01-intro-ml.html#czym-jest-machine-learning",
    "title": "Wprowadzenie do Machine Learning â€” fundament",
    "section": "",
    "text": "Machine Learning (ML) to dziedzina informatyki, ktÃ³ra pozwala komputerom uczyÄ‡ siÄ™ i podejmowaÄ‡ decyzje na podstawie danych, bez koniecznoÅ›ci programowania kaÅ¼dej reguÅ‚y z gÃ³ry.\n\n\n\n\n\n\nğŸ’¡ Intuicja\n\n\n\nZamiast pisaÄ‡ kod â€œjeÅ›li temperatura &gt; 25Â°C, to bÄ™dzie sÅ‚onecznieâ€, ML pozwala algorytmowi samemu odkryÄ‡ te zaleÅ¼noÅ›ci z historycznych danych pogodowych."
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#gÅ‚Ã³wne-rodzaje-ml",
    "href": "cheatsheets/01-intro-ml.html#gÅ‚Ã³wne-rodzaje-ml",
    "title": "Wprowadzenie do Machine Learning â€” fundament",
    "section": "ğŸ“Š GÅ‚Ã³wne rodzaje ML",
    "text": "ğŸ“Š GÅ‚Ã³wne rodzaje ML\n\n1) Supervised Learning (Uczenie nadzorowane)\nMamy dane + znamy prawidÅ‚owe odpowiedzi\n\n# PrzykÅ‚ad: predykcja ceny domu\n# Dane wejÅ›ciowe: powierzchnia, lokalizacja, rok budowy\n# Cel: przewidzieÄ‡ cenÄ™\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# PrzykÅ‚adowe dane\ndata = pd.DataFrame({\n    'powierzchnia': [50, 75, 100, 120, 150],\n    'rok_budowy': [1990, 2000, 2010, 2015, 2020],\n    'cena': [300000, 400000, 550000, 650000, 800000]  # znamy prawdziwe ceny!\n})\n\n# Trenowanie modelu\nmodel = LinearRegression()\nX = data[['powierzchnia', 'rok_budowy']]\ny = data['cena']\nmodel.fit(X, y)\n\n# Predykcja dla nowego domu\nnowy_dom = [[90, 2005]]\nprzewidywana_cena = model.predict(nowy_dom)\n\n# Zapisz wyniki do zmiennych dla expandera\ndane_treningowe = data.to_string()\nwynik_predykcji = f\"Przewidywana cena: {przewidywana_cena[0]:.0f} zÅ‚\"\n\nprint(\"Dane treningowe:\")\nprint(dane_treningowe)\nprint(f\"\\nPredykcja dla domu 90mÂ², rok 2005:\")\nprint(wynik_predykcji)\n\n\n\n\n\n\n\nPokaÅ¼ wyniki predykcji cen\n\n\n\n\n\n\n\nDane treningowe:\n   powierzchnia  rok_budowy    cena\n0            50        1990  300000\n1            75        2000  400000\n2           100        2010  550000\n3           120        2015  650000\n4           150        2020  800000\n\nPredykcja dla domu 90mÂ², rok 2005:\nPrzewidywana cena: 493819 zÅ‚\n\n\n\n\n\nReal-world zastosowania:\n\nPredykcja cen akcji/nieruchomoÅ›ci\nDiagnoza medyczna (klasyfikacja chorÃ³b)\nFiltrowanie spamu w emailach\nRozpoznawanie mowy/obrazÃ³w\n\n\n\n\n2) Unsupervised Learning (Uczenie nienadzorowane)\nMamy tylko dane, szukamy ukrytych wzorcÃ³w\n\n# PrzykÅ‚ad: segmentacja klientÃ³w sklepu\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Dane klientÃ³w: wiek i wydatki miesiÄ™czne\nklienci = pd.DataFrame({\n    'wiek': [25, 30, 35, 22, 28, 45, 50, 55, 60, 65],\n    'wydatki': [2000, 2500, 3000, 1800, 2200, 4000, 4500, 3500, 3000, 2800]\n})\n\n# Grupowanie klientÃ³w w 3 segmenty\nkmeans = KMeans(n_clusters=3, random_state=42)\nklienci['segment'] = kmeans.fit_predict(klienci[['wiek', 'wydatki']])\n\n# Przygotuj wyniki do wyÅ›wietlenia\ndane_klientow = klienci.head().to_string()\nsegmenty_info = []\nfor i in range(3):\n    segment = klienci[klienci['segment'] == i]\n    segmenty_info.append(f\"Segment {i}: Å›redni wiek {segment['wiek'].mean():.0f}, Å›rednie wydatki {segment['wydatki'].mean():.0f}\")\n\nprzyklad_predykcji = kmeans.predict([[30, 2500]])[0]\n\nprint(\"Dane klientÃ³w:\")\nprint(dane_klientow)\nprint(\"\\nSegmenty klientÃ³w:\")\nfor info in segmenty_info:\n    print(info)\nprint(f\"\\nPrzykÅ‚ad: klient 30 lat, wydaje 2500zÅ‚ -&gt; segment {przyklad_predykcji}\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki segmentacji klientÃ³w\n\n\n\n\n\n\n\nDane klientÃ³w:\n   wiek  wydatki  segment\n0    25     2000        0\n1    30     2500        2\n2    35     3000        2\n3    22     1800        0\n4    28     2200        0\n\nSegmenty klientÃ³w:\nSegment 0: Å›redni wiek 25, Å›rednie wydatki 2000\nSegment 1: Å›redni wiek 50, Å›rednie wydatki 4000\nSegment 2: Å›redni wiek 48, Å›rednie wydatki 2825\n\nPrzykÅ‚ad: klient 30 lat, wydaje 2500zÅ‚ -&gt; segment 2\n\n\n\n\n\nReal-world zastosowania:\n\nSegmentacja klientÃ³w (marketing)\nWykrywanie anomalii (cyberbezpieczeÅ„stwo)\nAnaliza koszykowa (co kupujÄ… razem)\nKompresja danych\n\n\n\n\n3) Reinforcement Learning (Uczenie ze wzmocnieniem)\nAgent uczy siÄ™ przez interakcjÄ™ i nagrody/kary\n\n# PrzykÅ‚ad koncepcyjny: optymalizacja reklam\nclass SimpleAgent:\n    def __init__(self):\n        # Jakie reklamy pokazywaÄ‡: [\"sportowe\", \"technologiczne\", \"modowe\"]\n        self.ad_types = [\"sportowe\", \"technologiczne\", \"modowe\"]\n        self.rewards = [0, 0, 0]  # nagrody za kaÅ¼dy typ\n        self.counts = [0, 0, 0]   # ile razy pokazane\n    \n    def choose_ad(self):\n        # Wybierz reklamÄ™ z najwyÅ¼szÄ… Å›redniÄ… nagrodÄ…\n        avg_rewards = [r/max(c,1) for r, c in zip(self.rewards, self.counts)]\n        return avg_rewards.index(max(avg_rewards))\n    \n    def update_reward(self, ad_type, clicked):\n        # Aktualizuj nagrody na podstawie klikniÄ™Ä‡\n        self.counts[ad_type] += 1\n        if clicked:\n            self.rewards[ad_type] += 1\n    \n    def get_stats(self):\n        stats_list = []\n        for i, ad_type in enumerate(self.ad_types):\n            rate = self.rewards[i] / max(self.counts[i], 1) * 100\n            stats_list.append(f\"{ad_type}: {self.counts[i]} pokazaÅ„, {self.rewards[i]} klikniÄ™Ä‡ ({rate:.1f}%)\")\n        return stats_list\n\n# Symulacja\nimport numpy as np\nnp.random.seed(42)\n\nagent = SimpleAgent()\nsymulacja_wyniki = []\nsymulacja_wyniki.append(\"ğŸ¯ Symulacja optymalizacji reklam:\")\nsymulacja_wyniki.append(\"Agent uczy siÄ™, ktÃ³re reklamy dziaÅ‚ajÄ… najlepiej...\")\n\nfor day in range(10):\n    ad = agent.choose_ad()\n    # RÃ³Å¼ne prawdopodobieÅ„stwa klikniÄ™Ä‡ dla rÃ³Å¼nych typÃ³w reklam\n    click_probs = [0.1, 0.3, 0.2]  # technologiczne najlepsze\n    clicked = np.random.random() &lt; click_probs[ad]\n    agent.update_reward(ad, clicked)\n    symulacja_wyniki.append(f\"DzieÅ„ {day+1}: pokazano {agent.ad_types[ad]}, klikniÄ™ta: {clicked}\")\n\nkoncowe_statystyki = agent.get_stats()\n\nfor wynik in symulacja_wyniki:\n    print(wynik)\nprint(\"\\nğŸ“Š KoÅ„cowe statystyki:\")\nfor stat in koncowe_statystyki:\n    print(stat)\n\n\n\n\n\n\n\nPokaÅ¼ symulacjÄ™ agenta reklamowego\n\n\n\n\n\n\n\nğŸ¯ Symulacja optymalizacji reklam:\nAgent uczy siÄ™, ktÃ³re reklamy dziaÅ‚ajÄ… najlepiej...\nDzieÅ„ 1: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 2: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 3: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 4: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 5: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 6: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 7: pokazano sportowe, klikniÄ™ta: True\nDzieÅ„ 8: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 9: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 10: pokazano sportowe, klikniÄ™ta: False\n\nğŸ“Š KoÅ„cowe statystyki:\nsportowe: 10 pokazaÅ„, 1 klikniÄ™Ä‡ (10.0%)\ntechnologiczne: 0 pokazaÅ„, 0 klikniÄ™Ä‡ (0.0%)\nmodowe: 0 pokazaÅ„, 0 klikniÄ™Ä‡ (0.0%)\n\n\n\n\n\nReal-world zastosowania:\n\nGry komputerowe (AI graczy)\nAutonomiczne pojazdy\nOptymalizacja reklam online\nRoboty przemysÅ‚owe"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#jak-wybraÄ‡-odpowiedni-typ-ml",
    "href": "cheatsheets/01-intro-ml.html#jak-wybraÄ‡-odpowiedni-typ-ml",
    "title": "Wprowadzenie do Machine Learning â€” fundament",
    "section": "ğŸ¯ Jak wybraÄ‡ odpowiedni typ ML?",
    "text": "ğŸ¯ Jak wybraÄ‡ odpowiedni typ ML?\n\n\n\n\n\n\n\n\nSytuacja\nTyp ML\nPrzykÅ‚ad\n\n\n\n\nMasz dane z prawidÅ‚owymi odpowiedziami\nSupervised\nSpam/nie-spam w emailach\n\n\nChcesz znaleÅºÄ‡ ukryte grupÑ‹\nUnsupervised\nSegmentacja klientÃ³w\n\n\nSystem ma siÄ™ uczyÄ‡ przez trial & error\nReinforcement\nBot do gier"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#podstawowe-kroki-projektu-ml",
    "href": "cheatsheets/01-intro-ml.html#podstawowe-kroki-projektu-ml",
    "title": "Wprowadzenie do Machine Learning â€” fundament",
    "section": "ğŸ”§ Podstawowe kroki projektu ML",
    "text": "ğŸ”§ Podstawowe kroki projektu ML\n\n# Kompletny workflow ML na przykÅ‚adzie klasyfikacji iris\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# 1. ZaÅ‚aduj i poznaj dane\niris = load_iris()\ndata = pd.DataFrame(iris.data, columns=iris.feature_names)\ndata['target'] = iris.target\ntarget_names = iris.target_names\n\n# 2. Przygotuj dane (czyszczenie, encoding)\nX = data.drop('target', axis=1)\ny = data['target']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 3. Podziel na train/test\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# 4. Wybierz i wytrenuj model\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# 5. OceÅ„ wyniki\npredictions = model.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\n\n# PrzykÅ‚ad predykcji\nsample_flower = X_test[0:1]\npredicted_class = model.predict(sample_flower)[0]\npredicted_name = target_names[predicted_class]\nactual_name = target_names[y_test.iloc[0]]\n\n# Przygotuj wyniki do wyÅ›wietlenia\nwyniki_workflow = []\nwyniki_workflow.append(\"ğŸ”¬ Kompletny workflow projektu ML\")\nwyniki_workflow.append(\"=\" * 40)\nwyniki_workflow.append(\"1ï¸âƒ£ Dane zaÅ‚adowane:\")\nwyniki_workflow.append(f\"KsztaÅ‚t: {data.shape}\")\nwyniki_workflow.append(f\"Klasy: {target_names}\")\nwyniki_workflow.append(f\"Pierwsze 3 wiersze:\\n{data.head(3).to_string()}\")\nwyniki_workflow.append(f\"\\n2ï¸âƒ£ Dane przeskalowane (pierwsze 3 cechy pierwszej prÃ³bki):\")\nwyniki_workflow.append(f\"Przed: {X.iloc[0, :3].values}\")\nwyniki_workflow.append(f\"Po: {X_scaled[0, :3]}\")\nwyniki_workflow.append(f\"\\n3ï¸âƒ£ PodziaÅ‚ danych:\")\nwyniki_workflow.append(f\"Train: {len(X_train)} prÃ³bek\")\nwyniki_workflow.append(f\"Test: {len(X_test)} prÃ³bek\")\nwyniki_workflow.append(f\"\\n4ï¸âƒ£ Model wytrenowany: RandomForestClassifier\")\nwyniki_workflow.append(f\"\\n5ï¸âƒ£ Wyniki:\")\nwyniki_workflow.append(f\"DokÅ‚adnoÅ›Ä‡: {accuracy:.2%}\")\nwyniki_workflow.append(f\"\\nPrzykÅ‚ad predykcji:\")\nwyniki_workflow.append(f\"Przewidywana klasa: {predicted_name}\")\nwyniki_workflow.append(f\"Rzeczywista klasa: {actual_name}\")\nwyniki_workflow.append(\"âœ… Poprawnie!\" if predicted_name == actual_name else \"âŒ BÅ‚Ä…d\")\n\nfor wynik in wyniki_workflow:\n    print(wynik)\n\n\n\n\n\n\n\nPokaÅ¼ kompletny workflow ML\n\n\n\n\n\n\n\nğŸ”¬ Kompletny workflow projektu ML\n========================================\n1ï¸âƒ£ Dane zaÅ‚adowane:\nKsztaÅ‚t: (150, 5)\nKlasy: ['setosa' 'versicolor' 'virginica']\nPierwsze 3 wiersze:\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n0                5.1               3.5                1.4               0.2       0\n1                4.9               3.0                1.4               0.2       0\n2                4.7               3.2                1.3               0.2       0\n\n2ï¸âƒ£ Dane przeskalowane (pierwsze 3 cechy pierwszej prÃ³bki):\nPrzed: [5.1 3.5 1.4]\nPo: [-0.90068117  1.01900435 -1.34022653]\n\n3ï¸âƒ£ PodziaÅ‚ danych:\nTrain: 120 prÃ³bek\nTest: 30 prÃ³bek\n\n4ï¸âƒ£ Model wytrenowany: RandomForestClassifier\n\n5ï¸âƒ£ Wyniki:\nDokÅ‚adnoÅ›Ä‡: 100.00%\n\nPrzykÅ‚ad predykcji:\nPrzewidywana klasa: versicolor\nRzeczywista klasa: versicolor\nâœ… Poprawnie!"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#najwaÅ¼niejsze-biblioteki-python-dla-ml",
    "href": "cheatsheets/01-intro-ml.html#najwaÅ¼niejsze-biblioteki-python-dla-ml",
    "title": "Wprowadzenie do Machine Learning â€” fundament",
    "section": "ğŸ’¡ NajwaÅ¼niejsze biblioteki Python dla ML",
    "text": "ğŸ’¡ NajwaÅ¼niejsze biblioteki Python dla ML\n# Podstawowe przetwarzanie danych\nimport pandas as pd      # DataFrames, CSV, analiza\nimport numpy as np       # obliczenia numeryczne\n\n# Machine Learning\nfrom sklearn import *    # algorytmy ML, preprocessing, metryki\nimport xgboost as xgb   # zaawansowane drzewa decyzyjne\n\n# Wizualizacja\nimport matplotlib.pyplot as plt  # wykresy\nimport seaborn as sns           # piÄ™kne wykresy statystyczne\n\n# Deep Learning\nimport tensorflow as tf  # sieci neuronowe (Google)\nimport torch            # sieci neuronowe (Facebook)\n\n\n\n\n\n\n\nğŸ¯ Pro tips dla poczÄ…tkujÄ…cych\n\n\n\n\nZacznij od prostych algorytmÃ³w - Linear Regression, Decision Trees\n80% czasu to przygotowanie danych - czyszczenie, eksploracja, feature engineering\nZawsze sprawdÅº czy model nie jest overfitted - uÅ¼yj validation set\nRozumiej swoje dane przed wyborem algorytmu\nPraktyka &gt; teoria - rÃ³b duÅ¼o projektÃ³w na rÃ³Å¼nych danych!\n\n\n\nNastÄ™pna Å›ciÄ…gawka: Linear Regression w praktyce ğŸš€"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html",
    "href": "cheatsheets/03-decision-trees.html",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "",
    "text": "Decision Trees to jeden z najbardziej intuicyjnych algorytmÃ³w ML, ktÃ³ry podejmuje decyzje jak czÅ‚owiek - zadajÄ…c szereg pytaÅ„ typu â€œtak/nieâ€ i na podstawie odpowiedzi klasyfikuje lub przewiduje wartoÅ›ci.\n\n\n\n\n\n\nğŸ’¡ Intuicja\n\n\n\nWyobraÅº sobie lekarza, ktÃ³ry diagnozuje chorobÄ™: â€œCzy ma gorÄ…czkÄ™? TAK â†’ Czy boli gardÅ‚o? TAK â†’ Czy ma katar? NIE â†’ Prawdopodobnie anginaâ€. Decision Tree dziaÅ‚a dokÅ‚adnie tak samo!"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#czym-sÄ…-decision-trees",
    "href": "cheatsheets/03-decision-trees.html#czym-sÄ…-decision-trees",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "",
    "text": "Decision Trees to jeden z najbardziej intuicyjnych algorytmÃ³w ML, ktÃ³ry podejmuje decyzje jak czÅ‚owiek - zadajÄ…c szereg pytaÅ„ typu â€œtak/nieâ€ i na podstawie odpowiedzi klasyfikuje lub przewiduje wartoÅ›ci.\n\n\n\n\n\n\nğŸ’¡ Intuicja\n\n\n\nWyobraÅº sobie lekarza, ktÃ³ry diagnozuje chorobÄ™: â€œCzy ma gorÄ…czkÄ™? TAK â†’ Czy boli gardÅ‚o? TAK â†’ Czy ma katar? NIE â†’ Prawdopodobnie anginaâ€. Decision Tree dziaÅ‚a dokÅ‚adnie tak samo!"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#praktyczny-przykÅ‚ad-klasyfikacja-klientÃ³w-banku",
    "href": "cheatsheets/03-decision-trees.html#praktyczny-przykÅ‚ad-klasyfikacja-klientÃ³w-banku",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "ğŸ¯ Praktyczny przykÅ‚ad: klasyfikacja klientÃ³w banku",
    "text": "ğŸ¯ Praktyczny przykÅ‚ad: klasyfikacja klientÃ³w banku\nCzy klient weÅºmie kredyt na podstawie jego profilu?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Tworzenie realistycznych danych klientÃ³w banku\nnp.random.seed(42)\nn_clients = 2000\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_clients),\n    'dochod': np.random.lognormal(10, 0.5, n_clients),  # log-normal dla dochodÃ³w\n    'historia_kredytowa': np.random.choice(['dobra', 'Å›rednia', 'sÅ‚aba'], n_clients, p=[0.6, 0.3, 0.1]),\n    'zatrudnienie': np.random.choice(['staÅ‚e', 'tymczasowe', 'bezrobotny'], n_clients, p=[0.7, 0.25, 0.05]),\n    'oszczednosci': np.random.exponential(20000, n_clients),\n    'liczba_dzieci': np.random.randint(0, 4, n_clients)\n})\n\n# Realistyczna logika decyzyjna banku\ndef czy_kredyt(row):\n    score = 0\n    \n    # Wiek (30-50 lat = najlepsi klienci)\n    if 30 &lt;= row['wiek'] &lt;= 50:\n        score += 2\n    elif row['wiek'] &lt; 25 or row['wiek'] &gt; 60:\n        score -= 1\n    \n    # DochÃ³d\n    if row['dochod'] &gt; 50000:\n        score += 3\n    elif row['dochod'] &gt; 30000:\n        score += 1\n    else:\n        score -= 2\n    \n    # Historia kredytowa\n    if row['historia_kredytowa'] == 'dobra':\n        score += 2\n    elif row['historia_kredytowa'] == 'sÅ‚aba':\n        score -= 3\n    \n    # Zatrudnienie\n    if row['zatrudnienie'] == 'staÅ‚e':\n        score += 2\n    elif row['zatrudnienie'] == 'bezrobotny':\n        score -= 4\n    \n    # OszczÄ™dnoÅ›ci\n    if row['oszczednosci'] &gt; 50000:\n        score += 1\n    \n    # Dzieci (wiÄ™cej dzieci = wiÄ™ksze ryzyko)\n    score -= row['liczba_dzieci'] * 0.5\n    \n    # KoÅ„cowa decyzja z odrobinÄ… losowoÅ›ci\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 0.5)))\n    return probability &gt; 0.5\n\ndata['kredyt_przyznany'] = data.apply(czy_kredyt, axis=1)\n\nprint(\"Statystyki klientÃ³w:\")\nprint(data.describe())\nprint(f\"\\nOdsetek przyznanych kredytÃ³w: {data['kredyt_przyznany'].mean():.1%}\")\n\n\n\n\n\n\n\nPokaÅ¼ statystyki i odsetki przyznanych kredytÃ³w\n\n\n\n\n\n\n\nStatystyki klientÃ³w:\n              wiek         dochod   oszczednosci  liczba_dzieci\ncount  2000.000000    2000.000000    2000.000000    2000.000000\nmean     43.805500   25726.361013   19950.270464       1.523000\nstd      14.929203   14090.406882   20299.940268       1.130535\nmin      18.000000    4047.221838      27.128881       0.000000\n25%      31.000000   15922.761710    5923.833818       1.000000\n50%      44.000000   22782.391426   13844.909016       2.000000\n75%      56.000000   31922.003675   27566.091135       3.000000\nmax      69.000000  112447.929023  165194.684774       3.000000\n\nOdsetek przyznanych kredytÃ³w: 63.8%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#budowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/03-decision-trees.html#budowanie-modelu-krok-po-kroku",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "ğŸ”§ Budowanie modelu krok po kroku",
    "text": "ğŸ”§ Budowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Encoding kategorycznych zmiennych\nfrom sklearn.preprocessing import LabelEncoder\n\ndata_encoded = data.copy()\nle_historia = LabelEncoder()\nle_zatrudnienie = LabelEncoder()\n\ndata_encoded['historia_kredytowa_num'] = le_historia.fit_transform(data['historia_kredytowa'])\ndata_encoded['zatrudnienie_num'] = le_zatrudnienie.fit_transform(data['zatrudnienie'])\n\n# Features do modelu\nfeature_columns = ['wiek', 'dochod', 'historia_kredytowa_num', 'zatrudnienie_num', 'oszczednosci', 'liczba_dzieci']\nX = data_encoded[feature_columns]\ny = data_encoded['kredyt_przyznany']\n\n# PodziaÅ‚ train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Dane treningowe: {len(X_train)} klientÃ³w\")\nprint(f\"Dane testowe: {len(X_test)} klientÃ³w\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki dane treningowe i testowe\n\n\n\n\n\n\n\nDane treningowe: 1600 klientÃ³w\nDane testowe: 400 klientÃ³w\n\n\n\n\n\n\n\n2) Trenowanie Decision Tree\n\n# Tworzenie modelu z ograniczeniami (Å¼eby nie byÅ‚ za gÅ‚Ä™boki)\ndt_model = DecisionTreeClassifier(\n    max_depth=5,        # maksymalna gÅ‚Ä™bokoÅ›Ä‡ drzewa\n    min_samples_split=50,  # min. prÃ³bek do podziaÅ‚u wÄ™zÅ‚a\n    min_samples_leaf=20,   # min. prÃ³bek w liÅ›ciu\n    random_state=42\n)\n\n# Trenowanie\ndt_model.fit(X_train, y_train)\n\nprint(\"Model wytrenowany!\")\nprint(f\"GÅ‚Ä™bokoÅ›Ä‡ drzewa: {dt_model.tree_.max_depth}\")\nprint(f\"Liczba liÅ›ci: {dt_model.tree_.n_leaves}\")\n\n\n\n\n\n\n\nPokaÅ¼ parametry wytrenowanego modelu\n\n\n\n\n\n\n\nModel wytrenowany!\nGÅ‚Ä™bokoÅ›Ä‡ drzewa: 5\nLiczba liÅ›ci: 20\n\n\n\n\n\n\n\n3) Ewaluacja modelu\n\n# Predykcje\ny_pred = dt_model.predict(X_test)\ny_pred_proba = dt_model.predict_proba(X_test)[:, 1]\n\n# Metryki\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nDokÅ‚adnoÅ›Ä‡ modelu: {accuracy:.1%}\")\n\n# SzczegÃ³Å‚owy raport\nprint(\"\\nRaport klasyfikacji:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:    Nie    Tak\")\nprint(f\"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki\n\n\n\n\n\n\n\n\nConfusion Matrix:\nPrzewidywane:    Nie    Tak\nRzeczywiste Nie: 122    34\nRzeczywiste Tak:  27   217"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#wizualizacja-drzewa-decyzyjnego",
    "href": "cheatsheets/03-decision-trees.html#wizualizacja-drzewa-decyzyjnego",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "ğŸ¨ Wizualizacja drzewa decyzyjnego",
    "text": "ğŸ¨ Wizualizacja drzewa decyzyjnego\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Tworzenie przykÅ‚adowych danych (kompletny przykÅ‚ad)\nnp.random.seed(42)\nn_clients = 2000\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_clients),\n    'dochod': np.random.lognormal(10, 0.5, n_clients),\n    'historia_kredytowa': np.random.choice(['dobra', 'Å›rednia', 'sÅ‚aba'], n_clients, p=[0.6, 0.3, 0.1]),\n    'zatrudnienie': np.random.choice(['staÅ‚e', 'tymczasowe', 'bezrobotny'], n_clients, p=[0.7, 0.25, 0.05]),\n    'oszczednosci': np.random.exponential(20000, n_clients),\n    'liczba_dzieci': np.random.randint(0, 4, n_clients)\n})\n\n# Prosta logika przyznawania kredytu\ndef czy_kredyt(row):\n    score = 0\n    if 30 &lt;= row['wiek'] &lt;= 50:\n        score += 2\n    if row['dochod'] &gt; 50000:\n        score += 3\n    if row['historia_kredytowa'] == 'dobra':\n        score += 2\n    if row['zatrudnienie'] == 'staÅ‚e':\n        score += 2\n    if row['oszczednosci'] &gt; 50000:\n        score += 1\n    score -= row['liczba_dzieci'] * 0.5\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 0.5)))\n    return probability &gt; 0.5\n\ndata['kredyt_przyznany'] = data.apply(czy_kredyt, axis=1)\n\n# Encoding i model\ndata_encoded = data.copy()\nle_historia = LabelEncoder()\nle_zatrudnienie = LabelEncoder()\ndata_encoded['historia_kredytowa_num'] = le_historia.fit_transform(data['historia_kredytowa'])\ndata_encoded['zatrudnienie_num'] = le_zatrudnienie.fit_transform(data['zatrudnienie'])\n\nfeature_columns = ['wiek', 'dochod', 'historia_kredytowa_num', 'zatrudnienie_num', 'oszczednosci', 'liczba_dzieci']\nX = data_encoded[feature_columns]\ny = data_encoded['kredyt_przyznany']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie drzewa\ndt_model = DecisionTreeClassifier(max_depth=5, min_samples_split=50, min_samples_leaf=20, random_state=42)\ndt_model.fit(X_train, y_train)\n\n# Ewaluacja\ny_pred = dt_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\n# WaÅ¼noÅ›Ä‡ features\nfeature_names = ['wiek', 'dochÃ³d', 'historia_kred.', 'zatrudnienie', 'oszczÄ™dnoÅ›ci', 'liczba_dzieci']\nimportance = dt_model.feature_importances_\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importance\n}).sort_values('importance', ascending=False)\n\n# Przygotuj wykres drzewa\nplt.figure(figsize=(20, 10))\ntree.plot_tree(dt_model, \n               feature_names=feature_names,\n               class_names=['Nie', 'Tak'],\n               filled=True,\n               rounded=True,\n               fontsize=10)\nplt.title(\"Drzewo decyzyjne - Przyznanie kredytu\")\nplt.close()\n\nprint(f\"Wyniki modelu Decision Tree:\")\nprint(f\"GÅ‚Ä™bokoÅ›Ä‡ drzewa: {dt_model.tree_.max_depth}\")\nprint(f\"Liczba liÅ›ci: {dt_model.tree_.n_leaves}\")\nprint(f\"DokÅ‚adnoÅ›Ä‡ modelu: {accuracy:.1%}\")\n\nprint(f\"\\nStatystyki danych:\")\nprint(f\"Dane treningowe: {len(X_train)} klientÃ³w\")\nprint(f\"Dane testowe: {len(X_test)} klientÃ³w\")\nprint(f\"Odsetek przyznanych kredytÃ³w: {data['kredyt_przyznany'].mean():.1%}\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:    Nie    Tak\")\nprint(f\"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}\")\n\nprint(\"\\nWaÅ¼noÅ›Ä‡ cech:\")\nfor _, row in feature_importance.iterrows():\n    print(f\"{row['feature']}: {row['importance']:.3f}\")\n\n# OdtwÃ³rz wykres drzewa\nplt.figure(figsize=(20, 10))\ntree.plot_tree(dt_model, \n               feature_names=feature_names,\n               class_names=['Nie', 'Tak'],\n               filled=True,\n               rounded=True,\n               fontsize=10)\nplt.title(\"Drzewo decyzyjne - Przyznanie kredytu\")\nplt.show()\n\n\n\n\n\n\n\nPokaÅ¼ wyniki i drzewo decyzyjne\n\n\n\n\n\n\n\nWyniki modelu Decision Tree:\nGÅ‚Ä™bokoÅ›Ä‡ drzewa: 5\nLiczba liÅ›ci: 13\nDokÅ‚adnoÅ›Ä‡ modelu: 95.0%\n\nStatystyki danych:\nDane treningowe: 1600 klientÃ³w\nDane testowe: 400 klientÃ³w\nOdsetek przyznanych kredytÃ³w: 93.2%\n\nConfusion Matrix:\nPrzewidywane:    Nie    Tak\nRzeczywiste Nie:  19    11\nRzeczywiste Tak:   9   361\n\nWaÅ¼noÅ›Ä‡ cech:\nzatrudnienie: 0.441\nwiek: 0.317\nhistoria_kred.: 0.178\nliczba_dzieci: 0.055\noszczÄ™dnoÅ›ci: 0.008\ndochÃ³d: 0.001\n\n\n\n\n\nWizualizacja drzewa decyzyjnego dla kredytÃ³w"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#interpretacja-decyzji-dla-konkretnego-klienta",
    "href": "cheatsheets/03-decision-trees.html#interpretacja-decyzji-dla-konkretnego-klienta",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "ğŸ” Interpretacja decyzji dla konkretnego klienta",
    "text": "ğŸ” Interpretacja decyzji dla konkretnego klienta\n\n# Funkcja do interpretacji Å›cieÅ¼ki decyzyjnej\ndef explain_decision(model, X_sample, feature_names):\n    # Pobierz Å›cieÅ¼kÄ™ w drzewie\n    leaf_id = model.decision_path(X_sample.reshape(1, -1)).toarray()[0]\n    feature = model.tree_.feature\n    threshold = model.tree_.threshold\n    \n    print(\"ÅšcieÅ¼ka decyzyjna:\")\n    for node_id in range(len(leaf_id)):\n        if leaf_id[node_id] == 1:  # jeÅ›li wÄ™zeÅ‚ jest na Å›cieÅ¼ce\n            if feature[node_id] != -2:  # jeÅ›li nie jest liÅ›ciem\n                feature_name = feature_names[feature[node_id]]\n                threshold_val = threshold[node_id]\n                feature_val = X_sample[feature[node_id]]\n                \n                if feature_val &lt;= threshold_val:\n                    condition = \"&lt;=\"\n                else:\n                    condition = \"&gt;\"\n                    \n                print(f\"  {feature_name} ({feature_val:.0f}) {condition} {threshold_val:.0f}\")\n\n# PrzykÅ‚ad dla konkretnego klienta\nsample_client = X_test.iloc[0]\nprediction = dt_model.predict([sample_client])[0]\nprobability = dt_model.predict_proba([sample_client])[0]\n\nprint(f\"Klient testowy:\")\nprint(f\"Wiek: {sample_client['wiek']}\")\nprint(f\"DochÃ³d: {sample_client['dochod']:.0f}\")\nprint(f\"OszczÄ™dnoÅ›ci: {sample_client['oszczednosci']:.0f}\")\nprint(f\"\\nDecyzja: {'KREDYT PRZYZNANY' if prediction else 'KREDYT ODRZUCONY'}\")\nprint(f\"PrawdopodobieÅ„stwo: {probability[1]:.1%}\")\n\nexplain_decision(dt_model, sample_client.values, feature_names)\n\n\n\n\n\n\n\nPokaÅ¼ dane klienta testowego\n\n\n\n\n\n\n\nKlient testowy:\nWiek: 56.0\nDochÃ³d: 7927\nOszczÄ™dnoÅ›ci: 32270\n\nDecyzja: KREDYT ODRZUCONY\nPrawdopodobieÅ„stwo: 3.0%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#rÃ³Å¼ne-zastosowania-decision-trees",
    "href": "cheatsheets/03-decision-trees.html#rÃ³Å¼ne-zastosowania-decision-trees",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "ğŸ¯ RÃ³Å¼ne zastosowania Decision Trees",
    "text": "ğŸ¯ RÃ³Å¼ne zastosowania Decision Trees\n\n1) Medyczna diagnoza\n\n# PrzykÅ‚ad klasyfikacji ryzyka chorÃ³b serca\nmedical_features = ['wiek', 'cholesterol', 'ciÅ›nienie', 'BMI', 'pali_papierosy']\n# Target: 'ryzyko_chorÃ³b_serca' (wysokie/niskie)\n\nmedical_tree = DecisionTreeClassifier(max_depth=4)\n# Model automatycznie znajdzie progi: \"JeÅ›li cholesterol &gt; 240 I BMI &gt; 30 ORAZ wiek &gt; 50...\"\n\n\n\n2) Marketing - segmentacja klientÃ³w\n\n# Przewidywanie czy klient kupi produkt premium\nmarketing_features = ['dochÃ³d_roczny', 'wiek', 'wyksztaÅ‚cenie', 'poprzednie_zakupy']\n# Target: 'kupi_premium' (tak/nie)\n\nmarketing_tree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=100)\n# Wynik: jasne reguÅ‚y marketingowe do targetowania reklam\n\n\n\n3) HR - decyzje o zatrudnieniu\n\n# Przewidywanie sukcesu kandydata w rekrutacji\nhr_features = ['doÅ›wiadczenie_lat', 'wyksztaÅ‚cenie', 'wynik_testÃ³w', 'referencje']\n# Target: 'zatrudniony' (tak/nie)\n\nhr_tree = DecisionTreeClassifier(max_depth=5)\n# Wynik: automatyczne zasady rekrutacyjne"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#tuning-parametrÃ³w",
    "href": "cheatsheets/03-decision-trees.html#tuning-parametrÃ³w",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "âš™ï¸ Tuning parametrÃ³w",
    "text": "âš™ï¸ Tuning parametrÃ³w\n\nfrom sklearn.model_selection import GridSearchCV\n\n# NajwaÅ¼niejsze parametry do tuningu\nparam_grid = {\n    'max_depth': [3, 5, 7, 10, None],\n    'min_samples_split': [20, 50, 100],\n    'min_samples_leaf': [10, 20, 50],\n    'criterion': ['gini', 'entropy']\n}\n\n# Grid search z cross-validation\ngrid_search = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(\"Najlepsze parametry:\")\nprint(grid_search.best_params_)\nprint(f\"Najlepsza dokÅ‚adnoÅ›Ä‡ CV: {grid_search.best_score_:.1%}\")\n\n# Model z najlepszymi parametrami\nbest_model = grid_search.best_estimator_\nbest_accuracy = accuracy_score(y_test, best_model.predict(X_test))\nprint(f\"DokÅ‚adnoÅ›Ä‡ na test set: {best_accuracy:.1%}\")\n\n\n\n\n\n\n\nPokaÅ¼ najlepsze parametry\n\n\n\n\n\n\n\nNajlepsze parametry:\n{'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 20}\nNajlepsza dokÅ‚adnoÅ›Ä‡ CV: 96.5%\nDokÅ‚adnoÅ›Ä‡ na test set: 95.0%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#problemy-i-rozwiÄ…zania",
    "href": "cheatsheets/03-decision-trees.html#problemy-i-rozwiÄ…zania",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "âš ï¸ Problemy i rozwiÄ…zania",
    "text": "âš ï¸ Problemy i rozwiÄ…zania\n\n1) Overfitting - drzewo za gÅ‚Ä™bokie\n\n# Problem: drzewo \"pamiÄ™ta\" dane treningowe\noverfitted_tree = DecisionTreeClassifier()  # bez ograniczeÅ„!\noverfitted_tree.fit(X_train, y_train)\n\ntrain_acc = accuracy_score(y_train, overfitted_tree.predict(X_train))\ntest_acc = accuracy_score(y_test, overfitted_tree.predict(X_test))\n\nprint(f\"Overfitted model:\")\nprint(f\"Train accuracy: {train_acc:.1%}\")\nprint(f\"Test accuracy: {test_acc:.1%}\")\nprint(f\"RÃ³Å¼nica: {train_acc - test_acc:.1%} - to overfitting!\")\n\n# RozwiÄ…zanie: ograniczenia\npruned_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=20)\npruned_tree.fit(X_train, y_train)\n\ntrain_acc_pruned = accuracy_score(y_train, pruned_tree.predict(X_train))\ntest_acc_pruned = accuracy_score(y_test, pruned_tree.predict(X_test))\n\nprint(f\"\\nPruned model:\")\nprint(f\"Train accuracy: {train_acc_pruned:.1%}\")\nprint(f\"Test accuracy: {test_acc_pruned:.1%}\")\nprint(f\"RÃ³Å¼nica: {train_acc_pruned - test_acc_pruned:.1%} - znacznie lepiej!\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki\n\n\n\n\n\n\n\nOverfitted model:\nTrain accuracy: 100.0%\nTest accuracy: 94.8%\nRÃ³Å¼nica: 5.2% - to overfitting!\n\nPruned model:\nTrain accuracy: 96.7%\nTest accuracy: 95.0%\nRÃ³Å¼nica: 1.7% - znacznie lepiej!\n\n\n\n\n\n\n\n2) Brak stabilnoÅ›ci - maÅ‚e zmiany = rÃ³Å¼ne drzewa\n\n# Problem demonstracji\nresults = []\nfor i in range(10):\n    # RÃ³Å¼ne random_state = rÃ³Å¼ne drzewa\n    tree_test = DecisionTreeClassifier(random_state=i, max_depth=5)\n    tree_test.fit(X_train, y_train)\n    acc = accuracy_score(y_test, tree_test.predict(X_test))\n    results.append(acc)\n\nprint(f\"DokÅ‚adnoÅ›Ä‡ rÃ³Å¼nych drzew: {min(results):.1%} - {max(results):.1%}\")\nprint(f\"Rozrzut: {max(results) - min(results):.1%}\")\nprint(\"RozwiÄ…zanie: Random Forest (nastÄ™pna Å›ciÄ…gawka!)\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki\n\n\n\n\n\n\n\nDokÅ‚adnoÅ›Ä‡ rÃ³Å¼nych drzew: 96.2% - 96.2%\nRozrzut: 0.0%\nRozwiÄ…zanie: Random Forest (nastÄ™pna Å›ciÄ…gawka!)"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#real-world-przypadki-uÅ¼ycia",
    "href": "cheatsheets/03-decision-trees.html#real-world-przypadki-uÅ¼ycia",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "ğŸŒ Real-world przypadki uÅ¼ycia",
    "text": "ğŸŒ Real-world przypadki uÅ¼ycia\n\nBankowoÅ›Ä‡: Ocena ryzyka kredytowego, wykrywanie fraudÃ³w\nMedycyna: Systemy wspomagania diagnostyki, triage pacjentÃ³w\n\nE-commerce: Rekomendacje produktÃ³w, ustalanie cen dynamicznych\nHR: Automatyzacja procesÃ³w rekrutacyjnych\nMarketing: Segmentacja klientÃ³w, personalizacja oferowania\n\n\n\n\n\n\n\nğŸ’¡ Kiedy uÅ¼ywaÄ‡ Decision Trees?\n\n\n\nâœ… UÅ»YJ GDY:\n\nPotrzebujesz interpretowalnego modelu\nDane majÄ… kategoryczne zmienne\n\nChcesz zrozumieÄ‡ â€œdlaczegoâ€ model podjÄ…Å‚ decyzjÄ™\nMasz nieliniowe zaleÅ¼noÅ›ci w danych\nBrakÃ³w w danych nie trzeba imputeâ€™owaÄ‡\n\nâŒ NIE UÅ»YWAJ GDY:\n\nPotrzebujesz najwyÅ¼szej dokÅ‚adnoÅ›ci (uÅ¼yj Random Forest/XGBoost)\nMasz bardzo gÅ‚Ä™bokie wzorce w danych (uÅ¼yj Neural Networks)\nDane sÄ… bardzo haÅ‚aÅ›liwe\nPrzewidujesz wartoÅ›ci ciÄ…gÅ‚e (lepiej Linear Regression)\n\n\n\nNastÄ™pna Å›ciÄ…gawka: Random Forest - zespÃ³Å‚ drzew (wkrÃ³tce)! ğŸŒ²ğŸŒ²ğŸŒ²"
  }
]