[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Witaj w Å›wiecie Machine Learning!",
    "section": "",
    "text": "Witaj w praktycznej kolekcji Å›ciÄ…g z Machine Learning! Znajdziesz tutaj algorytmy, teoriÄ™ i real-world przykÅ‚ady przedstawione w przystÄ™pny sposÃ³b dla kursantÃ³w Data Science. KaÅ¼da Å›ciÄ…gawka to kompaktowa porcja wiedzy gotowa do zastosowania.\n\n\n\nWyjaÅ›nienie algorytmu - jak dziaÅ‚a i dlaczego\nPraktyczne przykÅ‚ady kodu gotowe do skopiowania i uruchomienia\nReal-world zastosowania - kiedy i gdzie uÅ¼yÄ‡\nKluczowe parametry - co moÅ¼na dostroiÄ‡ i jak\nTipsy od praktykÃ³w - na co uwaÅ¼aÄ‡ w rzeczywistych projektach\n\n\n\n\nNajlepsza metoda nauki:\n\nPrzeczytaj teoriÄ™ - zrozum podstawy algorytmu\nPrzeanalizuj przykÅ‚ad - zobacz jak teoria przekÅ‚ada siÄ™ na kod\n\nSkopiuj snippety - uÅ¼yj przyciska ğŸ“‹ Copy w prawym gÃ³rnym rogu\nTestuj w swoim Å›rodowisku - Jupyter, Colab, VS Code\nEksperymentuj z parametrami - zmieÅ„ wartoÅ›ci i obserwuj efekty!\n\n\n\n\n\n\n\nğŸ’¡ Pro tip dla kursantÃ³w\n\n\n\nNie tylko kopiuj kod - zrozum co robi kaÅ¼da linia. SprÃ³buj zmieniÄ‡ dane wejÅ›ciowe, parametry algorytmu lub dodaÄ‡ wÅ‚asne modyfikacje. To najlepszy sposÃ³b na naukÄ™ ML!\n\n\n\n\n\n\n\n\nWprowadzenie do ML - Czym jest ML i jakie sÄ… gÅ‚Ã³wne rodzaje algorytmÃ³w\n\n\n\n\n\nLinear Regression - Predykcja wartoÅ›ci liczbowych\n\n\n\n\n\nDecision Trees - Proste drzewa decyzyjne\n\n\n\n\n\n\n\nğŸ”¥ Kolejne Å›ciÄ…gi w przygotowaniu!\n\n\n\n\nK-Means Clustering (segmentacja klientÃ³w)\nRandom Forest (ensembles)\nLogistic Regression (klasyfikacja binarna)\nFeature Engineering (przygotowanie danych)\nModel Evaluation (metryki i walidacja)\n\nÅšledÅº aktualizacje!\n\n\n\n\n\n\n\nZacznij od podstaw - Wprowadzenie do ML\nWybierz algorytm ktÃ³ry CiÄ™ interesuje z menu ML Cheatsheets\n\nPrzeczytaj teoriÄ™ - zrozum jak dziaÅ‚a algorytm\nSkopiuj kod (ikona ğŸ“‹ Copy w prawym gÃ³rnym rogu bloku)\nTestuj w praktyce - uruchom w Jupyter/Colab i eksperymentuj!\n\n\nPowodzenia w nauce Machine Learning! ğŸš€ğŸ¤–"
  },
  {
    "objectID": "index.html#co-znajdziesz-w-kaÅ¼dej-Å›ciÄ…gawce",
    "href": "index.html#co-znajdziesz-w-kaÅ¼dej-Å›ciÄ…gawce",
    "title": "Witaj w Å›wiecie Machine Learning!",
    "section": "",
    "text": "WyjaÅ›nienie algorytmu - jak dziaÅ‚a i dlaczego\nPraktyczne przykÅ‚ady kodu gotowe do skopiowania i uruchomienia\nReal-world zastosowania - kiedy i gdzie uÅ¼yÄ‡\nKluczowe parametry - co moÅ¼na dostroiÄ‡ i jak\nTipsy od praktykÃ³w - na co uwaÅ¼aÄ‡ w rzeczywistych projektach"
  },
  {
    "objectID": "index.html#jak-korzystaÄ‡-z-ml-cheatsheets",
    "href": "index.html#jak-korzystaÄ‡-z-ml-cheatsheets",
    "title": "Witaj w Å›wiecie Machine Learning!",
    "section": "",
    "text": "Najlepsza metoda nauki:\n\nPrzeczytaj teoriÄ™ - zrozum podstawy algorytmu\nPrzeanalizuj przykÅ‚ad - zobacz jak teoria przekÅ‚ada siÄ™ na kod\n\nSkopiuj snippety - uÅ¼yj przyciska ğŸ“‹ Copy w prawym gÃ³rnym rogu\nTestuj w swoim Å›rodowisku - Jupyter, Colab, VS Code\nEksperymentuj z parametrami - zmieÅ„ wartoÅ›ci i obserwuj efekty!\n\n\n\n\n\n\n\nğŸ’¡ Pro tip dla kursantÃ³w\n\n\n\nNie tylko kopiuj kod - zrozum co robi kaÅ¼da linia. SprÃ³buj zmieniÄ‡ dane wejÅ›ciowe, parametry algorytmu lub dodaÄ‡ wÅ‚asne modyfikacje. To najlepszy sposÃ³b na naukÄ™ ML!"
  },
  {
    "objectID": "index.html#dostÄ™pne-Å›ciÄ…gi-ml",
    "href": "index.html#dostÄ™pne-Å›ciÄ…gi-ml",
    "title": "Witaj w Å›wiecie Machine Learning!",
    "section": "",
    "text": "Wprowadzenie do ML - Czym jest ML i jakie sÄ… gÅ‚Ã³wne rodzaje algorytmÃ³w\n\n\n\n\n\nLinear Regression - Predykcja wartoÅ›ci liczbowych\n\n\n\n\n\nDecision Trees - Proste drzewa decyzyjne\n\n\n\n\n\n\n\nğŸ”¥ Kolejne Å›ciÄ…gi w przygotowaniu!\n\n\n\n\nK-Means Clustering (segmentacja klientÃ³w)\nRandom Forest (ensembles)\nLogistic Regression (klasyfikacja binarna)\nFeature Engineering (przygotowanie danych)\nModel Evaluation (metryki i walidacja)\n\nÅšledÅº aktualizacje!"
  },
  {
    "objectID": "index.html#szybki-start",
    "href": "index.html#szybki-start",
    "title": "Witaj w Å›wiecie Machine Learning!",
    "section": "",
    "text": "Zacznij od podstaw - Wprowadzenie do ML\nWybierz algorytm ktÃ³ry CiÄ™ interesuje z menu ML Cheatsheets\n\nPrzeczytaj teoriÄ™ - zrozum jak dziaÅ‚a algorytm\nSkopiuj kod (ikona ğŸ“‹ Copy w prawym gÃ³rnym rogu bloku)\nTestuj w praktyce - uruchom w Jupyter/Colab i eksperymentuj!\n\n\nPowodzenia w nauce Machine Learning! ğŸš€ğŸ¤–"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html",
    "href": "cheatsheets/02-linear-regression.html",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "",
    "text": "Linear Regression to jeden z najprostszych i najczÄ™Å›ciej uÅ¼ywanych algorytmÃ³w ML do przewidywania wartoÅ›ci liczbowych (np. ceny, temperatury, sprzedaÅ¼y). Szuka najlepszej linii prostej, ktÃ³ra opisuje zaleÅ¼noÅ›Ä‡ miÄ™dzy zmiennymi.\n\n\n\n\n\n\nğŸ’¡ Intuicja matematyczna\n\n\n\nSzukamy takiej linii y = ax + b, ktÃ³ra najlepiej â€œprzechodziâ€ przez nasze punkty danych. Algorytm automatycznie dobiera optymalne wartoÅ›ci a (slope) i b (intercept)."
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#czym-jest-linear-regression",
    "href": "cheatsheets/02-linear-regression.html#czym-jest-linear-regression",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "",
    "text": "Linear Regression to jeden z najprostszych i najczÄ™Å›ciej uÅ¼ywanych algorytmÃ³w ML do przewidywania wartoÅ›ci liczbowych (np. ceny, temperatury, sprzedaÅ¼y). Szuka najlepszej linii prostej, ktÃ³ra opisuje zaleÅ¼noÅ›Ä‡ miÄ™dzy zmiennymi.\n\n\n\n\n\n\nğŸ’¡ Intuicja matematyczna\n\n\n\nSzukamy takiej linii y = ax + b, ktÃ³ra najlepiej â€œprzechodziâ€ przez nasze punkty danych. Algorytm automatycznie dobiera optymalne wartoÅ›ci a (slope) i b (intercept)."
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#praktyczny-przykÅ‚ad-predykcja-cen-mieszkaÅ„",
    "href": "cheatsheets/02-linear-regression.html#praktyczny-przykÅ‚ad-predykcja-cen-mieszkaÅ„",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "ğŸ  Praktyczny przykÅ‚ad: predykcja cen mieszkaÅ„",
    "text": "ğŸ  Praktyczny przykÅ‚ad: predykcja cen mieszkaÅ„\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Tworzenie przykÅ‚adowych danych mieszkaÅ„\nnp.random.seed(42)\nn_mieszkan = 1000\n\ndata = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_mieszkan),\n    'liczba_pokoi': np.random.randint(1, 5, n_mieszkan),\n    'wiek_budynku': np.random.randint(0, 50, n_mieszkan),\n    'odleglosc_centrum': np.random.exponential(5, n_mieszkan)\n})\n\n# Realistyczna formuÅ‚a ceny (z szumem)\ndata['cena'] = (\n    data['powierzchnia'] * 8000 +           # 8k za mÂ²\n    data['liczba_pokoi'] * 15000 +          # 15k za pokÃ³j\n    (50 - data['wiek_budynku']) * 2000 +    # starsze = taÅ„sze\n    data['odleglosc_centrum'] * (-3000) +   # dalej = taÅ„sze\n    np.random.normal(0, 50000, n_mieszkan)  # szum losowy\n)\n\nprint(\"Podstawowe statystyki:\")\nprint(data.describe())\n\n\n\n\n\n\n\nPokaÅ¼ statystyki\n\n\n\n\n\n\n\nPodstawowe statystyki:\n       powierzchnia  liczba_pokoi  wiek_budynku  odleglosc_centrum  \\\ncount   1000.000000   1000.000000   1000.000000        1000.000000   \nmean      70.483301      2.473000     25.049000           4.971457   \nstd       24.480398      1.128958     14.384001           4.883716   \nmin      -11.031684      1.000000      0.000000           0.000058   \n25%       53.810242      1.000000     13.000000           1.459988   \n50%       70.632515      2.000000     25.000000           3.505541   \n75%       86.198597      4.000000     38.000000           6.954361   \nmax      166.318287      4.000000     49.000000          34.028756   \n\n               cena  \ncount  1.000000e+03  \nmean   6.363187e+05  \nstd    2.004394e+05  \nmin    1.686504e+04  \n25%    5.049745e+05  \n50%    6.356690e+05  \n75%    7.647808e+05  \nmax    1.440259e+06"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#trenowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/02-linear-regression.html#trenowanie-modelu-krok-po-kroku",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "ğŸ”§ Trenowanie modelu krok po kroku",
    "text": "ğŸ”§ Trenowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Sprawdzenie korelacji - ktÃ³re zmienne sÄ… najwaÅ¼niejsze?\ncorrelation = data.corr()['cena'].sort_values(ascending=False)\n\n# PodziaÅ‚ na features (X) i target (y)\nX = data[['powierzchnia', 'liczba_pokoi', 'wiek_budynku', 'odleglosc_centrum']]\ny = data['cena']\n\n# PodziaÅ‚ train/test (80/20)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"Korelacja z cenÄ…:\")\nprint(correlation)\n\nprint(f\"Dane treningowe: {X_train.shape[0]} mieszkaÅ„\")\nprint(f\"Dane testowe: {X_test.shape[0]} mieszkaÅ„\")\n\n\n\n\n\n\n\nPokaÅ¼ korelacje oraz dane treningowe\n\n\n\n\n\n\n\nKorelacja z cenÄ…:\ncena                 1.000000\npowierzchnia         0.949080\nliczba_pokoi         0.025094\nodleglosc_centrum   -0.042040\nwiek_budynku        -0.115566\nName: cena, dtype: float64\nDane treningowe: 800 mieszkaÅ„\nDane testowe: 200 mieszkaÅ„\n\n\n\n\n\n\n\n2) Trenowanie modelu\n\n# Utworzenie i trenowanie modelu\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Sprawdzenie wspÃ³Å‚czynnikÃ³w\nprint(\"WspÃ³Å‚czynniki modelu:\")\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"{feature}: {coef:.0f} zÅ‚\")\nprint(f\"Intercept (staÅ‚a): {model.intercept_:.0f} zÅ‚\")\n\n\n\n\n\n\n\nPokaÅ¼ wspÃ³Å‚czynniki\n\n\n\n\n\n\n\nWspÃ³Å‚czynniki modelu:\npowierzchnia: 7924 zÅ‚\nliczba_pokoi: 15811 zÅ‚\nwiek_budynku: -2108 zÅ‚\nodleglosc_centrum: -2698 zÅ‚\nIntercept (staÅ‚a): 104042 zÅ‚\n\n\n\n\n\n\n\n3) Ewaluacja wynikÃ³w\n\n# Predykcje na zbiorze testowym\ny_pred = model.predict(X_test)\n\n# Metryki\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"\\nWyniki modelu:\")\nprint(f\"Mean Absolute Error: {mae:.0f} zÅ‚\")\nprint(f\"RÂ² Score: {r2:.3f}\")\nprint(f\"Åšredni bÅ‚Ä…d to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania\")\n\n# PrzykÅ‚adowe predykcje\nfor i in range(5):\n    rzeczywista = y_test.iloc[i]\n    przewidywana = y_pred[i]\n    bÅ‚Ä…d = abs(rzeczywista - przewidywana)\n    print(f\"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}zÅ‚, \"\n          f\"przewidywana={przewidywana:.0f}zÅ‚, bÅ‚Ä…d={bÅ‚Ä…d:.0f}zÅ‚\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki modelu\n\n\n\n\n\n\n\n\nWyniki modelu:\nMean Absolute Error: 38912 zÅ‚\nRÂ² Score: 0.934\nÅšredni bÅ‚Ä…d to 6.1% ceny mieszkania\nMieszkanie 1: rzeczywista=702313zÅ‚, przewidywana=771489zÅ‚, bÅ‚Ä…d=69177zÅ‚\nMieszkanie 2: rzeczywista=791174zÅ‚, przewidywana=816756zÅ‚, bÅ‚Ä…d=25582zÅ‚\nMieszkanie 3: rzeczywista=326702zÅ‚, przewidywana=274297zÅ‚, bÅ‚Ä…d=52405zÅ‚\nMieszkanie 4: rzeczywista=441380zÅ‚, przewidywana=456054zÅ‚, bÅ‚Ä…d=14674zÅ‚\nMieszkanie 5: rzeczywista=445427zÅ‚, przewidywana=383199zÅ‚, bÅ‚Ä…d=62228zÅ‚"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#wizualizacja-wynikÃ³w",
    "href": "cheatsheets/02-linear-regression.html#wizualizacja-wynikÃ³w",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "ğŸ“Š Wizualizacja wynikÃ³w",
    "text": "ğŸ“Š Wizualizacja wynikÃ³w\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\n\n# PrzykÅ‚adowe dane mieszkaÅ„ (powtarzamy dla demonstracji)\nnp.random.seed(42)\nn_mieszkan = 1000\n\ndata = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_mieszkan),\n    'liczba_pokoi': np.random.randint(1, 5, n_mieszkan),\n    'wiek_budynku': np.random.randint(0, 50, n_mieszkan),\n    'odleglosc_centrum': np.random.exponential(5, n_mieszkan)\n})\n\ndata['cena'] = (\n    data['powierzchnia'] * 8000 +\n    data['liczba_pokoi'] * 15000 +\n    (50 - data['wiek_budynku']) * 2000 +\n    data['odleglosc_centrum'] * (-3000) +\n    np.random.normal(0, 50000, n_mieszkan)\n)\n\n# Przygotowanie i trenowanie modelu\nX = data[['powierzchnia', 'liczba_pokoi', 'wiek_budynku', 'odleglosc_centrum']]\ny = data['cena']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Oblicz metryki\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Przygotuj wykresy\nplt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Rzeczywiste ceny')\nplt.ylabel('Przewidywane ceny')\nplt.title('Rzeczywiste vs Przewidywane')\n\nplt.subplot(1, 2, 2)\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Przewidywane ceny')\nplt.ylabel('Residuals (bÅ‚Ä™dy)')\nplt.title('Analiza residuals')\n\nplt.tight_layout()\n\n# Zapisz wykres do zmiennej\nimport io\nimport base64\nbuf = io.BytesIO()\nplt.savefig(buf, format='png', dpi=100, bbox_inches='tight')\nbuf.seek(0)\nplt.close()\n\n# Przygotuj przykÅ‚adowe predykcje\nprzykladowe_predykcje = []\nfor i in range(3):\n    rzeczywista = y_test.iloc[i]\n    przewidywana = y_pred[i]\n    blad = abs(rzeczywista - przewidywana)\n    przykladowe_predykcje.append(f\"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}zÅ‚, przewidywana={przewidywana:.0f}zÅ‚, bÅ‚Ä…d={blad:.0f}zÅ‚\")\n\nprint(f\"Wyniki modelu Linear Regression:\")\nprint(f\"Mean Absolute Error: {mae:.0f} zÅ‚\")\nprint(f\"RÂ² Score: {r2:.3f}\")\nprint(f\"Åšredni bÅ‚Ä…d to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania\")\nprint(f\"\\nPrzykÅ‚adowe predykcje:\")\nfor pred in przykladowe_predykcje:\n    print(pred)\n\n# OdtwÃ³rz wykres\nplt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Rzeczywiste ceny')\nplt.ylabel('Przewidywane ceny')\nplt.title('Rzeczywiste vs Przewidywane')\n\nplt.subplot(1, 2, 2)\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Przewidywane ceny')\nplt.ylabel('Residuals (bÅ‚Ä™dy)')\nplt.title('Analiza residuals')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nPokaÅ¼ wyniki i wykresy analizy\n\n\n\n\n\n\n\nWyniki modelu Linear Regression:\nMean Absolute Error: 38912 zÅ‚\nRÂ² Score: 0.934\nÅšredni bÅ‚Ä…d to 6.1% ceny mieszkania\n\nPrzykÅ‚adowe predykcje:\nMieszkanie 1: rzeczywista=702313zÅ‚, przewidywana=771489zÅ‚, bÅ‚Ä…d=69177zÅ‚\nMieszkanie 2: rzeczywista=791174zÅ‚, przewidywana=816756zÅ‚, bÅ‚Ä…d=25582zÅ‚\nMieszkanie 3: rzeczywista=326702zÅ‚, przewidywana=274297zÅ‚, bÅ‚Ä…d=52405zÅ‚\n\n\n\n\n\nAnaliza wynikÃ³w modelu Linear Regression"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#praktyczne-zastosowania-linear-regression",
    "href": "cheatsheets/02-linear-regression.html#praktyczne-zastosowania-linear-regression",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "ğŸ¯ Praktyczne zastosowania Linear Regression",
    "text": "ğŸ¯ Praktyczne zastosowania Linear Regression\n\n1) Biznesowe\n\n# Przewidywanie sprzedaÅ¼y na podstawie budÅ¼etu marketingowego\nsales_data = pd.DataFrame({\n    'marketing_budget': [10000, 15000, 20000, 25000, 30000],\n    'sales': [100000, 140000, 180000, 220000, 260000]\n})\n\nmodel_sales = LinearRegression()\nmodel_sales.fit(sales_data[['marketing_budget']], sales_data['sales'])\n\n# \"JeÅ›li zwiÄ™kszÄ™ budÅ¼et do 35k, sprzedaÅ¼ wzroÅ›nie do:\"\nnew_sales = model_sales.predict([[35000]])\nprint(f\"Przewidywana sprzedaÅ¼ przy budÅ¼ecie 35k: {new_sales[0]:.0f}\")\n\n\n\n\n\n\n\nPokaÅ¼ przewidywanÄ… sprzedaÅ¼\n\n\n\n\n\n\n\nPrzewidywana sprzedaÅ¼ przy budÅ¼ecie 35k: 300000\n\n\n\n\n\n\n\n2) Analizy finansowe\n\n# Relacja miÄ™dzy PKB a konsumpcjÄ…\neconomics_data = pd.DataFrame({\n    'pkb_per_capita': [25000, 30000, 35000, 40000, 45000],\n    'konsumpcja': [18000, 21000, 24000, 27000, 30000]\n})\n\nmodel_econ = LinearRegression()\nmodel_econ.fit(economics_data[['pkb_per_capita']], economics_data['konsumpcja'])\n\n# WspÃ³Å‚czynnik skÅ‚onnoÅ›ci do konsumpcji\nprint(f\"Na kaÅ¼de dodatkowe 1000zÅ‚ PKB, konsumpcja roÅ›nie o: {model_econ.coef_[0]:.0f}zÅ‚\")\n\n\n\n\n\n\n\nPokaÅ¼ wspÃ³Å‚czynnik skÅ‚onnoÅ›ci do konsumpcji\n\n\n\n\n\n\n\nNa kaÅ¼de dodatkowe 1000zÅ‚ PKB, konsumpcja roÅ›nie o: 1zÅ‚"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#najczÄ™stsze-puÅ‚apki-i-jak-ich-unikaÄ‡",
    "href": "cheatsheets/02-linear-regression.html#najczÄ™stsze-puÅ‚apki-i-jak-ich-unikaÄ‡",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "âš ï¸ NajczÄ™stsze puÅ‚apki i jak ich unikaÄ‡",
    "text": "âš ï¸ NajczÄ™stsze puÅ‚apki i jak ich unikaÄ‡\n\n1) Overfitting z wieloma zmiennymi\n\n# ZÅO: za duÅ¼o features wzglÄ™dem danych\n# JeÅ›li masz 100 mieszkaÅ„, nie uÅ¼ywaj 50 features!\n\n# DOBRZE: zasada kciuka\ndef check_features_ratio(X, y):\n    ratio = len(y) / X.shape[1]\n    if ratio &lt; 10:\n        print(f\"âš ï¸ Uwaga: masz tylko {ratio:.1f} obserwacji na feature!\")\n        print(\"RozwaÅ¼: wiÄ™cej danych lub mniej features\")\n    else:\n        print(f\"âœ… OK: {ratio:.1f} obserwacji na feature\")\n\ncheck_features_ratio(X_train, y_train)\n\n\n\n\n\n\n\nPokaÅ¼ wyniki\n\n\n\n\n\n\n\nâœ… OK: 200.0 obserwacji na feature\n\n\n\n\n\n\n\n2) Sprawdzanie zaÅ‚oÅ¼eÅ„ linearnoÅ›ci\n\n# SprawdÅº czy zaleÅ¼noÅ›ci sÄ… rzeczywiÅ›cie liniowe\nfrom scipy import stats\n\nfor col in X.columns:\n    correlation, p_value = stats.pearsonr(data[col], data['cena'])\n    print(f\"{col}: korelacja={correlation:.3f}, p-value={p_value:.3f}\")\n    \n    if abs(correlation) &lt; 0.1:\n        print(f\"âš ï¸ {col} ma sÅ‚abÄ… korelacjÄ™ - moÅ¼e nie byÄ‡ przydatna\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki korelacji\n\n\n\n\n\n\n\npowierzchnia: korelacja=0.949, p-value=0.000\nliczba_pokoi: korelacja=0.025, p-value=0.428\nâš ï¸ liczba_pokoi ma sÅ‚abÄ… korelacjÄ™ - moÅ¼e nie byÄ‡ przydatna\nwiek_budynku: korelacja=-0.116, p-value=0.000\nodleglosc_centrum: korelacja=-0.042, p-value=0.184\nâš ï¸ odleglosc_centrum ma sÅ‚abÄ… korelacjÄ™ - moÅ¼e nie byÄ‡ przydatna\n\n\n\n\n\n\n\n3) WielokoliniowoÅ›Ä‡ (features korelujÄ… miÄ™dzy sobÄ…)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# PrzykÅ‚adowe dane features (demonstracja)\nnp.random.seed(42)\nn_samples = 1000\n\nX = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_samples),\n    'liczba_pokoi': np.random.randint(1, 5, n_samples),\n    'wiek_budynku': np.random.randint(0, 50, n_samples),\n    'odleglosc_centrum': np.random.exponential(5, n_samples)\n})\n\n# Oblicz korelacje miÄ™dzy features\ncorrelation_matrix = X.corr()\n\n# Przygotuj wykres\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Korelacje miÄ™dzy features')\nplt.close()\n\nprint(\"Macierz korelacji miÄ™dzy zmiennymi:\")\nprint(correlation_matrix)\nprint(\"\\nInterpretacja:\")\nprint(\"â€¢ WartoÅ›ci blisko 1.0 = silna korelacja pozytywna\")\nprint(\"â€¢ WartoÅ›ci blisko -1.0 = silna korelacja negatywna\") \nprint(\"â€¢ WartoÅ›ci blisko 0.0 = brak korelacji\")\nprint(\"\\nJeÅ›li korelacja miÄ™dzy features &gt; 0.8, usuÅ„ jednÄ… z nich (multicollinearity)!\")\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Korelacje miÄ™dzy features')\nplt.show()\n\n\n\n\n\n\n\nPokaÅ¼ mapÄ™ korelacji\n\n\n\n\n\n\n\nMacierz korelacji miÄ™dzy zmiennymi:\n                   powierzchnia  liczba_pokoi  wiek_budynku  odleglosc_centrum\npowierzchnia           1.000000     -0.064763      0.033920           0.029856\nliczba_pokoi          -0.064763      1.000000     -0.003894          -0.057555\nwiek_budynku           0.033920     -0.003894      1.000000          -0.037242\nodleglosc_centrum      0.029856     -0.057555     -0.037242           1.000000\n\nInterpretacja:\nâ€¢ WartoÅ›ci blisko 1.0 = silna korelacja pozytywna\nâ€¢ WartoÅ›ci blisko -1.0 = silna korelacja negatywna\nâ€¢ WartoÅ›ci blisko 0.0 = brak korelacji\n\nJeÅ›li korelacja miÄ™dzy features &gt; 0.8, usuÅ„ jednÄ… z nich (multicollinearity)!\n\n\n\n\n\nMapa korelacji miÄ™dzy zmiennymi"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#parametry-do-tuningu",
    "href": "cheatsheets/02-linear-regression.html#parametry-do-tuningu",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "ğŸ”§ Parametry do tuningu",
    "text": "ğŸ”§ Parametry do tuningu\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n# 1) Standardization - gdy features majÄ… rÃ³Å¼ne skale\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train)\n\n# 2) Polynomial Features - dla nieliniowych zaleÅ¼noÅ›ci\npoly_model = Pipeline([\n    ('poly', PolynomialFeatures(degree=2)),\n    ('linear', LinearRegression())\n])\npoly_model.fit(X_train, y_train)\n\npoly_pred = poly_model.predict(X_test)\npoly_r2 = r2_score(y_test, poly_pred)\nprint(f\"Polynomial Regression RÂ²: {poly_r2:.3f}\")\n\n# 3) Regularization - Ridge i Lasso\nfrom sklearn.linear_model import Ridge, Lasso\n\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\nridge_r2 = r2_score(y_test, ridge.predict(X_test))\n\nlasso = Lasso(alpha=1.0)\nlasso.fit(X_train, y_train)\nlasso_r2 = r2_score(y_test, lasso.predict(X_test))\n\nprint(f\"Ridge RÂ²: {ridge_r2:.3f}\")\nprint(f\"Lasso RÂ²: {lasso_r2:.3f}\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki predykcji cen\n\n\n\n\n\n\n\nPolynomial Regression RÂ²: 0.933\nRidge RÂ²: 0.934\nLasso RÂ²: 0.934"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#real-world-przykÅ‚ady-zastosowaÅ„",
    "href": "cheatsheets/02-linear-regression.html#real-world-przykÅ‚ady-zastosowaÅ„",
    "title": "Linear Regression â€” przewiduj wartoÅ›ci liczbowe",
    "section": "ğŸŒ Real-world przykÅ‚ady zastosowaÅ„",
    "text": "ğŸŒ Real-world przykÅ‚ady zastosowaÅ„\n\nE-commerce: Przewidywanie wartoÅ›ci Å¼yciowej klienta (LTV)\nNieruchomoÅ›ci: Automatyczna wycena domÃ³w (Zillow)\nFinanse: Scoring kredytowy, przewidywanie cen akcji\nMarketing: ROI kampanii reklamowych\nSupply Chain: Prognozowanie popytu na produkty\n\n\n\n\n\n\n\nğŸ’¡ Kiedy uÅ¼ywaÄ‡ Linear Regression?\n\n\n\nâœ… UÅ»YJ GDY:\n\nPrzewidujesz wartoÅ›ci liczbowe (continuous target)\nZaleÅ¼noÅ›ci wydajÄ… siÄ™ liniowe\nChcesz interpretowalny model\nMasz stosunkowo maÅ‚o features\n\nâŒ NIE UÅ»YWAJ GDY:\n\nTarget jest kategoryczny (uÅ¼yj klasyfikacji)\nZaleÅ¼noÅ›ci sÄ… bardzo nieliniowe (uÅ¼yj Random Forest, XGBoost)\nMasz tysiÄ…ce features (uÅ¼yj regularization)\n\n\n\nNastÄ™pna Å›ciÄ…gawka: Decision Trees - klasyfikacja ğŸŒ³"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html",
    "href": "cheatsheets/01-intro-ml.html",
    "title": "Wprowadzenie do Machine Learning â€” fundament",
    "section": "",
    "text": "Machine Learning (ML) to dziedzina informatyki, ktÃ³ra pozwala komputerom uczyÄ‡ siÄ™ i podejmowaÄ‡ decyzje na podstawie danych, bez koniecznoÅ›ci programowania kaÅ¼dej reguÅ‚y z gÃ³ry.\n\n\n\n\n\n\nğŸ’¡ Intuicja\n\n\n\nZamiast pisaÄ‡ kod â€œjeÅ›li temperatura &gt; 25Â°C, to bÄ™dzie sÅ‚onecznieâ€, ML pozwala algorytmowi samemu odkryÄ‡ te zaleÅ¼noÅ›ci z historycznych danych pogodowych."
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#czym-jest-machine-learning",
    "href": "cheatsheets/01-intro-ml.html#czym-jest-machine-learning",
    "title": "Wprowadzenie do Machine Learning â€” fundament",
    "section": "",
    "text": "Machine Learning (ML) to dziedzina informatyki, ktÃ³ra pozwala komputerom uczyÄ‡ siÄ™ i podejmowaÄ‡ decyzje na podstawie danych, bez koniecznoÅ›ci programowania kaÅ¼dej reguÅ‚y z gÃ³ry.\n\n\n\n\n\n\nğŸ’¡ Intuicja\n\n\n\nZamiast pisaÄ‡ kod â€œjeÅ›li temperatura &gt; 25Â°C, to bÄ™dzie sÅ‚onecznieâ€, ML pozwala algorytmowi samemu odkryÄ‡ te zaleÅ¼noÅ›ci z historycznych danych pogodowych."
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#gÅ‚Ã³wne-rodzaje-ml",
    "href": "cheatsheets/01-intro-ml.html#gÅ‚Ã³wne-rodzaje-ml",
    "title": "Wprowadzenie do Machine Learning â€” fundament",
    "section": "ğŸ“Š GÅ‚Ã³wne rodzaje ML",
    "text": "ğŸ“Š GÅ‚Ã³wne rodzaje ML\n\n1) Supervised Learning (Uczenie nadzorowane)\nMamy dane + znamy prawidÅ‚owe odpowiedzi\n\n# PrzykÅ‚ad: predykcja ceny domu\n# Dane wejÅ›ciowe: powierzchnia, lokalizacja, rok budowy\n# Cel: przewidzieÄ‡ cenÄ™\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# PrzykÅ‚adowe dane\ndata = pd.DataFrame({\n    'powierzchnia': [50, 75, 100, 120, 150],\n    'rok_budowy': [1990, 2000, 2010, 2015, 2020],\n    'cena': [300000, 400000, 550000, 650000, 800000]  # znamy prawdziwe ceny!\n})\n\n# Trenowanie modelu\nmodel = LinearRegression()\nX = data[['powierzchnia', 'rok_budowy']]\ny = data['cena']\nmodel.fit(X, y)\n\n# Predykcja dla nowego domu\nnowy_dom = [[90, 2005]]\nprzewidywana_cena = model.predict(nowy_dom)\n\n# Zapisz wyniki do zmiennych dla expandera\ndane_treningowe = data.to_string()\nwynik_predykcji = f\"Przewidywana cena: {przewidywana_cena[0]:.0f} zÅ‚\"\n\nprint(\"Dane treningowe:\")\nprint(dane_treningowe)\nprint(f\"\\nPredykcja dla domu 90mÂ², rok 2005:\")\nprint(wynik_predykcji)\n\n\n\n\n\n\n\nPokaÅ¼ wyniki predykcji cen\n\n\n\n\n\n\n\nDane treningowe:\n   powierzchnia  rok_budowy    cena\n0            50        1990  300000\n1            75        2000  400000\n2           100        2010  550000\n3           120        2015  650000\n4           150        2020  800000\n\nPredykcja dla domu 90mÂ², rok 2005:\nPrzewidywana cena: 493819 zÅ‚\n\n\n\n\n\nReal-world zastosowania:\n\nPredykcja cen akcji/nieruchomoÅ›ci\nDiagnoza medyczna (klasyfikacja chorÃ³b)\nFiltrowanie spamu w emailach\nRozpoznawanie mowy/obrazÃ³w\n\n\n\n\n2) Unsupervised Learning (Uczenie nienadzorowane)\nMamy tylko dane, szukamy ukrytych wzorcÃ³w\n\n# PrzykÅ‚ad: segmentacja klientÃ³w sklepu\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Dane klientÃ³w: wiek i wydatki miesiÄ™czne\nklienci = pd.DataFrame({\n    'wiek': [25, 30, 35, 22, 28, 45, 50, 55, 60, 65],\n    'wydatki': [2000, 2500, 3000, 1800, 2200, 4000, 4500, 3500, 3000, 2800]\n})\n\n# Grupowanie klientÃ³w w 3 segmenty\nkmeans = KMeans(n_clusters=3, random_state=42)\nklienci['segment'] = kmeans.fit_predict(klienci[['wiek', 'wydatki']])\n\n# Przygotuj wyniki do wyÅ›wietlenia\ndane_klientow = klienci.head().to_string()\nsegmenty_info = []\nfor i in range(3):\n    segment = klienci[klienci['segment'] == i]\n    segmenty_info.append(f\"Segment {i}: Å›redni wiek {segment['wiek'].mean():.0f}, Å›rednie wydatki {segment['wydatki'].mean():.0f}\")\n\nprzyklad_predykcji = kmeans.predict([[30, 2500]])[0]\n\nprint(\"Dane klientÃ³w:\")\nprint(dane_klientow)\nprint(\"\\nSegmenty klientÃ³w:\")\nfor info in segmenty_info:\n    print(info)\nprint(f\"\\nPrzykÅ‚ad: klient 30 lat, wydaje 2500zÅ‚ -&gt; segment {przyklad_predykcji}\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki segmentacji klientÃ³w\n\n\n\n\n\n\n\nDane klientÃ³w:\n   wiek  wydatki  segment\n0    25     2000        0\n1    30     2500        2\n2    35     3000        2\n3    22     1800        0\n4    28     2200        0\n\nSegmenty klientÃ³w:\nSegment 0: Å›redni wiek 25, Å›rednie wydatki 2000\nSegment 1: Å›redni wiek 50, Å›rednie wydatki 4000\nSegment 2: Å›redni wiek 48, Å›rednie wydatki 2825\n\nPrzykÅ‚ad: klient 30 lat, wydaje 2500zÅ‚ -&gt; segment 2\n\n\n\n\n\nReal-world zastosowania:\n\nSegmentacja klientÃ³w (marketing)\nWykrywanie anomalii (cyberbezpieczeÅ„stwo)\nAnaliza koszykowa (co kupujÄ… razem)\nKompresja danych\n\n\n\n\n3) Reinforcement Learning (Uczenie ze wzmocnieniem)\nAgent uczy siÄ™ przez interakcjÄ™ i nagrody/kary\n\n# PrzykÅ‚ad koncepcyjny: optymalizacja reklam\nclass SimpleAgent:\n    def __init__(self):\n        # Jakie reklamy pokazywaÄ‡: [\"sportowe\", \"technologiczne\", \"modowe\"]\n        self.ad_types = [\"sportowe\", \"technologiczne\", \"modowe\"]\n        self.rewards = [0, 0, 0]  # nagrody za kaÅ¼dy typ\n        self.counts = [0, 0, 0]   # ile razy pokazane\n    \n    def choose_ad(self):\n        # Wybierz reklamÄ™ z najwyÅ¼szÄ… Å›redniÄ… nagrodÄ…\n        avg_rewards = [r/max(c,1) for r, c in zip(self.rewards, self.counts)]\n        return avg_rewards.index(max(avg_rewards))\n    \n    def update_reward(self, ad_type, clicked):\n        # Aktualizuj nagrody na podstawie klikniÄ™Ä‡\n        self.counts[ad_type] += 1\n        if clicked:\n            self.rewards[ad_type] += 1\n    \n    def get_stats(self):\n        stats_list = []\n        for i, ad_type in enumerate(self.ad_types):\n            rate = self.rewards[i] / max(self.counts[i], 1) * 100\n            stats_list.append(f\"{ad_type}: {self.counts[i]} pokazaÅ„, {self.rewards[i]} klikniÄ™Ä‡ ({rate:.1f}%)\")\n        return stats_list\n\n# Symulacja\nimport numpy as np\nnp.random.seed(42)\n\nagent = SimpleAgent()\nsymulacja_wyniki = []\nsymulacja_wyniki.append(\"ğŸ¯ Symulacja optymalizacji reklam:\")\nsymulacja_wyniki.append(\"Agent uczy siÄ™, ktÃ³re reklamy dziaÅ‚ajÄ… najlepiej...\")\n\nfor day in range(10):\n    ad = agent.choose_ad()\n    # RÃ³Å¼ne prawdopodobieÅ„stwa klikniÄ™Ä‡ dla rÃ³Å¼nych typÃ³w reklam\n    click_probs = [0.1, 0.3, 0.2]  # technologiczne najlepsze\n    clicked = np.random.random() &lt; click_probs[ad]\n    agent.update_reward(ad, clicked)\n    symulacja_wyniki.append(f\"DzieÅ„ {day+1}: pokazano {agent.ad_types[ad]}, klikniÄ™ta: {clicked}\")\n\nkoncowe_statystyki = agent.get_stats()\n\nfor wynik in symulacja_wyniki:\n    print(wynik)\nprint(\"\\nğŸ“Š KoÅ„cowe statystyki:\")\nfor stat in koncowe_statystyki:\n    print(stat)\n\n\n\n\n\n\n\nPokaÅ¼ symulacjÄ™ agenta reklamowego\n\n\n\n\n\n\n\nğŸ¯ Symulacja optymalizacji reklam:\nAgent uczy siÄ™, ktÃ³re reklamy dziaÅ‚ajÄ… najlepiej...\nDzieÅ„ 1: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 2: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 3: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 4: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 5: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 6: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 7: pokazano sportowe, klikniÄ™ta: True\nDzieÅ„ 8: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 9: pokazano sportowe, klikniÄ™ta: False\nDzieÅ„ 10: pokazano sportowe, klikniÄ™ta: False\n\nğŸ“Š KoÅ„cowe statystyki:\nsportowe: 10 pokazaÅ„, 1 klikniÄ™Ä‡ (10.0%)\ntechnologiczne: 0 pokazaÅ„, 0 klikniÄ™Ä‡ (0.0%)\nmodowe: 0 pokazaÅ„, 0 klikniÄ™Ä‡ (0.0%)\n\n\n\n\n\nReal-world zastosowania:\n\nGry komputerowe (AI graczy)\nAutonomiczne pojazdy\nOptymalizacja reklam online\nRoboty przemysÅ‚owe"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#jak-wybraÄ‡-odpowiedni-typ-ml",
    "href": "cheatsheets/01-intro-ml.html#jak-wybraÄ‡-odpowiedni-typ-ml",
    "title": "Wprowadzenie do Machine Learning â€” fundament",
    "section": "ğŸ¯ Jak wybraÄ‡ odpowiedni typ ML?",
    "text": "ğŸ¯ Jak wybraÄ‡ odpowiedni typ ML?\n\n\n\n\n\n\n\n\nSytuacja\nTyp ML\nPrzykÅ‚ad\n\n\n\n\nMasz dane z prawidÅ‚owymi odpowiedziami\nSupervised\nSpam/nie-spam w emailach\n\n\nChcesz znaleÅºÄ‡ ukryte grupÑ‹\nUnsupervised\nSegmentacja klientÃ³w\n\n\nSystem ma siÄ™ uczyÄ‡ przez trial & error\nReinforcement\nBot do gier"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#podstawowe-kroki-projektu-ml",
    "href": "cheatsheets/01-intro-ml.html#podstawowe-kroki-projektu-ml",
    "title": "Wprowadzenie do Machine Learning â€” fundament",
    "section": "ğŸ”§ Podstawowe kroki projektu ML",
    "text": "ğŸ”§ Podstawowe kroki projektu ML\n\n# Kompletny workflow ML na przykÅ‚adzie klasyfikacji iris\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# 1. ZaÅ‚aduj i poznaj dane\niris = load_iris()\ndata = pd.DataFrame(iris.data, columns=iris.feature_names)\ndata['target'] = iris.target\ntarget_names = iris.target_names\n\n# 2. Przygotuj dane (czyszczenie, encoding)\nX = data.drop('target', axis=1)\ny = data['target']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 3. Podziel na train/test\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# 4. Wybierz i wytrenuj model\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# 5. OceÅ„ wyniki\npredictions = model.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\n\n# PrzykÅ‚ad predykcji\nsample_flower = X_test[0:1]\npredicted_class = model.predict(sample_flower)[0]\npredicted_name = target_names[predicted_class]\nactual_name = target_names[y_test.iloc[0]]\n\n# Przygotuj wyniki do wyÅ›wietlenia\nwyniki_workflow = []\nwyniki_workflow.append(\"ğŸ”¬ Kompletny workflow projektu ML\")\nwyniki_workflow.append(\"=\" * 40)\nwyniki_workflow.append(\"1ï¸âƒ£ Dane zaÅ‚adowane:\")\nwyniki_workflow.append(f\"KsztaÅ‚t: {data.shape}\")\nwyniki_workflow.append(f\"Klasy: {target_names}\")\nwyniki_workflow.append(f\"Pierwsze 3 wiersze:\\n{data.head(3).to_string()}\")\nwyniki_workflow.append(f\"\\n2ï¸âƒ£ Dane przeskalowane (pierwsze 3 cechy pierwszej prÃ³bki):\")\nwyniki_workflow.append(f\"Przed: {X.iloc[0, :3].values}\")\nwyniki_workflow.append(f\"Po: {X_scaled[0, :3]}\")\nwyniki_workflow.append(f\"\\n3ï¸âƒ£ PodziaÅ‚ danych:\")\nwyniki_workflow.append(f\"Train: {len(X_train)} prÃ³bek\")\nwyniki_workflow.append(f\"Test: {len(X_test)} prÃ³bek\")\nwyniki_workflow.append(f\"\\n4ï¸âƒ£ Model wytrenowany: RandomForestClassifier\")\nwyniki_workflow.append(f\"\\n5ï¸âƒ£ Wyniki:\")\nwyniki_workflow.append(f\"DokÅ‚adnoÅ›Ä‡: {accuracy:.2%}\")\nwyniki_workflow.append(f\"\\nPrzykÅ‚ad predykcji:\")\nwyniki_workflow.append(f\"Przewidywana klasa: {predicted_name}\")\nwyniki_workflow.append(f\"Rzeczywista klasa: {actual_name}\")\nwyniki_workflow.append(\"âœ… Poprawnie!\" if predicted_name == actual_name else \"âŒ BÅ‚Ä…d\")\n\nfor wynik in wyniki_workflow:\n    print(wynik)\n\n\n\n\n\n\n\nPokaÅ¼ kompletny workflow ML\n\n\n\n\n\n\n\nğŸ”¬ Kompletny workflow projektu ML\n========================================\n1ï¸âƒ£ Dane zaÅ‚adowane:\nKsztaÅ‚t: (150, 5)\nKlasy: ['setosa' 'versicolor' 'virginica']\nPierwsze 3 wiersze:\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n0                5.1               3.5                1.4               0.2       0\n1                4.9               3.0                1.4               0.2       0\n2                4.7               3.2                1.3               0.2       0\n\n2ï¸âƒ£ Dane przeskalowane (pierwsze 3 cechy pierwszej prÃ³bki):\nPrzed: [5.1 3.5 1.4]\nPo: [-0.90068117  1.01900435 -1.34022653]\n\n3ï¸âƒ£ PodziaÅ‚ danych:\nTrain: 120 prÃ³bek\nTest: 30 prÃ³bek\n\n4ï¸âƒ£ Model wytrenowany: RandomForestClassifier\n\n5ï¸âƒ£ Wyniki:\nDokÅ‚adnoÅ›Ä‡: 100.00%\n\nPrzykÅ‚ad predykcji:\nPrzewidywana klasa: versicolor\nRzeczywista klasa: versicolor\nâœ… Poprawnie!"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#najwaÅ¼niejsze-biblioteki-python-dla-ml",
    "href": "cheatsheets/01-intro-ml.html#najwaÅ¼niejsze-biblioteki-python-dla-ml",
    "title": "Wprowadzenie do Machine Learning â€” fundament",
    "section": "ğŸ’¡ NajwaÅ¼niejsze biblioteki Python dla ML",
    "text": "ğŸ’¡ NajwaÅ¼niejsze biblioteki Python dla ML\n# Podstawowe przetwarzanie danych\nimport pandas as pd      # DataFrames, CSV, analiza\nimport numpy as np       # obliczenia numeryczne\n\n# Machine Learning\nfrom sklearn import *    # algorytmy ML, preprocessing, metryki\nimport xgboost as xgb   # zaawansowane drzewa decyzyjne\n\n# Wizualizacja\nimport matplotlib.pyplot as plt  # wykresy\nimport seaborn as sns           # piÄ™kne wykresy statystyczne\n\n# Deep Learning\nimport tensorflow as tf  # sieci neuronowe (Google)\nimport torch            # sieci neuronowe (Facebook)\n\n\n\n\n\n\n\nğŸ¯ Pro tips dla poczÄ…tkujÄ…cych\n\n\n\n\nZacznij od prostych algorytmÃ³w - Linear Regression, Decision Trees\n80% czasu to przygotowanie danych - czyszczenie, eksploracja, feature engineering\nZawsze sprawdÅº czy model nie jest overfitted - uÅ¼yj validation set\nRozumiej swoje dane przed wyborem algorytmu\nPraktyka &gt; teoria - rÃ³b duÅ¼o projektÃ³w na rÃ³Å¼nych danych!\n\n\n\nNastÄ™pna Å›ciÄ…gawka: Linear Regression w praktyce ğŸš€"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html",
    "href": "cheatsheets/03-decision-trees.html",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "",
    "text": "Decision Trees to jeden z najbardziej intuicyjnych algorytmÃ³w ML, ktÃ³ry podejmuje decyzje jak czÅ‚owiek - zadajÄ…c szereg pytaÅ„ typu â€œtak/nieâ€ i na podstawie odpowiedzi klasyfikuje lub przewiduje wartoÅ›ci.\n\n\n\n\n\n\nğŸ’¡ Intuicja\n\n\n\nWyobraÅº sobie lekarza, ktÃ³ry diagnozuje chorobÄ™: â€œCzy ma gorÄ…czkÄ™? TAK â†’ Czy boli gardÅ‚o? TAK â†’ Czy ma katar? NIE â†’ Prawdopodobnie anginaâ€. Decision Tree dziaÅ‚a dokÅ‚adnie tak samo!"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#czym-sÄ…-decision-trees",
    "href": "cheatsheets/03-decision-trees.html#czym-sÄ…-decision-trees",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "",
    "text": "Decision Trees to jeden z najbardziej intuicyjnych algorytmÃ³w ML, ktÃ³ry podejmuje decyzje jak czÅ‚owiek - zadajÄ…c szereg pytaÅ„ typu â€œtak/nieâ€ i na podstawie odpowiedzi klasyfikuje lub przewiduje wartoÅ›ci.\n\n\n\n\n\n\nğŸ’¡ Intuicja\n\n\n\nWyobraÅº sobie lekarza, ktÃ³ry diagnozuje chorobÄ™: â€œCzy ma gorÄ…czkÄ™? TAK â†’ Czy boli gardÅ‚o? TAK â†’ Czy ma katar? NIE â†’ Prawdopodobnie anginaâ€. Decision Tree dziaÅ‚a dokÅ‚adnie tak samo!"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#praktyczny-przykÅ‚ad-klasyfikacja-klientÃ³w-banku",
    "href": "cheatsheets/03-decision-trees.html#praktyczny-przykÅ‚ad-klasyfikacja-klientÃ³w-banku",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "ğŸ¯ Praktyczny przykÅ‚ad: klasyfikacja klientÃ³w banku",
    "text": "ğŸ¯ Praktyczny przykÅ‚ad: klasyfikacja klientÃ³w banku\nCzy klient weÅºmie kredyt na podstawie jego profilu?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Tworzenie realistycznych danych klientÃ³w banku\nnp.random.seed(42)\nn_clients = 2000\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_clients),\n    'dochod': np.random.lognormal(10, 0.5, n_clients),  # log-normal dla dochodÃ³w\n    'historia_kredytowa': np.random.choice(['dobra', 'Å›rednia', 'sÅ‚aba'], n_clients, p=[0.6, 0.3, 0.1]),\n    'zatrudnienie': np.random.choice(['staÅ‚e', 'tymczasowe', 'bezrobotny'], n_clients, p=[0.7, 0.25, 0.05]),\n    'oszczednosci': np.random.exponential(20000, n_clients),\n    'liczba_dzieci': np.random.randint(0, 4, n_clients)\n})\n\n# Realistyczna logika decyzyjna banku\ndef czy_kredyt(row):\n    score = 0\n    \n    # Wiek (30-50 lat = najlepsi klienci)\n    if 30 &lt;= row['wiek'] &lt;= 50:\n        score += 2\n    elif row['wiek'] &lt; 25 or row['wiek'] &gt; 60:\n        score -= 1\n    \n    # DochÃ³d\n    if row['dochod'] &gt; 50000:\n        score += 3\n    elif row['dochod'] &gt; 30000:\n        score += 1\n    else:\n        score -= 2\n    \n    # Historia kredytowa\n    if row['historia_kredytowa'] == 'dobra':\n        score += 2\n    elif row['historia_kredytowa'] == 'sÅ‚aba':\n        score -= 3\n    \n    # Zatrudnienie\n    if row['zatrudnienie'] == 'staÅ‚e':\n        score += 2\n    elif row['zatrudnienie'] == 'bezrobotny':\n        score -= 4\n    \n    # OszczÄ™dnoÅ›ci\n    if row['oszczednosci'] &gt; 50000:\n        score += 1\n    \n    # Dzieci (wiÄ™cej dzieci = wiÄ™ksze ryzyko)\n    score -= row['liczba_dzieci'] * 0.5\n    \n    # KoÅ„cowa decyzja z odrobinÄ… losowoÅ›ci\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 0.5)))\n    return probability &gt; 0.5\n\ndata['kredyt_przyznany'] = data.apply(czy_kredyt, axis=1)\n\nprint(\"Statystyki klientÃ³w:\")\nprint(data.describe())\nprint(f\"\\nOdsetek przyznanych kredytÃ³w: {data['kredyt_przyznany'].mean():.1%}\")\n\n\n\n\n\n\n\nPokaÅ¼ statystyki i odsetki przyznanych kredytÃ³w\n\n\n\n\n\n\n\nStatystyki klientÃ³w:\n              wiek         dochod   oszczednosci  liczba_dzieci\ncount  2000.000000    2000.000000    2000.000000    2000.000000\nmean     43.805500   25726.361013   19950.270464       1.523000\nstd      14.929203   14090.406882   20299.940268       1.130535\nmin      18.000000    4047.221838      27.128881       0.000000\n25%      31.000000   15922.761710    5923.833818       1.000000\n50%      44.000000   22782.391426   13844.909016       2.000000\n75%      56.000000   31922.003675   27566.091135       3.000000\nmax      69.000000  112447.929023  165194.684774       3.000000\n\nOdsetek przyznanych kredytÃ³w: 63.8%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#budowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/03-decision-trees.html#budowanie-modelu-krok-po-kroku",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "ğŸ”§ Budowanie modelu krok po kroku",
    "text": "ğŸ”§ Budowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Encoding kategorycznych zmiennych\nfrom sklearn.preprocessing import LabelEncoder\n\ndata_encoded = data.copy()\nle_historia = LabelEncoder()\nle_zatrudnienie = LabelEncoder()\n\ndata_encoded['historia_kredytowa_num'] = le_historia.fit_transform(data['historia_kredytowa'])\ndata_encoded['zatrudnienie_num'] = le_zatrudnienie.fit_transform(data['zatrudnienie'])\n\n# Features do modelu\nfeature_columns = ['wiek', 'dochod', 'historia_kredytowa_num', 'zatrudnienie_num', 'oszczednosci', 'liczba_dzieci']\nX = data_encoded[feature_columns]\ny = data_encoded['kredyt_przyznany']\n\n# PodziaÅ‚ train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Dane treningowe: {len(X_train)} klientÃ³w\")\nprint(f\"Dane testowe: {len(X_test)} klientÃ³w\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki dane treningowe i testowe\n\n\n\n\n\n\n\nDane treningowe: 1600 klientÃ³w\nDane testowe: 400 klientÃ³w\n\n\n\n\n\n\n\n2) Trenowanie Decision Tree\n\n# Tworzenie modelu z ograniczeniami (Å¼eby nie byÅ‚ za gÅ‚Ä™boki)\ndt_model = DecisionTreeClassifier(\n    max_depth=5,        # maksymalna gÅ‚Ä™bokoÅ›Ä‡ drzewa\n    min_samples_split=50,  # min. prÃ³bek do podziaÅ‚u wÄ™zÅ‚a\n    min_samples_leaf=20,   # min. prÃ³bek w liÅ›ciu\n    random_state=42\n)\n\n# Trenowanie\ndt_model.fit(X_train, y_train)\n\nprint(\"Model wytrenowany!\")\nprint(f\"GÅ‚Ä™bokoÅ›Ä‡ drzewa: {dt_model.tree_.max_depth}\")\nprint(f\"Liczba liÅ›ci: {dt_model.tree_.n_leaves}\")\n\n\n\n\n\n\n\nPokaÅ¼ parametry wytrenowanego modelu\n\n\n\n\n\n\n\nModel wytrenowany!\nGÅ‚Ä™bokoÅ›Ä‡ drzewa: 5\nLiczba liÅ›ci: 20\n\n\n\n\n\n\n\n3) Ewaluacja modelu\n\n# Predykcje\ny_pred = dt_model.predict(X_test)\ny_pred_proba = dt_model.predict_proba(X_test)[:, 1]\n\n# Metryki\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nDokÅ‚adnoÅ›Ä‡ modelu: {accuracy:.1%}\")\n\n# SzczegÃ³Å‚owy raport\nprint(\"\\nRaport klasyfikacji:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:    Nie    Tak\")\nprint(f\"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki\n\n\n\n\n\n\n\n\nConfusion Matrix:\nPrzewidywane:    Nie    Tak\nRzeczywiste Nie: 122    34\nRzeczywiste Tak:  27   217"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#wizualizacja-drzewa-decyzyjnego",
    "href": "cheatsheets/03-decision-trees.html#wizualizacja-drzewa-decyzyjnego",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "ğŸ¨ Wizualizacja drzewa decyzyjnego",
    "text": "ğŸ¨ Wizualizacja drzewa decyzyjnego\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Tworzenie przykÅ‚adowych danych (kompletny przykÅ‚ad)\nnp.random.seed(42)\nn_clients = 2000\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_clients),\n    'dochod': np.random.lognormal(10, 0.5, n_clients),\n    'historia_kredytowa': np.random.choice(['dobra', 'Å›rednia', 'sÅ‚aba'], n_clients, p=[0.6, 0.3, 0.1]),\n    'zatrudnienie': np.random.choice(['staÅ‚e', 'tymczasowe', 'bezrobotny'], n_clients, p=[0.7, 0.25, 0.05]),\n    'oszczednosci': np.random.exponential(20000, n_clients),\n    'liczba_dzieci': np.random.randint(0, 4, n_clients)\n})\n\n# Prosta logika przyznawania kredytu\ndef czy_kredyt(row):\n    score = 0\n    if 30 &lt;= row['wiek'] &lt;= 50:\n        score += 2\n    if row['dochod'] &gt; 50000:\n        score += 3\n    if row['historia_kredytowa'] == 'dobra':\n        score += 2\n    if row['zatrudnienie'] == 'staÅ‚e':\n        score += 2\n    if row['oszczednosci'] &gt; 50000:\n        score += 1\n    score -= row['liczba_dzieci'] * 0.5\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 0.5)))\n    return probability &gt; 0.5\n\ndata['kredyt_przyznany'] = data.apply(czy_kredyt, axis=1)\n\n# Encoding i model\ndata_encoded = data.copy()\nle_historia = LabelEncoder()\nle_zatrudnienie = LabelEncoder()\ndata_encoded['historia_kredytowa_num'] = le_historia.fit_transform(data['historia_kredytowa'])\ndata_encoded['zatrudnienie_num'] = le_zatrudnienie.fit_transform(data['zatrudnienie'])\n\nfeature_columns = ['wiek', 'dochod', 'historia_kredytowa_num', 'zatrudnienie_num', 'oszczednosci', 'liczba_dzieci']\nX = data_encoded[feature_columns]\ny = data_encoded['kredyt_przyznany']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie drzewa\ndt_model = DecisionTreeClassifier(max_depth=5, min_samples_split=50, min_samples_leaf=20, random_state=42)\ndt_model.fit(X_train, y_train)\n\n# Ewaluacja\ny_pred = dt_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\n# WaÅ¼noÅ›Ä‡ features\nfeature_names = ['wiek', 'dochÃ³d', 'historia_kred.', 'zatrudnienie', 'oszczÄ™dnoÅ›ci', 'liczba_dzieci']\nimportance = dt_model.feature_importances_\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importance\n}).sort_values('importance', ascending=False)\n\n# Przygotuj wykres drzewa\nplt.figure(figsize=(20, 10))\ntree.plot_tree(dt_model, \n               feature_names=feature_names,\n               class_names=['Nie', 'Tak'],\n               filled=True,\n               rounded=True,\n               fontsize=10)\nplt.title(\"Drzewo decyzyjne - Przyznanie kredytu\")\nplt.close()\n\nprint(f\"Wyniki modelu Decision Tree:\")\nprint(f\"GÅ‚Ä™bokoÅ›Ä‡ drzewa: {dt_model.tree_.max_depth}\")\nprint(f\"Liczba liÅ›ci: {dt_model.tree_.n_leaves}\")\nprint(f\"DokÅ‚adnoÅ›Ä‡ modelu: {accuracy:.1%}\")\n\nprint(f\"\\nStatystyki danych:\")\nprint(f\"Dane treningowe: {len(X_train)} klientÃ³w\")\nprint(f\"Dane testowe: {len(X_test)} klientÃ³w\")\nprint(f\"Odsetek przyznanych kredytÃ³w: {data['kredyt_przyznany'].mean():.1%}\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:    Nie    Tak\")\nprint(f\"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}\")\n\nprint(\"\\nWaÅ¼noÅ›Ä‡ cech:\")\nfor _, row in feature_importance.iterrows():\n    print(f\"{row['feature']}: {row['importance']:.3f}\")\n\n# OdtwÃ³rz wykres drzewa\nplt.figure(figsize=(20, 10))\ntree.plot_tree(dt_model, \n               feature_names=feature_names,\n               class_names=['Nie', 'Tak'],\n               filled=True,\n               rounded=True,\n               fontsize=10)\nplt.title(\"Drzewo decyzyjne - Przyznanie kredytu\")\nplt.show()\n\n\n\n\n\n\n\nPokaÅ¼ wyniki i drzewo decyzyjne\n\n\n\n\n\n\n\nWyniki modelu Decision Tree:\nGÅ‚Ä™bokoÅ›Ä‡ drzewa: 5\nLiczba liÅ›ci: 13\nDokÅ‚adnoÅ›Ä‡ modelu: 95.0%\n\nStatystyki danych:\nDane treningowe: 1600 klientÃ³w\nDane testowe: 400 klientÃ³w\nOdsetek przyznanych kredytÃ³w: 93.2%\n\nConfusion Matrix:\nPrzewidywane:    Nie    Tak\nRzeczywiste Nie:  19    11\nRzeczywiste Tak:   9   361\n\nWaÅ¼noÅ›Ä‡ cech:\nzatrudnienie: 0.441\nwiek: 0.317\nhistoria_kred.: 0.178\nliczba_dzieci: 0.055\noszczÄ™dnoÅ›ci: 0.008\ndochÃ³d: 0.001\n\n\n\n\n\nWizualizacja drzewa decyzyjnego dla kredytÃ³w"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#interpretacja-decyzji-dla-konkretnego-klienta",
    "href": "cheatsheets/03-decision-trees.html#interpretacja-decyzji-dla-konkretnego-klienta",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "ğŸ” Interpretacja decyzji dla konkretnego klienta",
    "text": "ğŸ” Interpretacja decyzji dla konkretnego klienta\n\n# Funkcja do interpretacji Å›cieÅ¼ki decyzyjnej\ndef explain_decision(model, X_sample, feature_names):\n    # Pobierz Å›cieÅ¼kÄ™ w drzewie\n    leaf_id = model.decision_path(X_sample.reshape(1, -1)).toarray()[0]\n    feature = model.tree_.feature\n    threshold = model.tree_.threshold\n    \n    print(\"ÅšcieÅ¼ka decyzyjna:\")\n    for node_id in range(len(leaf_id)):\n        if leaf_id[node_id] == 1:  # jeÅ›li wÄ™zeÅ‚ jest na Å›cieÅ¼ce\n            if feature[node_id] != -2:  # jeÅ›li nie jest liÅ›ciem\n                feature_name = feature_names[feature[node_id]]\n                threshold_val = threshold[node_id]\n                feature_val = X_sample[feature[node_id]]\n                \n                if feature_val &lt;= threshold_val:\n                    condition = \"&lt;=\"\n                else:\n                    condition = \"&gt;\"\n                    \n                print(f\"  {feature_name} ({feature_val:.0f}) {condition} {threshold_val:.0f}\")\n\n# PrzykÅ‚ad dla konkretnego klienta\nsample_client = X_test.iloc[0]\nprediction = dt_model.predict([sample_client])[0]\nprobability = dt_model.predict_proba([sample_client])[0]\n\nprint(f\"Klient testowy:\")\nprint(f\"Wiek: {sample_client['wiek']}\")\nprint(f\"DochÃ³d: {sample_client['dochod']:.0f}\")\nprint(f\"OszczÄ™dnoÅ›ci: {sample_client['oszczednosci']:.0f}\")\nprint(f\"\\nDecyzja: {'KREDYT PRZYZNANY' if prediction else 'KREDYT ODRZUCONY'}\")\nprint(f\"PrawdopodobieÅ„stwo: {probability[1]:.1%}\")\n\nexplain_decision(dt_model, sample_client.values, feature_names)\n\n\n\n\n\n\n\nPokaÅ¼ dane klienta testowego\n\n\n\n\n\n\n\nKlient testowy:\nWiek: 56.0\nDochÃ³d: 7927\nOszczÄ™dnoÅ›ci: 32270\n\nDecyzja: KREDYT ODRZUCONY\nPrawdopodobieÅ„stwo: 3.0%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#rÃ³Å¼ne-zastosowania-decision-trees",
    "href": "cheatsheets/03-decision-trees.html#rÃ³Å¼ne-zastosowania-decision-trees",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "ğŸ¯ RÃ³Å¼ne zastosowania Decision Trees",
    "text": "ğŸ¯ RÃ³Å¼ne zastosowania Decision Trees\n\n1) Medyczna diagnoza\n\n# PrzykÅ‚ad klasyfikacji ryzyka chorÃ³b serca\nmedical_features = ['wiek', 'cholesterol', 'ciÅ›nienie', 'BMI', 'pali_papierosy']\n# Target: 'ryzyko_chorÃ³b_serca' (wysokie/niskie)\n\nmedical_tree = DecisionTreeClassifier(max_depth=4)\n# Model automatycznie znajdzie progi: \"JeÅ›li cholesterol &gt; 240 I BMI &gt; 30 ORAZ wiek &gt; 50...\"\n\n\n\n2) Marketing - segmentacja klientÃ³w\n\n# Przewidywanie czy klient kupi produkt premium\nmarketing_features = ['dochÃ³d_roczny', 'wiek', 'wyksztaÅ‚cenie', 'poprzednie_zakupy']\n# Target: 'kupi_premium' (tak/nie)\n\nmarketing_tree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=100)\n# Wynik: jasne reguÅ‚y marketingowe do targetowania reklam\n\n\n\n3) HR - decyzje o zatrudnieniu\n\n# Przewidywanie sukcesu kandydata w rekrutacji\nhr_features = ['doÅ›wiadczenie_lat', 'wyksztaÅ‚cenie', 'wynik_testÃ³w', 'referencje']\n# Target: 'zatrudniony' (tak/nie)\n\nhr_tree = DecisionTreeClassifier(max_depth=5)\n# Wynik: automatyczne zasady rekrutacyjne"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#tuning-parametrÃ³w",
    "href": "cheatsheets/03-decision-trees.html#tuning-parametrÃ³w",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "âš™ï¸ Tuning parametrÃ³w",
    "text": "âš™ï¸ Tuning parametrÃ³w\n\nfrom sklearn.model_selection import GridSearchCV\n\n# NajwaÅ¼niejsze parametry do tuningu\nparam_grid = {\n    'max_depth': [3, 5, 7, 10, None],\n    'min_samples_split': [20, 50, 100],\n    'min_samples_leaf': [10, 20, 50],\n    'criterion': ['gini', 'entropy']\n}\n\n# Grid search z cross-validation\ngrid_search = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(\"Najlepsze parametry:\")\nprint(grid_search.best_params_)\nprint(f\"Najlepsza dokÅ‚adnoÅ›Ä‡ CV: {grid_search.best_score_:.1%}\")\n\n# Model z najlepszymi parametrami\nbest_model = grid_search.best_estimator_\nbest_accuracy = accuracy_score(y_test, best_model.predict(X_test))\nprint(f\"DokÅ‚adnoÅ›Ä‡ na test set: {best_accuracy:.1%}\")\n\n\n\n\n\n\n\nPokaÅ¼ najlepsze parametry\n\n\n\n\n\n\n\nNajlepsze parametry:\n{'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 20}\nNajlepsza dokÅ‚adnoÅ›Ä‡ CV: 96.5%\nDokÅ‚adnoÅ›Ä‡ na test set: 95.0%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#problemy-i-rozwiÄ…zania",
    "href": "cheatsheets/03-decision-trees.html#problemy-i-rozwiÄ…zania",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "âš ï¸ Problemy i rozwiÄ…zania",
    "text": "âš ï¸ Problemy i rozwiÄ…zania\n\n1) Overfitting - drzewo za gÅ‚Ä™bokie\n\n# Problem: drzewo \"pamiÄ™ta\" dane treningowe\noverfitted_tree = DecisionTreeClassifier()  # bez ograniczeÅ„!\noverfitted_tree.fit(X_train, y_train)\n\ntrain_acc = accuracy_score(y_train, overfitted_tree.predict(X_train))\ntest_acc = accuracy_score(y_test, overfitted_tree.predict(X_test))\n\nprint(f\"Overfitted model:\")\nprint(f\"Train accuracy: {train_acc:.1%}\")\nprint(f\"Test accuracy: {test_acc:.1%}\")\nprint(f\"RÃ³Å¼nica: {train_acc - test_acc:.1%} - to overfitting!\")\n\n# RozwiÄ…zanie: ograniczenia\npruned_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=20)\npruned_tree.fit(X_train, y_train)\n\ntrain_acc_pruned = accuracy_score(y_train, pruned_tree.predict(X_train))\ntest_acc_pruned = accuracy_score(y_test, pruned_tree.predict(X_test))\n\nprint(f\"\\nPruned model:\")\nprint(f\"Train accuracy: {train_acc_pruned:.1%}\")\nprint(f\"Test accuracy: {test_acc_pruned:.1%}\")\nprint(f\"RÃ³Å¼nica: {train_acc_pruned - test_acc_pruned:.1%} - znacznie lepiej!\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki\n\n\n\n\n\n\n\nOverfitted model:\nTrain accuracy: 100.0%\nTest accuracy: 94.8%\nRÃ³Å¼nica: 5.2% - to overfitting!\n\nPruned model:\nTrain accuracy: 96.7%\nTest accuracy: 95.0%\nRÃ³Å¼nica: 1.7% - znacznie lepiej!\n\n\n\n\n\n\n\n2) Brak stabilnoÅ›ci - maÅ‚e zmiany = rÃ³Å¼ne drzewa\n\n# Problem demonstracji\nresults = []\nfor i in range(10):\n    # RÃ³Å¼ne random_state = rÃ³Å¼ne drzewa\n    tree_test = DecisionTreeClassifier(random_state=i, max_depth=5)\n    tree_test.fit(X_train, y_train)\n    acc = accuracy_score(y_test, tree_test.predict(X_test))\n    results.append(acc)\n\nprint(f\"DokÅ‚adnoÅ›Ä‡ rÃ³Å¼nych drzew: {min(results):.1%} - {max(results):.1%}\")\nprint(f\"Rozrzut: {max(results) - min(results):.1%}\")\nprint(\"RozwiÄ…zanie: Random Forest (nastÄ™pna Å›ciÄ…gawka!)\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki\n\n\n\n\n\n\n\nDokÅ‚adnoÅ›Ä‡ rÃ³Å¼nych drzew: 96.2% - 96.2%\nRozrzut: 0.0%\nRozwiÄ…zanie: Random Forest (nastÄ™pna Å›ciÄ…gawka!)"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#real-world-przypadki-uÅ¼ycia",
    "href": "cheatsheets/03-decision-trees.html#real-world-przypadki-uÅ¼ycia",
    "title": "Decision Trees â€” proste drzewa decyzyjne",
    "section": "ğŸŒ Real-world przypadki uÅ¼ycia",
    "text": "ğŸŒ Real-world przypadki uÅ¼ycia\n\nBankowoÅ›Ä‡: Ocena ryzyka kredytowego, wykrywanie fraudÃ³w\nMedycyna: Systemy wspomagania diagnostyki, triage pacjentÃ³w\n\nE-commerce: Rekomendacje produktÃ³w, ustalanie cen dynamicznych\nHR: Automatyzacja procesÃ³w rekrutacyjnych\nMarketing: Segmentacja klientÃ³w, personalizacja oferowania\n\n\n\n\n\n\n\nğŸ’¡ Kiedy uÅ¼ywaÄ‡ Decision Trees?\n\n\n\nâœ… UÅ»YJ GDY:\n\nPotrzebujesz interpretowalnego modelu\nDane majÄ… kategoryczne zmienne\n\nChcesz zrozumieÄ‡ â€œdlaczegoâ€ model podjÄ…Å‚ decyzjÄ™\nMasz nieliniowe zaleÅ¼noÅ›ci w danych\nBrakÃ³w w danych nie trzeba imputeâ€™owaÄ‡\n\nâŒ NIE UÅ»YWAJ GDY:\n\nPotrzebujesz najwyÅ¼szej dokÅ‚adnoÅ›ci (uÅ¼yj Random Forest/XGBoost)\nMasz bardzo gÅ‚Ä™bokie wzorce w danych (uÅ¼yj Neural Networks)\nDane sÄ… bardzo haÅ‚aÅ›liwe\nPrzewidujesz wartoÅ›ci ciÄ…gÅ‚e (lepiej Linear Regression)\n\n\n\nNastÄ™pna Å›ciÄ…gawka: Random Forest - zespÃ³Å‚ drzew (wkrÃ³tce)! ğŸŒ²ğŸŒ²ğŸŒ²"
  },
  {
    "objectID": "cheatsheets/06-svm.html",
    "href": "cheatsheets/06-svm.html",
    "title": "Support Vector Machines â€” optymalne granice decyzyjne",
    "section": "",
    "text": "SVM to algorytm, ktÃ³ry znajduje optymalnÄ… granicÄ™ decyzyjnÄ… miÄ™dzy klasami. Zamiast szukaÄ‡ â€œjakiejkolwiekâ€ linii podziaÅ‚u, SVM znajduje tÄ…, ktÃ³ra maksymalizuje margines - odlegÅ‚oÅ›Ä‡ do najbliÅ¼szych punktÃ³w z kaÅ¼dej klasy.\n\n\n\n\n\n\nğŸ’¡ Intuicja\n\n\n\nWyobraÅº sobie sÄ™dziego, ktÃ³ry musi wyznaczyÄ‡ granicÄ™ miÄ™dzy dwoma druÅ¼ynami. SVM nie tylko znajdzie granicÄ™, ale postawi jÄ… tak, Å¼eby byÅ‚a jak najdalej od najbliÅ¼szych zawodnikÃ³w z obu stron - dla maksymalnego bezpieczeÅ„stwa!"
  },
  {
    "objectID": "cheatsheets/06-svm.html#czym-jest-support-vector-machine-svm",
    "href": "cheatsheets/06-svm.html#czym-jest-support-vector-machine-svm",
    "title": "Support Vector Machines â€” optymalne granice decyzyjne",
    "section": "",
    "text": "SVM to algorytm, ktÃ³ry znajduje optymalnÄ… granicÄ™ decyzyjnÄ… miÄ™dzy klasami. Zamiast szukaÄ‡ â€œjakiejkolwiekâ€ linii podziaÅ‚u, SVM znajduje tÄ…, ktÃ³ra maksymalizuje margines - odlegÅ‚oÅ›Ä‡ do najbliÅ¼szych punktÃ³w z kaÅ¼dej klasy.\n\n\n\n\n\n\nğŸ’¡ Intuicja\n\n\n\nWyobraÅº sobie sÄ™dziego, ktÃ³ry musi wyznaczyÄ‡ granicÄ™ miÄ™dzy dwoma druÅ¼ynami. SVM nie tylko znajdzie granicÄ™, ale postawi jÄ… tak, Å¼eby byÅ‚a jak najdalej od najbliÅ¼szych zawodnikÃ³w z obu stron - dla maksymalnego bezpieczeÅ„stwa!"
  },
  {
    "objectID": "cheatsheets/06-svm.html#praktyczny-przykÅ‚ad-klasyfikacja-emaili-spamham",
    "href": "cheatsheets/06-svm.html#praktyczny-przykÅ‚ad-klasyfikacja-emaili-spamham",
    "title": "Support Vector Machines â€” optymalne granice decyzyjne",
    "section": "ğŸ¯ Praktyczny przykÅ‚ad: klasyfikacja emaili spam/ham",
    "text": "ğŸ¯ Praktyczny przykÅ‚ad: klasyfikacja emaili spam/ham\nCzy email to spam czy prawdziwa wiadomoÅ›Ä‡?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\n\n# Tworzenie realistycznych danych emaili\nnp.random.seed(42)\nn_emails = 2000\n\n# Generuj rÃ³Å¼ne typy emaili\nemail_types = np.random.choice(['spam', 'ham'], n_emails, p=[0.3, 0.7])\n\ndata = pd.DataFrame({\n    'dlugosc_tematu': np.random.randint(5, 100, n_emails),\n    'liczba_wielkich_liter': np.random.randint(0, 50, n_emails),\n    'liczba_wykrzyknikow': np.random.randint(0, 10, n_emails),\n    'liczba_linkow': np.random.randint(0, 20, n_emails),\n    'slowa_promocyjne': np.random.randint(0, 15, n_emails),\n    'dlugosc_wiadomosci': np.random.randint(50, 5000, n_emails),\n    'godzina_wyslania': np.random.randint(0, 24, n_emails),\n    'typ': email_types\n})\n\n# Realistyczne wzorce dla spam vs ham\nfor i, typ in enumerate(data['typ']):\n    if typ == 'spam':\n        # Spam charakterystyki\n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(20, 80)  # dÅ‚uÅ¼sze tematy\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(10, 50)  # DUÅ»O WIELKIMI\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(3, 10)  # wiÄ™cej !!!\n        data.loc[i, 'liczba_linkow'] = np.random.randint(5, 20)  # duÅ¼o linkÃ³w\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(5, 15)  # PROMOCJA, GRATIS\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(200, 1000)  # Å›rednie dÅ‚ugoÅ›ci\n        data.loc[i, 'godzina_wyslania'] = np.random.choice([2, 3, 22, 23, 1])  # dziwne godziny\n    else:\n        # Ham (prawdziwe) charakterystyki  \n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(5, 40)  # krÃ³tsze tematy\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(0, 8)  # normalne uÅ¼ywanie\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(0, 3)  # rzadko !\n        data.loc[i, 'liczba_linkow'] = np.random.randint(0, 3)  # maÅ‚o linkÃ³w\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(0, 2)  # prawie brak\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(50, 3000)  # rÃ³Å¼ne dÅ‚ugoÅ›ci\n        data.loc[i, 'godzina_wyslania'] = np.random.randint(8, 22)  # normalne godziny\n\n# Konwertuj target na binary\ndata['is_spam'] = (data['typ'] == 'spam').astype(int)\n\nprint(\"Statystyki emaili:\")\nprint(data.describe())\nprint(f\"\\nRozkÅ‚ad spam/ham:\")\nprint(data['typ'].value_counts())\nprint(f\"Procent spam: {data['is_spam'].mean():.1%}\")\n\n\n\n\n\n\n\nPokaÅ¼ statystyki emaili\n\n\n\n\n\n\n\nStatystyki emaili:\n       dlugosc_tematu  liczba_wielkich_liter  liczba_wykrzyknikow  \\\ncount     2000.000000            2000.000000           2000.00000   \nmean        30.482500              11.392500              2.60800   \nstd         18.078609              13.675451              2.69369   \nmin          5.000000               0.000000              0.00000   \n25%         17.000000               2.000000              1.00000   \n50%         27.000000               5.000000              2.00000   \n75%         38.000000              17.000000              4.00000   \nmax         79.000000              49.000000              9.00000   \n\n       liczba_linkow  slowa_promocyjne  dlugosc_wiadomosci  godzina_wyslania  \\\ncount    2000.000000       2000.000000         2000.000000       2000.000000   \nmean        4.360000          3.302500         1240.088000         13.031000   \nstd         5.642279          4.524183          834.469147          6.857445   \nmin         0.000000          0.000000           56.000000          1.000000   \n25%         1.000000          0.000000          539.000000          9.000000   \n50%         2.000000          1.000000          958.000000         14.000000   \n75%         7.000000          6.000000         1905.500000         19.000000   \nmax        19.000000         14.000000         2998.000000         23.000000   \n\n           is_spam  \ncount  2000.000000  \nmean      0.307500  \nstd       0.461574  \nmin       0.000000  \n25%       0.000000  \n50%       0.000000  \n75%       1.000000  \nmax       1.000000  \n\nRozkÅ‚ad spam/ham:\ntyp\nham     1385\nspam     615\nName: count, dtype: int64\nProcent spam: 30.8%"
  },
  {
    "objectID": "cheatsheets/06-svm.html#budowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/06-svm.html#budowanie-modelu-krok-po-kroku",
    "title": "Support Vector Machines â€” optymalne granice decyzyjne",
    "section": "ğŸ”§ Budowanie modelu krok po kroku",
    "text": "ğŸ”§ Budowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Features do modelu\nfeatures = ['dlugosc_tematu', 'liczba_wielkich_liter', 'liczba_wykrzyknikow', \n           'liczba_linkow', 'slowa_promocyjne', 'dlugosc_wiadomosci', 'godzina_wyslania']\nX = data[features]\ny = data['is_spam']\n\n# PodziaÅ‚ train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standaryzacja - KLUCZOWA dla SVM!\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"Dane treningowe: {len(X_train)} emaili\")\nprint(f\"Dane testowe: {len(X_test)} emaili\")\nprint(f\"Spam w train: {y_train.mean():.1%}\")\nprint(f\"Spam w test: {y_test.mean():.1%}\")\n\nprint(\"\\nPrzykÅ‚ad standaryzacji:\")\nprint(\"Przed:\", X_train.iloc[0].values)\nprint(\"Po:\", X_train_scaled[0])\n\n\n\n\n\n\n\nPokaÅ¼ podziaÅ‚ danych\n\n\n\n\n\n\n\nDane treningowe: 1600 emaili\nDane testowe: 400 emaili\nSpam w train: 30.9%\nSpam w test: 30.0%\n\nPrzykÅ‚ad standaryzacji:\nPrzed: [   6    1    0    2    0 2057   21]\nPo: [-1.36651173 -0.75972497 -0.9773396  -0.41945283 -0.73519846  1.00688364\n  1.1681456 ]\n\n\n\n\n\n\n\n2) Trenowanie rÃ³Å¼nych kerneli SVM\n\n# PorÃ³wnanie rÃ³Å¼nych kerneli\nkernels = ['linear', 'rbf', 'poly']\nsvm_results = {}\n\nfor kernel in kernels:\n    print(f\"\\nTrenowanie SVM z kernelem: {kernel}\")\n    \n    if kernel == 'poly':\n        svm = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm = SVC(kernel=kernel, random_state=42)\n    \n    # Trenowanie\n    svm.fit(X_train_scaled, y_train)\n    \n    # Predykcje\n    y_pred = svm.predict(X_test_scaled)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    svm_results[kernel] = {\n        'model': svm,\n        'accuracy': accuracy,\n        'predictions': y_pred\n    }\n    \n    print(f\"DokÅ‚adnoÅ›Ä‡: {accuracy:.1%}\")\n    print(f\"Liczba support vectors: {svm.n_support_}\")\n\n# ZnajdÅº najlepszy kernel\nbest_kernel = max(svm_results.keys(), key=lambda k: svm_results[k]['accuracy'])\nbest_model = svm_results[best_kernel]['model']\n\nprint(f\"\\nğŸ† Najlepszy kernel: {best_kernel}\")\nprint(f\"Najlepsza dokÅ‚adnoÅ›Ä‡: {svm_results[best_kernel]['accuracy']:.1%}\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki rÃ³Å¼nych kerneli\n\n\n\n\n\n\n\n\nKernel linear:\nDokÅ‚adnoÅ›Ä‡: 100.0%\nLiczba support vectors: [2 3]\n\nKernel rbf:\nDokÅ‚adnoÅ›Ä‡: 100.0%\nLiczba support vectors: [11 26]\n\nKernel poly:\nDokÅ‚adnoÅ›Ä‡: 100.0%\nLiczba support vectors: [16 16]\n\nğŸ† Najlepszy kernel: linear\nNajlepsza dokÅ‚adnoÅ›Ä‡: 100.0%\n\n\n\n\n\n\n\n3) Ewaluacja najlepszego modelu\n\n# SzczegÃ³Å‚owa analiza najlepszego modelu\nbest_predictions = svm_results[best_kernel]['predictions']\n\nprint(f\"SZCZEGÃ“ÅOWA ANALIZA - SVM {best_kernel.upper()}\")\nprint(\"=\" * 50)\n\n# Classification report\nprint(\"\\nRaport klasyfikacji:\")\nprint(classification_report(y_test, best_predictions, target_names=['Ham', 'Spam']))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, best_predictions)\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:     Ham    Spam\")\nprint(f\"Rzeczywiste Ham:  {cm[0,0]:3d}    {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Spam: {cm[1,0]:3d}    {cm[1,1]:3d}\")\n\n# Analiza bÅ‚Ä™dÃ³w\nfalse_positives = np.where((y_test == 0) & (best_predictions == 1))[0]\nfalse_negatives = np.where((y_test == 1) & (best_predictions == 0))[0]\n\nprint(f\"\\nAnaliza bÅ‚Ä™dÃ³w:\")\nprint(f\"False Positives (Ham â†’ Spam): {len(false_positives)}\")\nprint(f\"False Negatives (Spam â†’ Ham): {len(false_negatives)}\")\n\nif len(false_positives) &gt; 0:\n    print(f\"\\nPrzykÅ‚ad bÅ‚Ä™dnie sklasyfikowanego Ham jako Spam:\")\n    fp_idx = false_positives[0]\n    print(f\"Email: {X_test.iloc[fp_idx].to_dict()}\")\n\n\n\n\n\n\n\nPokaÅ¼ szczegÃ³Å‚owÄ… analizÄ™\n\n\n\n\n\n\n\nSZCZEGÃ“ÅOWA ANALIZA - SVM LINEAR\n==================================================\n\nRaport klasyfikacji:\n              precision    recall  f1-score   support\n\n         Ham       1.00      1.00      1.00       280\n        Spam       1.00      1.00      1.00       120\n\n    accuracy                           1.00       400\n   macro avg       1.00      1.00      1.00       400\nweighted avg       1.00      1.00      1.00       400\n\n\nConfusion Matrix:\nPrzewidywane:     Ham    Spam\nRzeczywiste Ham:  280      0\nRzeczywiste Spam:   0    120\n\nAnaliza bÅ‚Ä™dÃ³w:\nFalse Positives (Ham â†’ Spam): 0\nFalse Negatives (Spam â†’ Ham): 0"
  },
  {
    "objectID": "cheatsheets/06-svm.html#wizualizacja-granic-decyzyjnych",
    "href": "cheatsheets/06-svm.html#wizualizacja-granic-decyzyjnych",
    "title": "Support Vector Machines â€” optymalne granice decyzyjne",
    "section": "ğŸ“Š Wizualizacja granic decyzyjnych",
    "text": "ğŸ“Š Wizualizacja granic decyzyjnych\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Przygotuj dane (powtarzamy dla kompletnoÅ›ci)\nnp.random.seed(42)\nn_emails = 2000\n\nemail_types = np.random.choice(['spam', 'ham'], n_emails, p=[0.3, 0.7])\n\ndata = pd.DataFrame({\n    'dlugosc_tematu': np.random.randint(5, 100, n_emails),\n    'liczba_wielkich_liter': np.random.randint(0, 50, n_emails),\n    'liczba_wykrzyknikow': np.random.randint(0, 10, n_emails),\n    'liczba_linkow': np.random.randint(0, 20, n_emails),\n    'slowa_promocyjne': np.random.randint(0, 15, n_emails),\n    'dlugosc_wiadomosci': np.random.randint(50, 5000, n_emails),\n    'godzina_wyslania': np.random.randint(0, 24, n_emails),\n    'typ': email_types\n})\n\n# Popraw charakterystyki\nfor i, typ in enumerate(data['typ']):\n    if typ == 'spam':\n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(20, 80)\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(10, 50)\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(3, 10)\n        data.loc[i, 'liczba_linkow'] = np.random.randint(5, 20)\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(5, 15)\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(200, 1000)\n        data.loc[i, 'godzina_wyslania'] = np.random.choice([2, 3, 22, 23, 1])\n    else:\n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(5, 40)\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(0, 8)\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(0, 3)\n        data.loc[i, 'liczba_linkow'] = np.random.randint(0, 3)\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(0, 2)\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(50, 3000)\n        data.loc[i, 'godzina_wyslania'] = np.random.randint(8, 22)\n\ndata['is_spam'] = (data['typ'] == 'spam').astype(int)\n\n# Wybierz tylko 2 features dla wizualizacji 2D\nfeatures_2d = ['liczba_wielkich_liter', 'liczba_wykrzyknikow']\nX_2d = data[features_2d]\ny = data['is_spam']\n\nX_train_2d, X_test_2d, y_train, y_test = train_test_split(X_2d, y, test_size=0.2, random_state=42)\n\nscaler_2d = StandardScaler()\nX_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)\nX_test_2d_scaled = scaler_2d.transform(X_test_2d)\n\n# Trenuj modele z rÃ³Å¼nymi kernelami (na 2D)\nkernels = ['linear', 'rbf', 'poly']\nplt.figure(figsize=(15, 5))\n\nfor i, kernel in enumerate(kernels):\n    plt.subplot(1, 3, i+1)\n    \n    # Trenuj model\n    if kernel == 'poly':\n        svm = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm = SVC(kernel=kernel, random_state=42)\n    \n    svm.fit(X_train_2d_scaled, y_train)\n    \n    # StwÃ³rz mesh do wizualizacji granic\n    h = 0.02\n    x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1\n    y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    # Predykcje na mesh\n    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    # Narysuj granice i punkty\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    scatter = plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], \n                         c=y_train, cmap=plt.cm.RdYlBu, edgecolors='black')\n    \n    # PodÅ›wietl support vectors\n    if hasattr(svm, 'support_'):\n        plt.scatter(X_train_2d_scaled[svm.support_, 0], \n                   X_train_2d_scaled[svm.support_, 1],\n                   s=100, facecolors='none', edgecolors='black', linewidth=2)\n    \n    plt.xlabel('Liczba wielkich liter (scaled)')\n    plt.ylabel('Liczba wykrzyknikÃ³w (scaled)')\n    plt.title(f'SVM - {kernel.upper()} kernel')\n    \n    # DokÅ‚adnoÅ›Ä‡\n    accuracy = accuracy_score(y_test, svm.predict(X_test_2d_scaled))\n    plt.text(0.02, 0.98, f'Accuracy: {accuracy:.1%}', \n             transform=plt.gca().transAxes, \n             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n             verticalalignment='top')\n\nplt.tight_layout()\nplt.close()\n\n# Przygotuj dane dla peÅ‚nego modelu\nfeatures = ['dlugosc_tematu', 'liczba_wielkich_liter', 'liczba_wykrzyknikow', \n           'liczba_linkow', 'slowa_promocyjne', 'dlugosc_wiadomosci', 'godzina_wyslania']\nX_full = data[features]\nX_train_full, X_test_full, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42)\n\nscaler_full = StandardScaler()\nX_train_full_scaled = scaler_full.fit_transform(X_train_full)\nX_test_full_scaled = scaler_full.transform(X_test_full)\n\n# PorÃ³wnaj wyniki\nprint(\"PORÃ“WNANIE KERNELI (2D vs 7D features):\")\nprint(\"=\" * 50)\n\nfor kernel in kernels:\n    # 2D model\n    if kernel == 'poly':\n        svm_2d = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm_2d = SVC(kernel=kernel, random_state=42)\n    svm_2d.fit(X_train_2d_scaled, y_train)\n    acc_2d = accuracy_score(y_test, svm_2d.predict(X_test_2d_scaled))\n    \n    # 7D model\n    if kernel == 'poly':\n        svm_7d = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm_7d = SVC(kernel=kernel, random_state=42)\n    svm_7d.fit(X_train_full_scaled, y_train)\n    acc_7d = accuracy_score(y_test, svm_7d.predict(X_test_full_scaled))\n    \n    print(f\"{kernel.upper():6} - 2D: {acc_2d:.1%}, 7D: {acc_7d:.1%}, Poprawa: +{(acc_7d-acc_2d)*100:.1f}pp\")\n\n# OdtwÃ³rz wizualizacjÄ™\nplt.figure(figsize=(15, 5))\n\nfor i, kernel in enumerate(kernels):\n    plt.subplot(1, 3, i+1)\n    \n    if kernel == 'poly':\n        svm = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm = SVC(kernel=kernel, random_state=42)\n    \n    svm.fit(X_train_2d_scaled, y_train)\n    \n    h = 0.02\n    x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1\n    y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    \n    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    scatter = plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], \n                         c=y_train, cmap=plt.cm.RdYlBu, edgecolors='black')\n    \n    if hasattr(svm, 'support_'):\n        plt.scatter(X_train_2d_scaled[svm.support_, 0], \n                   X_train_2d_scaled[svm.support_, 1],\n                   s=100, facecolors='none', edgecolors='black', linewidth=2)\n    \n    plt.xlabel('Liczba wielkich liter (scaled)')\n    plt.ylabel('Liczba wykrzyknikÃ³w (scaled)')\n    plt.title(f'SVM - {kernel.upper()} kernel')\n    \n    accuracy = accuracy_score(y_test, svm.predict(X_test_2d_scaled))\n    plt.text(0.02, 0.98, f'Accuracy: {accuracy:.1%}', \n             transform=plt.gca().transAxes, \n             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n             verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nPokaÅ¼ wizualizacjÄ™ granic i porÃ³wnanie\n\n\n\n\n\n\n\nPORÃ“WNANIE KERNELI (2D vs 7D features):\n==================================================\nLINEAR - 2D: 100.0%, 7D: 100.0%, Poprawa: +0.0pp\nRBF    - 2D: 100.0%, 7D: 100.0%, Poprawa: +0.0pp\nPOLY   - 2D: 98.2%, 7D: 100.0%, Poprawa: +1.7pp\n\n\n\n\n\nWizualizacja granic decyzyjnych SVM dla rÃ³Å¼nych kerneli"
  },
  {
    "objectID": "cheatsheets/06-svm.html#rodzaje-kerneli-i-ich-zastosowania",
    "href": "cheatsheets/06-svm.html#rodzaje-kerneli-i-ich-zastosowania",
    "title": "Support Vector Machines â€” optymalne granice decyzyjne",
    "section": "ğŸ¯ Rodzaje kerneli i ich zastosowania",
    "text": "ğŸ¯ Rodzaje kerneli i ich zastosowania\n\n1) Linear Kernel - proste granice\n\n# UÅ¼yj gdy:\n# - Dane sÄ… liniowo separowalne\n# - Masz duÅ¼o features (high-dimensional)\n# - Potrzebujesz szybkiego modelu\n# - Chcesz interpretowalnoÅ›Ä‡\n\nlinear_use_cases = [\n    \"Text classification (high-dimensional sparse data)\",\n    \"Gene expression analysis\", \n    \"Document categorization\",\n    \"Sentiment analysis z bag-of-words\"\n]\n\nprint(\"LINEAR KERNEL - najlepsze zastosowania:\")\nfor i, use_case in enumerate(linear_use_cases, 1):\n    print(f\"{i}. {use_case}\")\n\n# Demo: Linear SVM na prostych danych\nX_simple = np.random.randn(100, 2)\ny_simple = (X_simple[:, 0] + X_simple[:, 1] &gt; 0).astype(int)\n\nlinear_svm = SVC(kernel='linear')\nlinear_svm.fit(X_simple, y_simple)\nprint(f\"\\nLinear SVM na prostych danych: {linear_svm.score(X_simple, y_simple):.1%}\")\n\n\n\n\n\n\n\nPokaÅ¼ zastosowania Linear Kernel\n\n\n\n\n\n\n\nLINEAR KERNEL - najlepsze zastosowania:\n1. Text classification (high-dimensional sparse data)\n2. Gene expression analysis\n3. Document categorization\n4. Sentiment analysis z bag-of-words\n\nLinear SVM na prostych danych: 98.0%\n\n\n\n\n\n\n\n2) RBF Kernel - uniwersalny wybÃ³r\n\n# RBF (Radial Basis Function) - najczÄ™Å›ciej uÅ¼ywany\n# MoÅ¼e aproksymowaÄ‡ dowolnÄ… granicÄ™ decyzyjnÄ…!\n\nrbf_use_cases = [\n    \"Image classification\",\n    \"Medical diagnosis\", \n    \"Fraud detection\",\n    \"Customer segmentation\",\n    \"General classification tasks\"\n]\n\nprint(\"RBF KERNEL - najlepsze zastosowania:\")\nfor i, use_case in enumerate(rbf_use_cases, 1):\n    print(f\"{i}. {use_case}\")\n\n# Demo: wpÅ‚yw parametru gamma\ngammas = [0.1, 1.0, 10.0]\nprint(f\"\\nWpÅ‚yw parametru gamma w RBF:\")\n\nfor gamma in gammas:\n    rbf_svm = SVC(kernel='rbf', gamma=gamma, random_state=42)\n    rbf_svm.fit(X_train_full_scaled, y_train)\n    accuracy = rbf_svm.score(X_test_full_scaled, y_test)\n    print(f\"Gamma {gamma:4.1f}: Accuracy = {accuracy:.1%}, Support Vectors = {rbf_svm.n_support_.sum()}\")\n\nprint(\"\\nğŸ’¡ Gamma wysoka = bardziej skomplikowane granice\")\nprint(\"ğŸ’¡ Gamma niska = gÅ‚adsze granice\")\n\n\n\n\n\n\n\nPokaÅ¼ zastosowania RBF Kernel\n\n\n\n\n\n\n\nRBF KERNEL - najlepsze zastosowania:\n1. Image classification\n2. Medical diagnosis\n3. Fraud detection\n4. Customer segmentation\n5. General classification tasks\n\nWpÅ‚yw parametru gamma w RBF:\nGamma  0.1: Accuracy = 100.0%, Support Vectors = 26\nGamma  1.0: Accuracy = 100.0%, Support Vectors = 374\nGamma 10.0: Accuracy = 100.0%, Support Vectors = 1546\n\nğŸ’¡ Gamma wysoka = bardziej skomplikowane granice\nğŸ’¡ Gamma niska = gÅ‚adsze granice\n\n\n\n\n\n\n\n3) Polynomial Kernel - dla zÅ‚oÅ¼onych wzorcÃ³w\n\npoly_use_cases = [\n    \"Computer vision (shape recognition)\",\n    \"Bioinformatics (protein classification)\",\n    \"Natural language processing\",\n    \"Pattern recognition\"\n]\n\nprint(\"POLYNOMIAL KERNEL - najlepsze zastosowania:\")\nfor i, use_case in enumerate(poly_use_cases, 1):\n    print(f\"{i}. {use_case}\")\n\n# Demo: wpÅ‚yw stopnia wielomianu\ndegrees = [2, 3, 4, 5]\nprint(f\"\\nWpÅ‚yw stopnia wielomianu:\")\n\nfor degree in degrees:\n    poly_svm = SVC(kernel='poly', degree=degree, random_state=42)\n    poly_svm.fit(X_train_full_scaled, y_train)\n    accuracy = poly_svm.score(X_test_full_scaled, y_test)\n    print(f\"Degree {degree}: Accuracy = {accuracy:.1%}\")\n\nprint(\"\\nâš ï¸ Uwaga: wyÅ¼szie stopnie = ryzyko overfittingu!\")\n\n\n\n\n\n\n\nPokaÅ¼ zastosowania Polynomial Kernel\n\n\n\n\n\n\n\nPOLYNOMIAL KERNEL - najlepsze zastosowania:\n1. Computer vision (shape recognition)\n2. Bioinformatics (protein classification)\n3. Natural language processing\n4. Pattern recognition\n\nWpÅ‚yw stopnia wielomianu:\nDegree 2: Accuracy = 99.8%\nDegree 3: Accuracy = 100.0%\nDegree 4: Accuracy = 99.5%\nDegree 5: Accuracy = 100.0%\n\nâš ï¸ Uwaga: wyÅ¼szie stopnie = ryzyko overfittingu!"
  },
  {
    "objectID": "cheatsheets/06-svm.html#tuning-parametrÃ³w-svm",
    "href": "cheatsheets/06-svm.html#tuning-parametrÃ³w-svm",
    "title": "Support Vector Machines â€” optymalne granice decyzyjne",
    "section": "âš™ï¸ Tuning parametrÃ³w SVM",
    "text": "âš™ï¸ Tuning parametrÃ³w SVM\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Grid Search dla RBF kernel\nparam_grid_rbf = {\n    'C': [0.1, 1, 10, 100],           # regularization strength\n    'gamma': [0.001, 0.01, 0.1, 1]   # kernel coefficient\n}\n\nprint(\"Rozpoczynam Grid Search dla RBF SVM...\")\ngrid_search = GridSearchCV(\n    SVC(kernel='rbf', random_state=42),\n    param_grid_rbf,\n    cv=3,  # 3-fold CV dla szybkoÅ›ci\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=0\n)\n\ngrid_search.fit(X_train_full_scaled, y_train)\n\nprint(\"Najlepsze parametry RBF:\")\nprint(grid_search.best_params_)\nprint(f\"Najlepsza dokÅ‚adnoÅ›Ä‡ CV: {grid_search.best_score_:.1%}\")\n\n# Test na zbiorze testowym\nbest_svm = grid_search.best_estimator_\ntest_accuracy = best_svm.score(X_test_full_scaled, y_test)\nprint(f\"DokÅ‚adnoÅ›Ä‡ na test set: {test_accuracy:.1%}\")\n\n# PorÃ³wnanie przed/po tuningu\nbaseline_svm = SVC(kernel='rbf', random_state=42)\nbaseline_svm.fit(X_train_full_scaled, y_train)\nbaseline_accuracy = baseline_svm.score(X_test_full_scaled, y_test)\n\nprint(f\"\\nPorÃ³wnanie:\")\nprint(f\"Baseline RBF:      {baseline_accuracy:.1%}\")\nprint(f\"Tuned RBF:         {test_accuracy:.1%}\")\nprint(f\"Poprawa:           +{(test_accuracy - baseline_accuracy)*100:.1f}pp\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki tuningu\n\n\n\n\n\n\n\nNajlepsze parametry RBF:\n{'C': 0.1, 'gamma': 0.01}\nNajlepsza dokÅ‚adnoÅ›Ä‡ CV: 100.0%\nDokÅ‚adnoÅ›Ä‡ na test set: 100.0%\n\nPorÃ³wnanie:\nBaseline RBF:      100.0%\nTuned RBF:         100.0%\nPoprawa:           +0.0pp"
  },
  {
    "objectID": "cheatsheets/06-svm.html#puÅ‚apki-i-rozwiÄ…zania",
    "href": "cheatsheets/06-svm.html#puÅ‚apki-i-rozwiÄ…zania",
    "title": "Support Vector Machines â€” optymalne granice decyzyjne",
    "section": "âš ï¸ PuÅ‚apki i rozwiÄ…zania",
    "text": "âš ï¸ PuÅ‚apki i rozwiÄ…zania\n\n1) Brak standaryzacji\n\n# Problem: SVM jest bardzo wraÅ¼liwy na skale danych\nprint(\"DEMONSTRACJA: wpÅ‚yw standaryzacji\")\n\n# Bez standaryzacji\nsvm_unscaled = SVC(kernel='rbf', random_state=42)\nsvm_unscaled.fit(X_train_full, y_train)  # raw data!\nacc_unscaled = svm_unscaled.score(X_test_full, y_test)\n\n# Ze standaryzacjÄ…\nsvm_scaled = SVC(kernel='rbf', random_state=42)\nsvm_scaled.fit(X_train_full_scaled, y_train)  # scaled data!\nacc_scaled = svm_scaled.score(X_test_full_scaled, y_test)\n\nprint(f\"Bez standaryzacji: {acc_unscaled:.1%}\")\nprint(f\"Ze standaryzacjÄ…:  {acc_scaled:.1%}\")\nprint(f\"Poprawa:           +{(acc_scaled - acc_unscaled)*100:.1f}pp\")\nprint(\"\\nâœ… ZAWSZE standaryzuj dane przed SVM!\")\n\n\n\n\n\n\n\nPokaÅ¼ wpÅ‚yw standaryzacji\n\n\n\n\n\n\n\nBez standaryzacji: 88.0%\nZe standaryzacjÄ…:  100.0%\nPoprawa:           +12.0pp\n\nâœ… ZAWSZE standaryzuj dane przed SVM!\n\n\n\n\n\n\n\n2) Niezbalansowane klasy\n\n# Problem: gdy jedna klasa jest bardzo rzadka\nprint(\"PROBLEM: niezbalansowane klasy\")\n\n# SprawdÅº balans w naszych danych\nprint(f\"Balans klas: Spam {y_train.mean():.1%}, Ham {(1-y_train.mean()):.1%}\")\n\n# RozwiÄ…zanie 1: class_weight='balanced'\nsvm_balanced = SVC(kernel='rbf', class_weight='balanced', random_state=42)\nsvm_balanced.fit(X_train_full_scaled, y_train)\n\n# RozwiÄ…zanie 2: manual class weights\nspam_weight = len(y_train) / (2 * y_train.sum())  # 1/frequency\nham_weight = len(y_train) / (2 * (len(y_train) - y_train.sum()))\nclass_weights = {0: ham_weight, 1: spam_weight}\n\nsvm_manual = SVC(kernel='rbf', class_weight=class_weights, random_state=42)\nsvm_manual.fit(X_train_full_scaled, y_train)\n\n# PorÃ³wnanie\nsvm_normal = SVC(kernel='rbf', random_state=42)\nsvm_normal.fit(X_train_full_scaled, y_train)\n\nmodels = {\n    'Normal': svm_normal,\n    'Balanced': svm_balanced, \n    'Manual weights': svm_manual\n}\n\nprint(\"\\nPorÃ³wnanie podejÅ›Ä‡:\")\nfor name, model in models.items():\n    pred = model.predict(X_test_full_scaled)\n    accuracy = accuracy_score(y_test, pred)\n    \n    # SprawdÅº recall dla spam (waÅ¼ne!)\n    spam_indices = y_test == 1\n    spam_recall = np.mean(pred[spam_indices] == 1) if spam_indices.sum() &gt; 0 else 0\n    \n    print(f\"{name:12}: Accuracy={accuracy:.1%}, Spam Recall={spam_recall:.1%}\")\n\n\n\n\n\n\n\nPokaÅ¼ rozwiÄ…zania dla niezbalansowanych klas\n\n\n\n\n\n\n\nBalans klas: Spam 30.9%, Ham 69.1%\n\nPorÃ³wnanie podejÅ›Ä‡:\nNormal      : Accuracy=100.0%, Spam Recall=100.0%\nBalanced    : Accuracy=100.0%, Spam Recall=100.0%\nManual weights: Accuracy=100.0%, Spam Recall=100.0%\n\n\n\n\n\n\n\n3) Overfitting z wysokim C\n\n# Problem: zbyt wysoka wartoÅ›Ä‡ C prowadzi do overfittingu\nC_values = [0.01, 0.1, 1, 10, 100, 1000]\nresults = []\n\nprint(\"WpÅ‚yw parametru C (regularization):\")\nprint(\"C      | Train Acc | Test Acc | Difference | Support Vectors\")\nprint(\"-\" * 60)\n\nfor C in C_values:\n    svm = SVC(kernel='rbf', C=C, random_state=42)\n    svm.fit(X_train_full_scaled, y_train)\n    \n    train_acc = svm.score(X_train_full_scaled, y_train)\n    test_acc = svm.score(X_test_full_scaled, y_test)\n    diff = train_acc - test_acc\n    n_sv = svm.n_support_.sum()\n    \n    print(f\"{C:6.2f} | {train_acc:8.1%} | {test_acc:7.1%} | {diff:9.1%} | {n_sv:14d}\")\n    results.append((C, train_acc, test_acc, diff, n_sv))\n\nprint(\"\\nğŸ’¡ Optimalne C: wysokie test accuracy, maÅ‚a rÃ³Å¼nica train-test\")\nprint(\"ğŸ’¡ Zbyt wysokie C: perfect train accuracy, sÅ‚abe test accuracy\")\n\n\n\n\n\n\n\nPokaÅ¼ wpÅ‚yw parametru C\n\n\n\n\n\n\n\nWpÅ‚yw parametru C (regularization):\nC      | Train Acc | Test Acc | Difference | Support Vectors\n------------------------------------------------------------\n  0.01 |   100.0% |  100.0% |      0.0% |            582\n  0.10 |   100.0% |  100.0% |      0.0% |             99\n  1.00 |   100.0% |  100.0% |      0.0% |             37\n 10.00 |   100.0% |  100.0% |      0.0% |             35\n100.00 |   100.0% |  100.0% |      0.0% |             35\n1000.00 |   100.0% |  100.0% |      0.0% |             35\n\nğŸ’¡ Optimalne C: wysokie test accuracy, maÅ‚a rÃ³Å¼nica train-test\nğŸ’¡ Zbyt wysokie C: perfect train accuracy, sÅ‚abe test accuracy"
  },
  {
    "objectID": "cheatsheets/06-svm.html#real-world-przypadki-uÅ¼ycia",
    "href": "cheatsheets/06-svm.html#real-world-przypadki-uÅ¼ycia",
    "title": "Support Vector Machines â€” optymalne granice decyzyjne",
    "section": "ğŸŒ Real-world przypadki uÅ¼ycia",
    "text": "ğŸŒ Real-world przypadki uÅ¼ycia\n\nText Classification: Spam detection, sentiment analysis, document categorization\nComputer Vision: Image classification, face recognition, medical image analysis\nFinance: Credit scoring, algorithmic trading, fraud detection\nHealthcare: Disease diagnosis, drug discovery, medical image analysis\nCybersecurity: Intrusion detection, malware classification, network security\n\n\n\n\n\n\n\nğŸ’¡ Kiedy uÅ¼ywaÄ‡ SVM?\n\n\n\nâœ… UÅ»YJ GDY:\n\nPotrzebujesz wysokiej dokÅ‚adnoÅ›ci klasyfikacji\nMasz Å›redniej wielkoÅ›ci dataset (1K-100K samples)\nDane majÄ… clear margins miÄ™dzy klasami\nNie potrzebujesz probabilistic outputs\nMemory efficiency jest waÅ¼na\n\nâŒ NIE UÅ»YWAJ GDY:\n\nMasz bardzo duÅ¼e datasety (&gt;100K samples - uÅ¼yj Neural Networks)\nPotrzebujesz probabilistic predictions (uÅ¼yj Logistic Regression)\nDane majÄ… duÅ¼o noiseâ€™u (uÅ¼yj Random Forest)\nTarget jest continuous (uÅ¼yj SVR - Support Vector Regression)\nPotrzebujesz bardzo szybkiego inference (uÅ¼yj Decision Tree)\n\n\n\nPodsumowanie: SVM to potÄ™Å¼ny algoritm do klasyfikacji z optymalnÄ… separacjÄ… klas. Wybierz kernel w zaleÅ¼noÅ›ci od zÅ‚oÅ¼onoÅ›ci danych: Linear dla prostych, RBF dla uniwersalnych, Polynomial dla specjalnych przypadkÃ³w. ğŸ¯"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html",
    "href": "cheatsheets/04-random-forest.html",
    "title": "Random Forest â€” las drzew decyzyjnych",
    "section": "",
    "text": "Random Forest to zespÃ³Å‚ wielu drzew decyzyjnych, ktÃ³re gÅ‚osujÄ… razem nad koÅ„cowÄ… decyzjÄ…. Jeden Decision Tree moÅ¼e siÄ™ myliÄ‡, ale gdy masz 100 drzew i wiÄ™kszoÅ›Ä‡ mÃ³wi â€œTAKâ€ - to prawdopodobnie dobra odpowiedÅº!\n\n\n\n\n\n\nğŸ’¡ Intuicja\n\n\n\nWyobraÅº sobie konsylium lekarskim: jeden lekarz moÅ¼e siÄ™ pomyliÄ‡ w diagnozie, ale gdy 7 z 10 ekspertÃ³w zgadza siÄ™ - diagnoza jest znacznie bardziej pewna. Random Forest dziaÅ‚a dokÅ‚adnie tak samo!"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#czym-jest-random-forest",
    "href": "cheatsheets/04-random-forest.html#czym-jest-random-forest",
    "title": "Random Forest â€” las drzew decyzyjnych",
    "section": "",
    "text": "Random Forest to zespÃ³Å‚ wielu drzew decyzyjnych, ktÃ³re gÅ‚osujÄ… razem nad koÅ„cowÄ… decyzjÄ…. Jeden Decision Tree moÅ¼e siÄ™ myliÄ‡, ale gdy masz 100 drzew i wiÄ™kszoÅ›Ä‡ mÃ³wi â€œTAKâ€ - to prawdopodobnie dobra odpowiedÅº!\n\n\n\n\n\n\nğŸ’¡ Intuicja\n\n\n\nWyobraÅº sobie konsylium lekarskim: jeden lekarz moÅ¼e siÄ™ pomyliÄ‡ w diagnozie, ale gdy 7 z 10 ekspertÃ³w zgadza siÄ™ - diagnoza jest znacznie bardziej pewna. Random Forest dziaÅ‚a dokÅ‚adnie tak samo!"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#praktyczny-przykÅ‚ad-przewidywanie-sukcesu-produktu",
    "href": "cheatsheets/04-random-forest.html#praktyczny-przykÅ‚ad-przewidywanie-sukcesu-produktu",
    "title": "Random Forest â€” las drzew decyzyjnych",
    "section": "ğŸ¯ Praktyczny przykÅ‚ad: przewidywanie sukcesu produktu",
    "text": "ğŸ¯ Praktyczny przykÅ‚ad: przewidywanie sukcesu produktu\nCzy nowy produkt odniesie sukces na rynku?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Tworzenie realistycznych danych produktÃ³w\nnp.random.seed(42)\nn_products = 2000\n\ndata = pd.DataFrame({\n    'cena': np.random.lognormal(6, 1, n_products),  # log-normal dla cen\n    'jakosc': np.random.randint(1, 11, n_products),  # 1-10 skala jakoÅ›ci\n    'marketing_budget': np.random.exponential(50000, n_products),\n    'konkurencja': np.random.randint(1, 21, n_products),  # liczba konkurentÃ³w\n    'sezon': np.random.choice(['wiosna', 'lato', 'jesien', 'zima'], n_products),\n    'kategoria': np.random.choice(['elektronika', 'moda', 'dom', 'sport', 'ksiazki'], n_products),\n    'ocena_testowa': np.random.normal(7, 2, n_products),  # oceny focus group\n    'innowacyjnosc': np.random.randint(1, 11, n_products),\n    'marka_znana': np.random.choice([0, 1], n_products, p=[0.3, 0.7])  # czy znana marka\n})\n\n# Realistyczna logika sukcesu produktu\ndef czy_sukces(row):\n    score = 0\n    \n    # Cena (sweet spot 50-500)\n    if 50 &lt;= row['cena'] &lt;= 500:\n        score += 2\n    elif row['cena'] &gt; 1000:\n        score -= 2\n    \n    # JakoÅ›Ä‡ (najwaÅ¼niejsze!)\n    score += row['jakosc'] * 0.8\n    \n    # Marketing\n    if row['marketing_budget'] &gt; 100000:\n        score += 3\n    elif row['marketing_budget'] &gt; 50000:\n        score += 1\n    \n    # Konkurencja (mniej = lepiej)\n    score -= row['konkurencja'] * 0.2\n    \n    # Sezon (lato i zima lepsze)\n    if row['sezon'] in ['lato', 'zima']:\n        score += 1\n    \n    # Kategoria\n    if row['kategoria'] == 'elektronika':\n        score += 1\n    elif row['kategoria'] == 'ksiazki':\n        score -= 1\n    \n    # Ocena testowa\n    score += (row['ocena_testowa'] - 5) * 0.5\n    \n    # InnowacyjnoÅ›Ä‡\n    score += row['innowacyjnosc'] * 0.3\n    \n    # Znana marka\n    if row['marka_znana']:\n        score += 2\n    \n    # KoÅ„cowa decyzja z odrobinÄ… losowoÅ›ci\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 1)))\n    return probability &gt; 0.5\n\ndata['sukces'] = data.apply(czy_sukces, axis=1)\n\nprint(\"Statystyki produktÃ³w:\")\nprint(data.describe())\nprint(f\"\\nOdsetek udanych produktÃ³w: {data['sukces'].mean():.1%}\")\n\n\n\n\n\n\n\nPokaÅ¼ statystyki produktÃ³w\n\n\n\n\n\n\n\nStatystyki produktÃ³w:\n               cena       jakosc  marketing_budget  konkurencja  \\\ncount   2000.000000  2000.000000       2000.000000   2000.00000   \nmean     692.530569     5.534500      49376.934612     10.53950   \nstd      929.701850     2.871922      49876.166534      5.76594   \nmin       15.779832     1.000000         11.353200      1.00000   \n25%      216.445352     3.000000      13957.783468      6.00000   \n50%      421.867820     6.000000      33526.684882     11.00000   \n75%      798.695019     8.000000      67692.822351     16.00000   \nmax    19010.210160    10.000000     376260.171802     20.00000   \n\n       ocena_testowa  innowacyjnosc  marka_znana  \ncount    2000.000000    2000.000000  2000.000000  \nmean        7.011461       5.500000     0.711000  \nstd         1.957420       2.909679     0.453411  \nmin        -0.844801       1.000000     0.000000  \n25%         5.700781       3.000000     0.000000  \n50%         7.041344       6.000000     1.000000  \n75%         8.307413       8.000000     1.000000  \nmax        13.754766      10.000000     1.000000  \n\nOdsetek udanych produktÃ³w: 99.2%"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#budowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/04-random-forest.html#budowanie-modelu-krok-po-kroku",
    "title": "Random Forest â€” las drzew decyzyjnych",
    "section": "ğŸ”§ Budowanie modelu krok po kroku",
    "text": "ğŸ”§ Budowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Encoding kategorycznych zmiennych\nfrom sklearn.preprocessing import LabelEncoder\n\ndata_encoded = data.copy()\nle_sezon = LabelEncoder()\nle_kategoria = LabelEncoder()\n\ndata_encoded['sezon_num'] = le_sezon.fit_transform(data['sezon'])\ndata_encoded['kategoria_num'] = le_kategoria.fit_transform(data['kategoria'])\n\n# Features do modelu\nfeature_columns = ['cena', 'jakosc', 'marketing_budget', 'konkurencja', \n                  'sezon_num', 'kategoria_num', 'ocena_testowa', \n                  'innowacyjnosc', 'marka_znana']\nX = data_encoded[feature_columns]\ny = data_encoded['sukces']\n\n# PodziaÅ‚ train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Dane treningowe: {len(X_train)} produktÃ³w\")\nprint(f\"Dane testowe: {len(X_test)} produktÃ³w\")\n\n\n\n\n\n\n\nPokaÅ¼ podziaÅ‚ danych\n\n\n\n\n\n\n\nDane treningowe: 1600 produktÃ³w\nDane testowe: 400 produktÃ³w\n\n\n\n\n\n\n\n2) Trenowanie Random Forest\n\n# Tworzenie modelu Random Forest\nrf_model = RandomForestClassifier(\n    n_estimators=100,       # liczba drzew w lesie\n    max_depth=10,          # maksymalna gÅ‚Ä™bokoÅ›Ä‡ kaÅ¼dego drzewa\n    min_samples_split=20,   # min. prÃ³bek do podziaÅ‚u\n    min_samples_leaf=10,    # min. prÃ³bek w liÅ›ciu\n    random_state=42,\n    n_jobs=-1              # uÅ¼yj wszystkie rdzenie CPU\n)\n\n# Trenowanie\nrf_model.fit(X_train, y_train)\n\nprint(\"Random Forest wytrenowany!\")\nprint(f\"Liczba drzew: {rf_model.n_estimators}\")\nprint(f\"Åšrednia gÅ‚Ä™bokoÅ›Ä‡ drzew: {np.mean([tree.tree_.max_depth for tree in rf_model.estimators_]):.1f}\")\n\n\n\n\n\n\n\nPokaÅ¼ parametry wytrenowanego modelu\n\n\n\n\n\n\n\nRandom Forest wytrenowany!\nLiczba drzew: 100\nÅšrednia gÅ‚Ä™bokoÅ›Ä‡ drzew: 5.2\n\n\n\n\n\n\n\n3) Ewaluacja modelu\n\n# Predykcje\ny_pred = rf_model.predict(X_test)\ny_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n\n# Metryki\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nDokÅ‚adnoÅ›Ä‡ Random Forest: {accuracy:.1%}\")\n\n# SzczegÃ³Å‚owy raport\nprint(\"\\nRaport klasyfikacji:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:     PoraÅ¼ka  Sukces\")\nprint(f\"Rzeczywiste PoraÅ¼ka: {cm[0,0]:3d}     {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Sukces:  {cm[1,0]:3d}     {cm[1,1]:3d}\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki Random Forest\n\n\n\n\n\n\n\n\nDokÅ‚adnoÅ›Ä‡ Random Forest: 99.8%\n\nConfusion Matrix:\nPrzewidywane:     PoraÅ¼ka  Sukces\nRzeczywiste PoraÅ¼ka:   0       1\nRzeczywiste Sukces:    0     399"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#analiza-waÅ¼noÅ›ci-cech-feature-importance",
    "href": "cheatsheets/04-random-forest.html#analiza-waÅ¼noÅ›ci-cech-feature-importance",
    "title": "Random Forest â€” las drzew decyzyjnych",
    "section": "ğŸ“Š Analiza waÅ¼noÅ›ci cech (Feature Importance)",
    "text": "ğŸ“Š Analiza waÅ¼noÅ›ci cech (Feature Importance)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Przygotuj dane (powtarzamy dla kompletnoÅ›ci)\nnp.random.seed(42)\nn_products = 2000\n\ndata = pd.DataFrame({\n    'cena': np.random.lognormal(6, 1, n_products),\n    'jakosc': np.random.randint(1, 11, n_products),\n    'marketing_budget': np.random.exponential(50000, n_products),\n    'konkurencja': np.random.randint(1, 21, n_products),\n    'sezon': np.random.choice(['wiosna', 'lato', 'jesien', 'zima'], n_products),\n    'kategoria': np.random.choice(['elektronika', 'moda', 'dom', 'sport', 'ksiazki'], n_products),\n    'ocena_testowa': np.random.normal(7, 2, n_products),\n    'innowacyjnosc': np.random.randint(1, 11, n_products),\n    'marka_znana': np.random.choice([0, 1], n_products, p=[0.3, 0.7])\n})\n\ndef czy_sukces(row):\n    score = 0\n    if 50 &lt;= row['cena'] &lt;= 500:\n        score += 2\n    elif row['cena'] &gt; 1000:\n        score -= 2\n    score += row['jakosc'] * 0.8\n    if row['marketing_budget'] &gt; 100000:\n        score += 3\n    elif row['marketing_budget'] &gt; 50000:\n        score += 1\n    score -= row['konkurencja'] * 0.2\n    if row['sezon'] in ['lato', 'zima']:\n        score += 1\n    if row['kategoria'] == 'elektronika':\n        score += 1\n    elif row['kategoria'] == 'ksiazki':\n        score -= 1\n    score += (row['ocena_testowa'] - 5) * 0.5\n    score += row['innowacyjnosc'] * 0.3\n    if row['marka_znana']:\n        score += 2\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 1)))\n    return probability &gt; 0.5\n\ndata['sukces'] = data.apply(czy_sukces, axis=1)\n\n# Encoding i model\ndata_encoded = data.copy()\nle_sezon = LabelEncoder()\nle_kategoria = LabelEncoder()\ndata_encoded['sezon_num'] = le_sezon.fit_transform(data['sezon'])\ndata_encoded['kategoria_num'] = le_kategoria.fit_transform(data['kategoria'])\n\nfeature_columns = ['cena', 'jakosc', 'marketing_budget', 'konkurencja', \n                  'sezon_num', 'kategoria_num', 'ocena_testowa', \n                  'innowacyjnosc', 'marka_znana']\nX = data_encoded[feature_columns]\ny = data_encoded['sukces']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenuj model\nrf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\nrf_model.fit(X_train, y_train)\n\n# Ewaluacja\ny_pred = rf_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\n# WaÅ¼noÅ›Ä‡ cech\nfeature_names = ['cena', 'jakoÅ›Ä‡', 'marketing_budget', 'konkurencja', \n                'sezon', 'kategoria', 'ocena_testowa', 'innowacyjnoÅ›Ä‡', 'marka_znana']\nimportance = rf_model.feature_importances_\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importance\n}).sort_values('importance', ascending=False)\n\n# Wykres waÅ¼noÅ›ci cech\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance['feature'], feature_importance['importance'])\nplt.xlabel('WaÅ¼noÅ›Ä‡ cechy')\nplt.title('WaÅ¼noÅ›Ä‡ cech w przewidywaniu sukcesu produktu')\nplt.gca().invert_yaxis()\nplt.close()\n\nprint(f\"Wyniki Random Forest:\")\nprint(f\"DokÅ‚adnoÅ›Ä‡ modelu: {accuracy:.1%}\")\nprint(f\"Liczba drzew: {rf_model.n_estimators}\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:     PoraÅ¼ka  Sukces\")\nprint(f\"Rzeczywiste PoraÅ¼ka: {cm[0,0]:3d}     {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Sukces:  {cm[1,0]:3d}     {cm[1,1]:3d}\")\n\nprint(\"\\nWaÅ¼noÅ›Ä‡ cech:\")\nfor _, row in feature_importance.iterrows():\n    print(f\"{row['feature']}: {row['importance']:.3f}\")\n\n# OdtwÃ³rz wykres\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance['feature'], feature_importance['importance'])\nplt.xlabel('WaÅ¼noÅ›Ä‡ cechy')\nplt.title('WaÅ¼noÅ›Ä‡ cech w przewidywaniu sukcesu produktu')\nplt.gca().invert_yaxis()\nplt.show()\n\n\n\n\n\n\n\nPokaÅ¼ wyniki i wykres waÅ¼noÅ›ci cech\n\n\n\n\n\n\n\nWyniki Random Forest:\nDokÅ‚adnoÅ›Ä‡ modelu: 99.8%\nLiczba drzew: 100\n\nConfusion Matrix:\nPrzewidywane:     PoraÅ¼ka  Sukces\nRzeczywiste PoraÅ¼ka:   0       1\nRzeczywiste Sukces:    0     399\n\nWaÅ¼noÅ›Ä‡ cech:\ncena: 0.294\nmarketing_budget: 0.168\nocena_testowa: 0.143\njakoÅ›Ä‡: 0.117\nkonkurencja: 0.105\ninnowacyjnoÅ›Ä‡: 0.069\nsezon: 0.042\nkategoria: 0.033\nmarka_znana: 0.028\n\n\n\n\n\nWaÅ¼noÅ›Ä‡ cech w Random Forest"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#random-forest-vs-decision-tree---porÃ³wnanie",
    "href": "cheatsheets/04-random-forest.html#random-forest-vs-decision-tree---porÃ³wnanie",
    "title": "Random Forest â€” las drzew decyzyjnych",
    "section": "ğŸ” Random Forest vs Decision Tree - porÃ³wnanie",
    "text": "ğŸ” Random Forest vs Decision Tree - porÃ³wnanie\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# PorÃ³wnanie z pojedynczym Decision Tree\ndt_model = DecisionTreeClassifier(max_depth=10, random_state=42)\ndt_model.fit(X_train, y_train)\n\ndt_pred = dt_model.predict(X_test)\ndt_accuracy = accuracy_score(y_test, dt_pred)\n\nprint(\"PORÃ“WNANIE MODELI:\")\nprint(f\"Decision Tree:    {dt_accuracy:.1%}\")\nprint(f\"Random Forest:    {accuracy:.1%}\")\nprint(f\"Poprawa:          +{(accuracy - dt_accuracy)*100:.1f} punktÃ³w procentowych\")\n\n# Test stabilnoÅ›ci - rÃ³Å¼ne random_state\ndt_results = []\nrf_results = []\n\nfor rs in range(10):\n    # Decision Tree\n    dt_temp = DecisionTreeClassifier(max_depth=10, random_state=rs)\n    dt_temp.fit(X_train, y_train)\n    dt_results.append(accuracy_score(y_test, dt_temp.predict(X_test)))\n    \n    # Random Forest  \n    rf_temp = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=rs)\n    rf_temp.fit(X_train, y_train)\n    rf_results.append(accuracy_score(y_test, rf_temp.predict(X_test)))\n\nprint(f\"\\nSTABILNOÅšÄ† (10 rÃ³Å¼nych random_state):\")\nprint(f\"Decision Tree rozrzut: {max(dt_results)-min(dt_results):.1%}\")\nprint(f\"Random Forest rozrzut: {max(rf_results)-min(rf_results):.1%}\")\nprint(\"Random Forest jest znacznie bardziej stabilny!\")\n\n\n\n\n\n\n\nPokaÅ¼ porÃ³wnanie modeli\n\n\n\n\n\n\n\nPORÃ“WNANIE MODELI:\nDecision Tree:    98.5%\nRandom Forest:    99.8%\nPoprawa:          +1.3 punktÃ³w procentowych\n\nSTABILNOÅšÄ† (10 rÃ³Å¼nych random_state):\nDecision Tree rozrzut: 1.0%\nRandom Forest rozrzut: 0.0%\nRandom Forest jest znacznie bardziej stabilny!"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#zastosowania-random-forest",
    "href": "cheatsheets/04-random-forest.html#zastosowania-random-forest",
    "title": "Random Forest â€” las drzew decyzyjnych",
    "section": "ğŸ¯ Zastosowania Random Forest",
    "text": "ğŸ¯ Zastosowania Random Forest\n\n1) E-commerce - rekomendacje produktÃ³w\n\n# Przewidywanie czy uÅ¼ytkownik kupi produkt\necommerce_features = ['historia_zakupÃ³w', 'czas_na_stronie', 'kategoria_preferowana', \n                     'cena_produktu', 'oceny_produktu', 'sezon']\n# Target: 'kupi_produkt' (tak/nie)\n\necommerce_rf = RandomForestClassifier(n_estimators=200)\n# Wynik: system rekomendacji uwzglÄ™dniajÄ…cy kompleksowe zachowania uÅ¼ytkownikÃ³w\n\n\n\n2) Finanse - wykrywanie fraudÃ³w\n\n# Identyfikacja podejrzanych transakcji\nfraud_features = ['kwota', 'godzina', 'lokalizacja', 'typ_karty', \n                 'historia_klienta', 'czÄ™stotliwoÅ›Ä‡_transakcji']\n# Target: 'fraud' (tak/nie)\n\nfraud_rf = RandomForestClassifier(n_estimators=500, class_weight='balanced')\n# Wynik: system antyfaudowy z wysokÄ… dokÅ‚adnoÅ›ciÄ…\n\n\n\n3) HR - przewidywanie odejÅ›Ä‡ pracownikÃ³w\n\n# Employee churn prediction\nhr_features = ['satisfaction', 'last_evaluation', 'projects', 'salary', \n              'time_company', 'work_accident', 'promotion']\n# Target: 'left_company' (tak/nie)\n\nhr_rf = RandomForestClassifier(n_estimators=300)\n# Wynik: wczesne ostrzeÅ¼enia przed odejÅ›ciami kluczowych pracownikÃ³w"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#tuning-parametrÃ³w-random-forest",
    "href": "cheatsheets/04-random-forest.html#tuning-parametrÃ³w-random-forest",
    "title": "Random Forest â€” las drzew decyzyjnych",
    "section": "âš™ï¸ Tuning parametrÃ³w Random Forest",
    "text": "âš™ï¸ Tuning parametrÃ³w Random Forest\n\nfrom sklearn.model_selection import GridSearchCV\n\n# NajwaÅ¼niejsze parametry do tuningu\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [5, 10, 15, None],\n    'min_samples_split': [10, 20, 50],\n    'min_samples_leaf': [5, 10, 20],\n    'max_features': ['sqrt', 'log2', None]\n}\n\n# Grid search z cross-validation (uwaga: moÅ¼e potrwaÄ‡!)\nprint(\"Rozpoczynam Grid Search - moÅ¼e potrwaÄ‡ kilka minut...\")\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_grid,\n    cv=3,  # redukcja CV dla szybkoÅ›ci\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(\"Najlepsze parametry:\")\nprint(grid_search.best_params_)\nprint(f\"Najlepsza dokÅ‚adnoÅ›Ä‡ CV: {grid_search.best_score_:.1%}\")\n\n# Model z najlepszymi parametrami\nbest_rf = grid_search.best_estimator_\nbest_accuracy = accuracy_score(y_test, best_rf.predict(X_test))\nprint(f\"DokÅ‚adnoÅ›Ä‡ na test set: {best_accuracy:.1%}\")\n\n\n\n\n\n\n\nPokaÅ¼ najlepsze parametry\n\n\n\n\n\n\n\nNajlepsze parametry:\n{'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 50}\nNajlepsza dokÅ‚adnoÅ›Ä‡ CV: 99.0%\nDokÅ‚adnoÅ›Ä‡ na test set: 99.8%"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#puÅ‚apki-i-rozwiÄ…zania",
    "href": "cheatsheets/04-random-forest.html#puÅ‚apki-i-rozwiÄ…zania",
    "title": "Random Forest â€” las drzew decyzyjnych",
    "section": "âš ï¸ PuÅ‚apki i rozwiÄ…zania",
    "text": "âš ï¸ PuÅ‚apki i rozwiÄ…zania\n\n1) Overfitting mimo ensemble\n\n# Problem: za gÅ‚Ä™bokie drzewa mogÄ… nadal prowadziÄ‡ do overfittingu\noverfitted_rf = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=None,    # bez limitu gÅ‚Ä™bokoÅ›ci!\n    min_samples_leaf=1  # pozwÃ³l na liÅ›cie z 1 prÃ³bkÄ…!\n)\noverfitted_rf.fit(X_train, y_train)\n\ntrain_acc_over = accuracy_score(y_train, overfitted_rf.predict(X_train))\ntest_acc_over = accuracy_score(y_test, overfitted_rf.predict(X_test))\n\nprint(f\"Overfitted Random Forest:\")\nprint(f\"Train accuracy: {train_acc_over:.1%}\")\nprint(f\"Test accuracy: {test_acc_over:.1%}\")\nprint(f\"RÃ³Å¼nica: {train_acc_over - test_acc_over:.1%}\")\n\n# RozwiÄ…zanie: odpowiednie parametry\nbalanced_rf = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,      # ogranicz gÅ‚Ä™bokoÅ›Ä‡\n    min_samples_leaf=10  # wymuÅ› wiÄ™cej prÃ³bek w liÅ›ciach\n)\nbalanced_rf.fit(X_train, y_train)\n\ntrain_acc_bal = accuracy_score(y_train, balanced_rf.predict(X_train))\ntest_acc_bal = accuracy_score(y_test, balanced_rf.predict(X_test))\n\nprint(f\"\\nBalanced Random Forest:\")\nprint(f\"Train accuracy: {train_acc_bal:.1%}\")\nprint(f\"Test accuracy: {test_acc_bal:.1%}\")\nprint(f\"RÃ³Å¼nica: {train_acc_bal - test_acc_bal:.1%} - znacznie lepiej!\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki overfittingu\n\n\n\n\n\n\n\nOverfitted Random Forest:\nTrain accuracy: 100.0%\nTest accuracy: 99.8%\nRÃ³Å¼nica: 0.2%\n\nBalanced Random Forest:\nTrain accuracy: 99.0%\nTest accuracy: 99.8%\nRÃ³Å¼nica: -0.8% - znacznie lepiej!\n\n\n\n\n\n\n\n2) Niezbalansowane klasy\n\n# Problem: gdy jedna klasa jest rzadka (np. 5% fraudÃ³w, 95% normalnych transakcji)\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# SprawdÅº balans klas w naszych danych\nprint(f\"RozkÅ‚ad klas:\")\nprint(f\"PoraÅ¼ka: {(~y_train).sum()} ({(~y_train).mean():.1%})\")\nprint(f\"Sukces: {y_train.sum()} ({y_train.mean():.1%})\")\n\n# RozwiÄ…zanie 1: class_weight='balanced'\nbalanced_class_rf = RandomForestClassifier(\n    n_estimators=100,\n    class_weight='balanced',  # automatyczne waÅ¼enie klas\n    random_state=42\n)\nbalanced_class_rf.fit(X_train, y_train)\n\n# RozwiÄ…zanie 2: bootstrap sampling\nbalanced_rf = RandomForestClassifier(\n    n_estimators=100,\n    max_samples=0.8,  # uÅ¼yj tylko 80% prÃ³bek do kaÅ¼dego drzewa\n    random_state=42\n)\nbalanced_rf.fit(X_train, y_train)\n\nprint(\"RozwiÄ…zania zostaÅ‚y zaimplementowane!\")\n\n\n\n\n\n\n\nPokaÅ¼ rozkÅ‚ad klas\n\n\n\n\n\n\n\nRozkÅ‚ad klas:\nPoraÅ¼ka: 16 (1.0%)\nSukces: 1584 (99.0%)\nRozwiÄ…zania zostaÅ‚y zaimplementowane!"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#real-world-przypadki-uÅ¼ycia",
    "href": "cheatsheets/04-random-forest.html#real-world-przypadki-uÅ¼ycia",
    "title": "Random Forest â€” las drzew decyzyjnych",
    "section": "ğŸŒ Real-world przypadki uÅ¼ycia",
    "text": "ğŸŒ Real-world przypadki uÅ¼ycia\n\nFinanse: Credit scoring, wykrywanie prania pieniÄ™dzy, trading algorytmiczny\nE-commerce: Systemy rekomendacji, dynamic pricing, churn prediction\nHealthcare: Diagnoza medyczna, drug discovery, analiza obrazÃ³w medycznych\nMarketing: Customer segmentation, campaign optimization, A/B testing\nCybersecurity: Intrusion detection, malware classification, anomaly detection\n\n\n\n\n\n\n\nğŸ’¡ Kiedy uÅ¼ywaÄ‡ Random Forest?\n\n\n\nâœ… UÅ»YJ GDY:\n\nPotrzebujesz wysokiej dokÅ‚adnoÅ›ci z interpretowalnym modelem\nMasz mixed features (liczbowe + kategoryczne)\nDane zawierajÄ… missing values (RF radzi sobie z nimi dobrze)\nChcesz feature importance bez complex feature engineering\nPotrzebujesz stabilnego modelu (maÅ‚a wariancja)\n\nâŒ NIE UÅ»YWAJ GDY:\n\nMasz bardzo duÅ¼e datasety (uÅ¼yj XGBoost/LightGBM)\nPotrzebujesz bardzo prostego modelu (uÅ¼yj Decision Tree)\nTarget jest continuous i potrzebujesz liniowej interpretacji (uÅ¼yj Linear Regression)\nMasz bardzo wysokowymiarowe dane (uÅ¼yj SVM lub Neural Networks)\n\n\n\nNastÄ™pna Å›ciÄ…gawka: K-Means Clustering - grupowanie bez etykiet! ğŸ¯"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html",
    "href": "cheatsheets/05-kmeans.html",
    "title": "K-Means â€” grupowanie klientÃ³w bez etykiet",
    "section": "",
    "text": "K-Means to algorytm uczenia nienadzorowanego, ktÃ³ry automatycznie grupuje podobne dane w klastry. Nie potrzebujesz etykiet - algorytm sam znajduje wzorce i dzieli dane na K grup.\n\n\n\n\n\n\nğŸ’¡ Intuicja\n\n\n\nWyobraÅº sobie imprezÄ™, gdzie ludzie sami grupujÄ… siÄ™ w krÄ™gi na podstawie wspÃ³lnych zainteresowaÅ„. K-Means dziaÅ‚a podobnie - znajduje â€œcentra grupâ€ i przydziela kaÅ¼dy punkt do najbliÅ¼szego centrum."
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#czym-jest-k-means",
    "href": "cheatsheets/05-kmeans.html#czym-jest-k-means",
    "title": "K-Means â€” grupowanie klientÃ³w bez etykiet",
    "section": "",
    "text": "K-Means to algorytm uczenia nienadzorowanego, ktÃ³ry automatycznie grupuje podobne dane w klastry. Nie potrzebujesz etykiet - algorytm sam znajduje wzorce i dzieli dane na K grup.\n\n\n\n\n\n\nğŸ’¡ Intuicja\n\n\n\nWyobraÅº sobie imprezÄ™, gdzie ludzie sami grupujÄ… siÄ™ w krÄ™gi na podstawie wspÃ³lnych zainteresowaÅ„. K-Means dziaÅ‚a podobnie - znajduje â€œcentra grupâ€ i przydziela kaÅ¼dy punkt do najbliÅ¼szego centrum."
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#praktyczny-przykÅ‚ad-segmentacja-klientÃ³w-sklepu",
    "href": "cheatsheets/05-kmeans.html#praktyczny-przykÅ‚ad-segmentacja-klientÃ³w-sklepu",
    "title": "K-Means â€” grupowanie klientÃ³w bez etykiet",
    "section": "ğŸ›ï¸ Praktyczny przykÅ‚ad: segmentacja klientÃ³w sklepu",
    "text": "ğŸ›ï¸ Praktyczny przykÅ‚ad: segmentacja klientÃ³w sklepu\nJak pogrupowaÄ‡ klientÃ³w do targetowanych kampanii marketingowych?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Tworzenie realistycznych danych klientÃ³w e-commerce\nnp.random.seed(42)\nn_customers = 1000\n\n# Generujemy rÃ³Å¼ne typy klientÃ³w z wyraÅºnymi wzorcami\ncustomer_types = np.random.choice(['premium', 'average', 'bargain', 'sporadic'], n_customers, \n                                 p=[0.15, 0.35, 0.35, 0.15])\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_customers),\n    'roczny_dochod': np.random.normal(50000, 20000, n_customers),\n    'wydatki_roczne': np.random.normal(25000, 15000, n_customers),\n    'liczba_zakupow': np.random.randint(1, 50, n_customers),\n    'sredni_koszt_zakupu': np.random.normal(200, 100, n_customers),\n    'lata_jako_klient': np.random.randint(0, 10, n_customers),\n    'typ_klienta': customer_types\n})\n\n# Realistyczne korelacje miÄ™dzy zmiennymi\nfor i, typ in enumerate(data['typ_klienta']):\n    if typ == 'premium':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(80000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(45000, 10000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(20, 50)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(400, 100)\n    elif typ == 'bargain':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(30000, 10000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(8000, 3000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(15, 40)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(80, 30)\n    elif typ == 'sporadic':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(45000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(5000, 2000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(1, 8)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(300, 150)\n\n# CzyÅ›Ä‡ dane - usuÅ„ wartoÅ›ci ujemne\ndata['roczny_dochod'] = np.maximum(data['roczny_dochod'], 15000)\ndata['wydatki_roczne'] = np.maximum(data['wydatki_roczne'], 1000)\ndata['sredni_koszt_zakupu'] = np.maximum(data['sredni_koszt_zakupu'], 20)\n\nprint(\"Statystyki klientÃ³w:\")\nprint(data[['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', 'sredni_koszt_zakupu']].describe())\nprint(f\"\\nRozkÅ‚ad typÃ³w klientÃ³w:\")\nprint(data['typ_klienta'].value_counts())\n\n\n\n\n\n\n\nPokaÅ¼ statystyki klientÃ³w\n\n\n\n\n\n\n\nStatystyki klientÃ³w:\n              wiek  roczny_dochod  wydatki_roczne  liczba_zakupow  \\\ncount  1000.000000    1000.000000     1000.000000      1000.00000   \nmean     43.359000   46988.933946    19816.768355        24.00700   \nstd      14.817276   22175.621844    17169.303009        13.41898   \nmin      18.000000   15000.000000     1000.000000         1.00000   \n25%      30.000000   29854.699281     6462.783773        15.00000   \n50%      43.000000   42533.564242    11041.836565        25.00000   \n75%      56.000000   62388.416902    33188.043332        35.00000   \nmax      69.000000  122502.434178    74817.934938        49.00000   \n\n       sredni_koszt_zakupu  \ncount          1000.000000  \nmean            203.883750  \nstd             143.706490  \nmin              20.000000  \n25%              83.839117  \n50%             162.389656  \n75%             303.607754  \nmax             763.483441  \n\nRozkÅ‚ad typÃ³w klientÃ³w:\ntyp_klienta\nbargain     344\naverage     337\npremium     166\nsporadic    153\nName: count, dtype: int64"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#budowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/05-kmeans.html#budowanie-modelu-krok-po-kroku",
    "title": "K-Means â€” grupowanie klientÃ³w bez etykiet",
    "section": "ğŸ”§ Budowanie modelu krok po kroku",
    "text": "ğŸ”§ Budowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Wybierz features do clusteringu (bez typ_klienta - to chcemy odkryÄ‡!)\nfeatures = ['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', \n           'sredni_koszt_zakupu', 'lata_jako_klient']\nX = data[features]\n\n# Standaryzacja - BARDZO WAÅ»NE w K-Means!\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(\"Przed standaryzacjÄ…:\")\nprint(X.describe())\n\nprint(\"\\nPo standaryzacji (pierwsze 5 wierszy):\")\nX_scaled_df = pd.DataFrame(X_scaled, columns=features)\nprint(X_scaled_df.head())\nprint(\"\\nÅšrednie po standaryzacji:\", X_scaled_df.mean().round(3))\nprint(\"Odchylenia po standaryzacji:\", X_scaled_df.std().round(3))\n\n\n\n\n\n\n\nPokaÅ¼ wyniki standaryzacji\n\n\n\n\n\n\n\nPrzed standaryzacjÄ…:\n              wiek  roczny_dochod  wydatki_roczne  liczba_zakupow  \\\ncount  1000.000000    1000.000000     1000.000000      1000.00000   \nmean     43.359000   46988.933946    19816.768355        24.00700   \nstd      14.817276   22175.621844    17169.303009        13.41898   \nmin      18.000000   15000.000000     1000.000000         1.00000   \n25%      30.000000   29854.699281     6462.783773        15.00000   \n50%      43.000000   42533.564242    11041.836565        25.00000   \n75%      56.000000   62388.416902    33188.043332        35.00000   \nmax      69.000000  122502.434178    74817.934938        49.00000   \n\n       sredni_koszt_zakupu  lata_jako_klient  \ncount          1000.000000       1000.000000  \nmean            203.883750          4.550000  \nstd             143.706490          2.914789  \nmin              20.000000          0.000000  \n25%              83.839117          2.000000  \n50%             162.389656          5.000000  \n75%             303.607754          7.000000  \nmax             763.483441          9.000000  \n\nPo standaryzacji (pierwsze 5 wierszy):\n       wiek  roczny_dochod  wydatki_roczne  liczba_zakupow  \\\n0  1.393733       0.346806        0.136732        0.894181   \n1 -0.969556       0.499661       -0.675331       -1.342577   \n2 -0.699466      -1.432630       -0.608789       -0.224198   \n3 -0.159286      -1.443249       -0.634140        0.223154   \n4 -0.496898      -1.443249       -0.966417        0.894181   \n\n   sredni_koszt_zakupu  lata_jako_klient  \n0             0.053442          1.184211  \n1             1.809673         -1.561786  \n2            -0.914825          0.154462  \n3            -1.101092         -1.561786  \n4            -0.652771         -0.875287  \n\nÅšrednie po standaryzacji: wiek                  -0.0\nroczny_dochod         -0.0\nwydatki_roczne         0.0\nliczba_zakupow        -0.0\nsredni_koszt_zakupu    0.0\nlata_jako_klient       0.0\ndtype: float64\nOdchylenia po standaryzacji: wiek                   1.001\nroczny_dochod          1.001\nwydatki_roczne         1.001\nliczba_zakupow         1.001\nsredni_koszt_zakupu    1.001\nlata_jako_klient       1.001\ndtype: float64\n\n\n\n\n\n\n\n2) Znajdowanie optymalnej liczby klastrÃ³w\n\n# Metoda Å‚okcia (Elbow Method)\ninertias = []\nsilhouette_scores = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n\n# ZnajdÅº optymalne K\nbest_k = K_range[np.argmax(silhouette_scores)]\nprint(f\"Optymalna liczba klastrÃ³w (Silhouette): {best_k}\")\n\nprint(\"\\nWyniki dla rÃ³Å¼nych K:\")\nfor k, inertia, sil_score in zip(K_range, inertias, silhouette_scores):\n    print(f\"K={k}: Inertia={inertia:.0f}, Silhouette={sil_score:.3f}\")\n\n\n\n\n\n\n\nPokaÅ¼ optymalne K\n\n\n\n\n\n\n\nOptymalna liczba klastrÃ³w (Silhouette): 2\n\nWyniki dla rÃ³Å¼nych K:\nK=2: Inertia=4416, Silhouette=0.281\nK=3: Inertia=3691, Silhouette=0.232\nK=4: Inertia=3296, Silhouette=0.200\nK=5: Inertia=3018, Silhouette=0.196\nK=6: Inertia=2828, Silhouette=0.195\nK=7: Inertia=2665, Silhouette=0.176\nK=8: Inertia=2526, Silhouette=0.177\nK=9: Inertia=2403, Silhouette=0.181\nK=10: Inertia=2286, Silhouette=0.176\n\n\n\n\n\n\n\n3) Trenowanie finalnego modelu\n\n# Trenuj model z optymalnym K\nfinal_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\ncluster_labels = final_kmeans.fit_predict(X_scaled)\n\n# Dodaj etykiety klastrÃ³w do danych\ndata['klaster'] = cluster_labels\n\nprint(f\"Model K-Means wytrenowany z K={best_k}\")\nprint(f\"Silhouette Score: {silhouette_score(X_scaled, cluster_labels):.3f}\")\n\n# Analiza klastrÃ³w\nprint(\"\\nRozmiary klastrÃ³w:\")\ncluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\nfor i, count in enumerate(cluster_counts):\n    print(f\"Klaster {i}: {count} klientÃ³w ({count/len(data)*100:.1f}%)\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki finalnego modelu\n\n\n\n\n\n\n\nModel K-Means wytrenowany z K=2\nSilhouette Score: 0.281\n\nRozmiary klastrÃ³w:\nKlaster 0: 275 klientÃ³w (27.5%)\nKlaster 1: 725 klientÃ³w (72.5%)"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#analiza-i-interpretacja-klastrÃ³w",
    "href": "cheatsheets/05-kmeans.html#analiza-i-interpretacja-klastrÃ³w",
    "title": "K-Means â€” grupowanie klientÃ³w bez etykiet",
    "section": "ğŸ“Š Analiza i interpretacja klastrÃ³w",
    "text": "ğŸ“Š Analiza i interpretacja klastrÃ³w\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Przygotuj dane (powtarzamy dla kompletnoÅ›ci)\nnp.random.seed(42)\nn_customers = 1000\n\ncustomer_types = np.random.choice(['premium', 'average', 'bargain', 'sporadic'], n_customers, \n                                 p=[0.15, 0.35, 0.35, 0.15])\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_customers),\n    'roczny_dochod': np.random.normal(50000, 20000, n_customers),\n    'wydatki_roczne': np.random.normal(25000, 15000, n_customers),\n    'liczba_zakupow': np.random.randint(1, 50, n_customers),\n    'sredni_koszt_zakupu': np.random.normal(200, 100, n_customers),\n    'lata_jako_klient': np.random.randint(0, 10, n_customers),\n    'typ_klienta': customer_types\n})\n\n# Popraw dane wedÅ‚ug typÃ³w\nfor i, typ in enumerate(data['typ_klienta']):\n    if typ == 'premium':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(80000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(45000, 10000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(20, 50)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(400, 100)\n    elif typ == 'bargain':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(30000, 10000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(8000, 3000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(15, 40)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(80, 30)\n    elif typ == 'sporadic':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(45000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(5000, 2000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(1, 8)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(300, 150)\n\ndata['roczny_dochod'] = np.maximum(data['roczny_dochod'], 15000)\ndata['wydatki_roczne'] = np.maximum(data['wydatki_roczne'], 1000)\ndata['sredni_koszt_zakupu'] = np.maximum(data['sredni_koszt_zakupu'], 20)\n\n# Przygotuj model\nfeatures = ['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', \n           'sredni_koszt_zakupu', 'lata_jako_klient']\nX = data[features]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# ZnajdÅº optymalne K\nsilhouette_scores = []\nK_range = range(2, 11)\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n\nbest_k = K_range[np.argmax(silhouette_scores)]\n\n# Trenuj finalny model\nfinal_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\ncluster_labels = final_kmeans.fit_predict(X_scaled)\ndata['klaster'] = cluster_labels\n\n# Analiza charakterystyk klastrÃ³w\ncluster_analysis = data.groupby('klaster')[features].mean()\n\nprint(\"CHARAKTERYSTYKI KLASTRÃ“W:\")\nprint(\"=\" * 50)\n\nfor cluster_id in range(best_k):\n    print(f\"\\nKLASTER {cluster_id}:\")\n    cluster_data = data[data['klaster'] == cluster_id]\n    \n    print(f\"Liczba klientÃ³w: {len(cluster_data)} ({len(cluster_data)/len(data)*100:.1f}%)\")\n    print(f\"Åšredni wiek: {cluster_data['wiek'].mean():.0f} lat\")\n    print(f\"Åšredni dochÃ³d: {cluster_data['roczny_dochod'].mean():.0f} zÅ‚\")\n    print(f\"Åšrednie wydatki: {cluster_data['wydatki_roczne'].mean():.0f} zÅ‚\")\n    print(f\"Åšrednia liczba zakupÃ³w: {cluster_data['liczba_zakupow'].mean():.0f}\")\n    print(f\"Åšredni koszt zakupu: {cluster_data['sredni_koszt_zakupu'].mean():.0f} zÅ‚\")\n    \n    # Nadaj nazwÄ™ klastrowi na podstawie charakterystyk\n    avg_income = cluster_data['roczny_dochod'].mean()\n    avg_spending = cluster_data['wydatki_roczne'].mean()\n    avg_frequency = cluster_data['liczba_zakupow'].mean()\n    \n    if avg_income &gt; 60000 and avg_spending &gt; 30000:\n        cluster_name = \"ğŸŒŸ PREMIUM CUSTOMERS\"\n    elif avg_frequency &lt; 10 and avg_spending &lt; 10000:\n        cluster_name = \"ğŸ˜´ SPORADYCZNI KLIENCI\"\n    elif avg_spending / avg_income &lt; 0.3 and avg_frequency &gt; 20:\n        cluster_name = \"ğŸ’° BARGAIN HUNTERS\"\n    else:\n        cluster_name = \"ğŸ”„ ÅšREDNI KLIENCI\"\n    \n    print(f\"Typ: {cluster_name}\")\n\n# Przygotuj wizualizacjÄ™\nplt.figure(figsize=(12, 8))\n\n# Wykres 1: DochÃ³d vs Wydatki\nplt.subplot(2, 2, 1)\nscatter = plt.scatter(data['roczny_dochod'], data['wydatki_roczne'], \n                     c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Roczny dochÃ³d')\nplt.ylabel('Wydatki roczne')\nplt.title('DochÃ³d vs Wydatki')\nplt.colorbar(scatter)\n\n# Wykres 2: Wiek vs Liczba zakupÃ³w\nplt.subplot(2, 2, 2)\nscatter2 = plt.scatter(data['wiek'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Wiek')\nplt.ylabel('Liczba zakupÃ³w')\nplt.title('Wiek vs Liczba zakupÃ³w')\nplt.colorbar(scatter2)\n\n# Wykres 3: Åšredni koszt vs Liczba zakupÃ³w\nplt.subplot(2, 2, 3)\nscatter3 = plt.scatter(data['sredni_koszt_zakupu'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Åšredni koszt zakupu')\nplt.ylabel('Liczba zakupÃ³w')\nplt.title('Koszt vs CzÄ™stotliwoÅ›Ä‡')\nplt.colorbar(scatter3)\n\n# Wykres 4: RozkÅ‚ad klastrÃ³w\nplt.subplot(2, 2, 4)\ncluster_counts = data['klaster'].value_counts().sort_index()\nplt.bar(range(len(cluster_counts)), cluster_counts.values, \n        color=plt.cm.viridis(np.linspace(0, 1, len(cluster_counts))))\nplt.xlabel('Klaster')\nplt.ylabel('Liczba klientÃ³w')\nplt.title('Rozmiary klastrÃ³w')\nplt.xticks(range(len(cluster_counts)))\n\nplt.tight_layout()\nplt.close()\n\n# PorÃ³wnanie z rzeczywistymi typami klientÃ³w\nif 'typ_klienta' in data.columns:\n    print(\"\\n\" + \"=\"*50)\n    print(\"PORÃ“WNANIE Z RZECZYWISTYMI TYPAMI:\")\n    comparison = pd.crosstab(data['klaster'], data['typ_klienta'])\n    print(comparison)\n\n# OdtwÃ³rz wykresy\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nscatter = plt.scatter(data['roczny_dochod'], data['wydatki_roczne'], \n                     c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Roczny dochÃ³d')\nplt.ylabel('Wydatki roczne')\nplt.title('DochÃ³d vs Wydatki')\nplt.colorbar(scatter)\n\nplt.subplot(2, 2, 2)\nscatter2 = plt.scatter(data['wiek'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Wiek')\nplt.ylabel('Liczba zakupÃ³w')\nplt.title('Wiek vs Liczba zakupÃ³w')\nplt.colorbar(scatter2)\n\nplt.subplot(2, 2, 3)\nscatter3 = plt.scatter(data['sredni_koszt_zakupu'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Åšredni koszt zakupu')\nplt.ylabel('Liczba zakupÃ³w')\nplt.title('Koszt vs CzÄ™stotliwoÅ›Ä‡')\nplt.colorbar(scatter3)\n\nplt.subplot(2, 2, 4)\ncluster_counts = data['klaster'].value_counts().sort_index()\nplt.bar(range(len(cluster_counts)), cluster_counts.values, \n        color=plt.cm.viridis(np.linspace(0, 1, len(cluster_counts))))\nplt.xlabel('Klaster')\nplt.ylabel('Liczba klientÃ³w')\nplt.title('Rozmiary klastrÃ³w')\nplt.xticks(range(len(cluster_counts)))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nPokaÅ¼ analizÄ™ klastrÃ³w i wizualizacje\n\n\n\n\n\n\n\nCHARAKTERYSTYKI KLASTRÃ“W:\n==================================================\n\nKLASTER 0:\nLiczba klientÃ³w: 275 (27.5%)\nÅšredni wiek: 43 lat\nÅšredni dochÃ³d: 71194 zÅ‚\nÅšrednie wydatki: 41779 zÅ‚\nÅšrednia liczba zakupÃ³w: 34\nÅšredni koszt zakupu: 332 zÅ‚\nTyp: ğŸŒŸ PREMIUM CUSTOMERS\n\nKLASTER 1:\nLiczba klientÃ³w: 725 (72.5%)\nÅšredni wiek: 44 lat\nÅšredni dochÃ³d: 37808 zÅ‚\nÅšrednie wydatki: 11486 zÅ‚\nÅšrednia liczba zakupÃ³w: 20\nÅšredni koszt zakupu: 155 zÅ‚\nTyp: ğŸ”„ ÅšREDNI KLIENCI\n\n==================================================\nPORÃ“WNANIE Z RZECZYWISTYMI TYPAMI:\ntyp_klienta  average  bargain  premium  sporadic\nklaster                                         \n0                109        0      166         0\n1                228      344        0       153\n\n\n\n\n\nAnaliza klastrÃ³w klientÃ³w"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#praktyczne-zastosowania-k-means",
    "href": "cheatsheets/05-kmeans.html#praktyczne-zastosowania-k-means",
    "title": "K-Means â€” grupowanie klientÃ³w bez etykiet",
    "section": "ğŸ¯ Praktyczne zastosowania K-Means",
    "text": "ğŸ¯ Praktyczne zastosowania K-Means\n\n1) Marketing - personalizacja kampanii\n\n# Strategia marketingowa na podstawie klastrÃ³w\ndef marketing_strategy(cluster_id, cluster_data):\n    avg_income = cluster_data['roczny_dochod'].mean()\n    avg_spending = cluster_data['wydatki_roczne'].mean()\n    avg_frequency = cluster_data['liczba_zakupow'].mean()\n    \n    if avg_income &gt; 60000 and avg_spending &gt; 30000:\n        return {\n            'segment': 'Premium Customers',\n            'strategy': 'Produkty luksusowe, VIP program, personal shopping',\n            'budget_allocation': '40%',\n            'channels': 'Email premium, personal calls, exclusive events'\n        }\n    elif avg_frequency &lt; 10 and avg_spending &lt; 10000:\n        return {\n            'segment': 'Sporadic Customers', \n            'strategy': 'Reaktywacja, oferty specjalne, przypomnienia',\n            'budget_allocation': '15%',\n            'channels': 'SMS, push notifications, retargeting ads'\n        }\n    elif avg_spending / avg_income &lt; 0.3 and avg_frequency &gt; 20:\n        return {\n            'segment': 'Bargain Hunters',\n            'strategy': 'Promocje, wyprzedaÅ¼e, programy lojalnoÅ›ciowe',\n            'budget_allocation': '25%', \n            'channels': 'Newsletter z promocjami, social media deals'\n        }\n    else:\n        return {\n            'segment': 'Average Customers',\n            'strategy': 'Standardowe produkty, cross-selling, up-selling',\n            'budget_allocation': '20%',\n            'channels': 'Email marketing, social media, display ads'\n        }\n\nprint(\"STRATEGIA MARKETINGOWA DLA KAÅ»DEGO KLASTRA:\")\nprint(\"=\" * 60)\n\nfor cluster_id in range(best_k):\n    cluster_data = data[data['klaster'] == cluster_id]\n    strategy = marketing_strategy(cluster_id, cluster_data)\n    \n    print(f\"\\nKLASTER {cluster_id}: {strategy['segment']}\")\n    print(f\"Strategia: {strategy['strategy']}\")\n    print(f\"BudÅ¼et: {strategy['budget_allocation']}\")\n    print(f\"KanaÅ‚y: {strategy['channels']}\")\n\n\n\n\n\n\n\nPokaÅ¼ strategiÄ™ marketingowÄ…\n\n\n\n\n\n\n\nSTRATEGIA MARKETINGOWA DLA KAÅ»DEGO KLASTRA:\n============================================================\n\nKLASTER 0: Premium Customers\nStrategia: Produkty luksusowe, VIP program, personal shopping\nBudÅ¼et: 40%\nKanaÅ‚y: Email premium, personal calls, exclusive events\n\nKLASTER 1: Average Customers\nStrategia: Standardowe produkty, cross-selling, up-selling\nBudÅ¼et: 20%\nKanaÅ‚y: Email marketing, social media, display ads\n\n\n\n\n\n\n\n2) Inne branÅ¼e\n\n# Healthcare - grupowanie pacjentÃ³w\nhealthcare_features = ['wiek', 'BMI', 'ciÅ›nienie', 'cholesterol', 'aktywnoÅ›Ä‡_fizyczna']\n# Wynik: programy profilaktyczne dostosowane do grup ryzyka\n\n# Finanse - portfolio management  \nfinance_features = ['dochÃ³d', 'tolerancja_ryzyka', 'horyzont_inwestycji', 'doÅ›wiadczenie']\n# Wynik: personalizowane porady inwestycyjne\n\n# Retail - optymalizacja sklepÃ³w\nretail_features = ['lokalizacja', 'demografia', 'konkurencja', 'ruch_pieszy']  \n# Wynik: optymalne rozmieszczenie produktÃ³w w rÃ³Å¼nych lokalizacjach"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#tuning-parametrÃ³w-k-means",
    "href": "cheatsheets/05-kmeans.html#tuning-parametrÃ³w-k-means",
    "title": "K-Means â€” grupowanie klientÃ³w bez etykiet",
    "section": "âš™ï¸ Tuning parametrÃ³w K-Means",
    "text": "âš™ï¸ Tuning parametrÃ³w K-Means\n\nfrom sklearn.cluster import KMeans\n\n# 1) RÃ³Å¼ne metody inicjalizacji\ninit_methods = ['k-means++', 'random']\nresults = {}\n\nfor init_method in init_methods:\n    kmeans = KMeans(n_clusters=best_k, init=init_method, n_init=10, random_state=42)\n    labels = kmeans.fit_predict(X_scaled)\n    sil_score = silhouette_score(X_scaled, labels)\n    results[init_method] = sil_score\n\nprint(\"PorÃ³wnanie metod inicjalizacji:\")\nfor method, score in results.items():\n    print(f\"{method}: Silhouette = {score:.3f}\")\n\n# 2) WpÅ‚yw liczby inicjalizacji\nn_init_values = [1, 5, 10, 20]\nfor n_init in n_init_values:\n    kmeans = KMeans(n_clusters=best_k, n_init=n_init, random_state=42)\n    start_time = pd.Timestamp.now()\n    kmeans.fit(X_scaled)\n    end_time = pd.Timestamp.now()\n    duration = (end_time - start_time).total_seconds()\n    sil_score = silhouette_score(X_scaled, kmeans.labels_)\n    print(f\"n_init={n_init}: Silhouette={sil_score:.3f}, Czas={duration:.2f}s\")\n\n# 3) RÃ³Å¼ne algorytmy\nalgorithms = ['lloyd', 'elkan']  # 'auto' wybiera automatycznie\nfor algorithm in algorithms:\n    try:\n        kmeans = KMeans(n_clusters=best_k, algorithm=algorithm, random_state=42)\n        start_time = pd.Timestamp.now()\n        kmeans.fit(X_scaled)\n        end_time = pd.Timestamp.now()\n        duration = (end_time - start_time).total_seconds()\n        print(f\"Algorytm {algorithm}: Czas={duration:.2f}s\")\n    except:\n        print(f\"Algorytm {algorithm}: NiedostÄ™pny w tej wersji\")\n\n\n\n\n\n\n\nPokaÅ¼ wyniki tuningu\n\n\n\n\n\n\n\nPorÃ³wnanie metod inicjalizacji:\nk-means++: Silhouette = 0.281\nrandom: Silhouette = 0.281\nn_init=1: Silhouette=0.280, Czas=0.00s\nn_init=5: Silhouette=0.281, Czas=0.00s\nn_init=10: Silhouette=0.281, Czas=0.01s\nn_init=20: Silhouette=0.281, Czas=0.03s"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#puÅ‚apki-i-rozwiÄ…zania",
    "href": "cheatsheets/05-kmeans.html#puÅ‚apki-i-rozwiÄ…zania",
    "title": "K-Means â€” grupowanie klientÃ³w bez etykiet",
    "section": "âš ï¸ PuÅ‚apki i rozwiÄ…zania",
    "text": "âš ï¸ PuÅ‚apki i rozwiÄ…zania\n\n1) Brak standaryzacji danych\n\n# Problem: rÃ³Å¼ne skale zmiennych\nprint(\"PROBLEM: Bez standaryzacji\")\nprint(\"DochÃ³d (tysiÄ…ce): 20-80\")  \nprint(\"Wiek (lata): 18-70\")\nprint(\"â†’ K-Means bÄ™dzie skupiaÄ‡ siÄ™ gÅ‚Ã³wnie na dochodzie!\")\n\n# Demonstracja\nkmeans_unscaled = KMeans(n_clusters=3, random_state=42)\nlabels_unscaled = kmeans_unscaled.fit_predict(X[['roczny_dochod', 'wiek']])\n\nkmeans_scaled = KMeans(n_clusters=3, random_state=42)  \nX_subset_scaled = scaler.fit_transform(X[['roczny_dochod', 'wiek']])\nlabels_scaled = kmeans_scaled.fit_predict(X_subset_scaled)\n\nsil_unscaled = silhouette_score(X[['roczny_dochod', 'wiek']], labels_unscaled)\nsil_scaled = silhouette_score(X_subset_scaled, labels_scaled)\n\nprint(f\"\\nWyniki:\")\nprint(f\"Bez standaryzacji: Silhouette = {sil_unscaled:.3f}\")\nprint(f\"Ze standaryzacjÄ…: Silhouette = {sil_scaled:.3f}\")\nprint(\"âœ… Standaryzacja ZAWSZE poprawia wyniki!\")\n\n\n\n\n\n\n\nPokaÅ¼ problem standaryzacji\n\n\n\n\n\n\n\n\nWyniki:\nBez standaryzacji: Silhouette = 0.562\nZe standaryzacjÄ…: Silhouette = 0.406\nâœ… Standaryzacja ZAWSZE poprawia wyniki!\n\n\n\n\n\n\n\n2) Outliers - wartoÅ›ci odstajÄ…ce\n\n# Problem: outliers zakÅ‚Ã³cajÄ… centroidy klastrÃ³w\nfrom sklearn.preprocessing import RobustScaler\n\n# Dodaj kilka outlierÃ³w\ndata_with_outliers = data.copy()\ndata_with_outliers.loc[0, 'roczny_dochod'] = 500000  # milioner!\ndata_with_outliers.loc[1, 'wydatki_roczne'] = 200000  # mega wydatki\n\nX_outliers = data_with_outliers[features]\n\n# PorÃ³wnaj rÃ³Å¼ne skalery\nscalers = {\n    'StandardScaler': StandardScaler(),\n    'RobustScaler': RobustScaler()  # odporny na outliers\n}\n\nprint(\"WpÅ‚yw outliers:\")\nfor scaler_name, scaler_obj in scalers.items():\n    X_scaled_comp = scaler_obj.fit_transform(X_outliers)\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    labels = kmeans.fit_predict(X_scaled_comp)\n    sil_score = silhouette_score(X_scaled_comp, labels)\n    print(f\"{scaler_name}: Silhouette = {sil_score:.3f}\")\n\nprint(\"\\nâœ… RobustScaler lepiej radzi sobie z outliers!\")\n\n\n\n\n\n\n\nPokaÅ¼ wpÅ‚yw outliers\n\n\n\n\n\n\n\nWpÅ‚yw outliers:\nStandardScaler: Silhouette = 0.193\nRobustScaler: Silhouette = 0.246\n\nâœ… RobustScaler lepiej radzi sobie z outliers!\n\n\n\n\n\n\n\n3) Curse of dimensionality\n\n# Problem: za duÅ¼o wymiarÃ³w\nfrom sklearn.decomposition import PCA\n\nprint(\"Problem wielu wymiarÃ³w:\")\ndimensions = [2, 5, 10, 20, 50]\n\nfor n_dims in dimensions:\n    if n_dims &lt;= X_scaled.shape[1]:\n        X_subset = X_scaled[:, :n_dims]\n    else:\n        # Dodaj sztuczne wymiary\n        extra_dims = np.random.randn(X_scaled.shape[0], n_dims - X_scaled.shape[1])\n        X_subset = np.hstack([X_scaled, extra_dims])\n    \n    kmeans = KMeans(n_clusters=3, random_state=42)\n    labels = kmeans.fit_predict(X_subset)\n    \n    if len(np.unique(labels)) &gt; 1:  # sprawdÅº czy sÄ… rÃ³Å¼ne klastry\n        sil_score = silhouette_score(X_subset, labels)\n        print(f\"{n_dims} wymiarÃ³w: Silhouette = {sil_score:.3f}\")\n    else:\n        print(f\"{n_dims} wymiarÃ³w: Wszystkie punkty w jednym klastrze!\")\n\nprint(\"\\nRozwiÄ…zanie: PCA dimensionality reduction\")\npca = PCA(n_components=0.95)  # zachowaj 95% wariancji\nX_pca = pca.fit_transform(X_scaled)\nprint(f\"Redukcja z {X_scaled.shape[1]} do {X_pca.shape[1]} wymiarÃ³w\")\n\nkmeans_pca = KMeans(n_clusters=3, random_state=42)\nlabels_pca = kmeans_pca.fit_predict(X_pca)\nsil_pca = silhouette_score(X_pca, labels_pca)\nprint(f\"Po PCA: Silhouette = {sil_pca:.3f}\")\n\n\n\n\n\n\n\nPokaÅ¼ problem wymiarowoÅ›ci\n\n\n\n\n\n\n\nProblem wielu wymiarÃ³w:\n2 wymiarÃ³w: Silhouette = 0.406\n5 wymiarÃ³w: Silhouette = 0.285\n10 wymiarÃ³w: Silhouette = 0.133\n20 wymiarÃ³w: Silhouette = 0.055\n50 wymiarÃ³w: Silhouette = 0.024\n\nRozwiÄ…zanie: PCA dimensionality reduction\nRedukcja z 6 do 6 wymiarÃ³w\nPo PCA: Silhouette = 0.233"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#real-world-przypadki-uÅ¼ycia",
    "href": "cheatsheets/05-kmeans.html#real-world-przypadki-uÅ¼ycia",
    "title": "K-Means â€” grupowanie klientÃ³w bez etykiet",
    "section": "ğŸŒ Real-world przypadki uÅ¼ycia",
    "text": "ğŸŒ Real-world przypadki uÅ¼ycia\n\nE-commerce: Segmentacja klientÃ³w, personalizacja, dynamic pricing\nMarketing: Customer personas, campaign optimization, market research\n\nFinance: Portfolio optimization, risk assessment, fraud detection\nHealthcare: Patient stratification, treatment personalization, drug discovery\nOperations: Supply chain optimization, demand forecasting, quality control\n\n\n\n\n\n\n\nğŸ’¡ Kiedy uÅ¼ywaÄ‡ K-Means?\n\n\n\nâœ… UÅ»YJ GDY:\n\nChcesz odkryÄ‡ ukryte grupy w danych bez etykiet\nDane majÄ… podobne gÄ™stoÅ›ci i sÄ… â€œokrÄ…gÅ‚eâ€ (sferyczne klastry)\nPotrzebujesz szybkiego i skalowalnego algorytmu\nWiesz w przybliÅ¼eniu ile moÅ¼e byÄ‡ grup (K)\nChcesz segmentowaÄ‡ klientÃ³w, produkty, rynki\n\nâŒ NIE UÅ»YWAJ GDY:\n\nKlastry majÄ… rÃ³Å¼ne rozmiary lub gÄ™stoÅ›ci (uÅ¼yj DBSCAN)\nKlastry majÄ… nieregularne ksztaÅ‚ty (uÅ¼yj Hierarchical Clustering)\nNie wiesz wcale ile moÅ¼e byÄ‡ grup (uÅ¼yj DBSCAN/HDBSCAN)\nMasz kategoryczne zmienne (uÅ¼yj K-Modes)\nDane majÄ… duÅ¼o outliers (uÅ¼yj DBSCAN)\n\n\n\nNastÄ™pna Å›ciÄ…gawka: Support Vector Machines - znajdowanie optymalnych granic! ğŸ¯"
  }
]