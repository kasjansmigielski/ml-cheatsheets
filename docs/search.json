[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Witaj w ≈õwiecie Machine Learning!",
    "section": "",
    "text": "Witaj w praktycznej kolekcji ≈õciƒÖg z Machine Learning! Znajdziesz tutaj algorytmy, teoriƒô i real-world przyk≈Çady przedstawione w przystƒôpny spos√≥b dla kursant√≥w Data Science. Ka≈ºda ≈õciƒÖgawka to kompaktowa porcja wiedzy gotowa do zastosowania.\n\n\n\nWyja≈õnienie algorytmu - jak dzia≈Ça i dlaczego\nPraktyczne przyk≈Çady kodu gotowe do skopiowania i uruchomienia\nReal-world zastosowania - kiedy i gdzie u≈ºyƒá\nKluczowe parametry - co mo≈ºna dostroiƒá i jak\nTipsy od praktyk√≥w - na co uwa≈ºaƒá w rzeczywistych projektach\n\n\n\n\nNajlepsza metoda nauki:\n\nPrzeczytaj teoriƒô - zrozum podstawy algorytmu\nPrzeanalizuj przyk≈Çad - zobacz jak teoria przek≈Çada siƒô na kod\n\nSkopiuj snippety - u≈ºyj przyciska üìã Copy w prawym g√≥rnym rogu\nTestuj w swoim ≈õrodowisku - Jupyter, Colab, VS Code\nEksperymentuj z parametrami - zmie≈Ñ warto≈õci i obserwuj efekty!\n\n\n\n\n\n\n\nüí° Pro tip dla kursant√≥w\n\n\n\nNie tylko kopiuj kod - zrozum co robi ka≈ºda linia. Spr√≥buj zmieniƒá dane wej≈õciowe, parametry algorytmu lub dodaƒá w≈Çasne modyfikacje. To najlepszy spos√≥b na naukƒô ML!\n\n\n\n\n\n\n\n\nWprowadzenie do ML - Czym jest ML i jakie sƒÖ g≈Ç√≥wne rodzaje algorytm√≥w\n\n\n\n\n\nLinear Regression - Predykcja warto≈õci liczbowych\n\n\n\n\n\nDecision Trees - Proste drzewa decyzyjne\n\n\n\n\n\n\n\nüî• Kolejne ≈õciƒÖgi w przygotowaniu!\n\n\n\n\nK-Means Clustering (segmentacja klient√≥w)\nRandom Forest (ensembles)\nLogistic Regression (klasyfikacja binarna)\nFeature Engineering (przygotowanie danych)\nModel Evaluation (metryki i walidacja)\n\n≈öled≈∫ aktualizacje!\n\n\n\n\n\n\n\nZacznij od podstaw - Wprowadzenie do ML\nWybierz algorytm kt√≥ry Ciƒô interesuje z menu ML Cheatsheets\n\nPrzeczytaj teoriƒô - zrozum jak dzia≈Ça algorytm\nSkopiuj kod (ikona üìã Copy w prawym g√≥rnym rogu bloku)\nTestuj w praktyce - uruchom w Jupyter/Colab i eksperymentuj!\n\n\nPowodzenia w nauce Machine Learning! üöÄü§ñ"
  },
  {
    "objectID": "index.html#co-znajdziesz-w-ka≈ºdej-≈õciƒÖgawce",
    "href": "index.html#co-znajdziesz-w-ka≈ºdej-≈õciƒÖgawce",
    "title": "Witaj w ≈õwiecie Machine Learning!",
    "section": "",
    "text": "Wyja≈õnienie algorytmu - jak dzia≈Ça i dlaczego\nPraktyczne przyk≈Çady kodu gotowe do skopiowania i uruchomienia\nReal-world zastosowania - kiedy i gdzie u≈ºyƒá\nKluczowe parametry - co mo≈ºna dostroiƒá i jak\nTipsy od praktyk√≥w - na co uwa≈ºaƒá w rzeczywistych projektach"
  },
  {
    "objectID": "index.html#jak-korzystaƒá-z-ml-cheatsheets",
    "href": "index.html#jak-korzystaƒá-z-ml-cheatsheets",
    "title": "Witaj w ≈õwiecie Machine Learning!",
    "section": "",
    "text": "Najlepsza metoda nauki:\n\nPrzeczytaj teoriƒô - zrozum podstawy algorytmu\nPrzeanalizuj przyk≈Çad - zobacz jak teoria przek≈Çada siƒô na kod\n\nSkopiuj snippety - u≈ºyj przyciska üìã Copy w prawym g√≥rnym rogu\nTestuj w swoim ≈õrodowisku - Jupyter, Colab, VS Code\nEksperymentuj z parametrami - zmie≈Ñ warto≈õci i obserwuj efekty!\n\n\n\n\n\n\n\nüí° Pro tip dla kursant√≥w\n\n\n\nNie tylko kopiuj kod - zrozum co robi ka≈ºda linia. Spr√≥buj zmieniƒá dane wej≈õciowe, parametry algorytmu lub dodaƒá w≈Çasne modyfikacje. To najlepszy spos√≥b na naukƒô ML!"
  },
  {
    "objectID": "index.html#dostƒôpne-≈õciƒÖgi-ml",
    "href": "index.html#dostƒôpne-≈õciƒÖgi-ml",
    "title": "Witaj w ≈õwiecie Machine Learning!",
    "section": "",
    "text": "Wprowadzenie do ML - Czym jest ML i jakie sƒÖ g≈Ç√≥wne rodzaje algorytm√≥w\n\n\n\n\n\nLinear Regression - Predykcja warto≈õci liczbowych\n\n\n\n\n\nDecision Trees - Proste drzewa decyzyjne\n\n\n\n\n\n\n\nüî• Kolejne ≈õciƒÖgi w przygotowaniu!\n\n\n\n\nK-Means Clustering (segmentacja klient√≥w)\nRandom Forest (ensembles)\nLogistic Regression (klasyfikacja binarna)\nFeature Engineering (przygotowanie danych)\nModel Evaluation (metryki i walidacja)\n\n≈öled≈∫ aktualizacje!"
  },
  {
    "objectID": "index.html#szybki-start",
    "href": "index.html#szybki-start",
    "title": "Witaj w ≈õwiecie Machine Learning!",
    "section": "",
    "text": "Zacznij od podstaw - Wprowadzenie do ML\nWybierz algorytm kt√≥ry Ciƒô interesuje z menu ML Cheatsheets\n\nPrzeczytaj teoriƒô - zrozum jak dzia≈Ça algorytm\nSkopiuj kod (ikona üìã Copy w prawym g√≥rnym rogu bloku)\nTestuj w praktyce - uruchom w Jupyter/Colab i eksperymentuj!\n\n\nPowodzenia w nauce Machine Learning! üöÄü§ñ"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html",
    "href": "cheatsheets/05-kmeans.html",
    "title": "K-Means ‚Äî grupowanie klient√≥w bez etykiet",
    "section": "",
    "text": "K-Means to algorytm uczenia nienadzorowanego, kt√≥ry automatycznie grupuje podobne dane w klastry. Nie potrzebujesz etykiet - algorytm sam znajduje wzorce i dzieli dane na K grup.\n\n\n\n\n\n\nüí° Intuicja\n\n\n\nWyobra≈∫ sobie imprezƒô, gdzie ludzie sami grupujƒÖ siƒô w krƒôgi na podstawie wsp√≥lnych zainteresowa≈Ñ. K-Means dzia≈Ça podobnie - znajduje ‚Äúcentra grup‚Äù i przydziela ka≈ºdy punkt do najbli≈ºszego centrum."
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#czym-jest-k-means",
    "href": "cheatsheets/05-kmeans.html#czym-jest-k-means",
    "title": "K-Means ‚Äî grupowanie klient√≥w bez etykiet",
    "section": "",
    "text": "K-Means to algorytm uczenia nienadzorowanego, kt√≥ry automatycznie grupuje podobne dane w klastry. Nie potrzebujesz etykiet - algorytm sam znajduje wzorce i dzieli dane na K grup.\n\n\n\n\n\n\nüí° Intuicja\n\n\n\nWyobra≈∫ sobie imprezƒô, gdzie ludzie sami grupujƒÖ siƒô w krƒôgi na podstawie wsp√≥lnych zainteresowa≈Ñ. K-Means dzia≈Ça podobnie - znajduje ‚Äúcentra grup‚Äù i przydziela ka≈ºdy punkt do najbli≈ºszego centrum."
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#praktyczny-przyk≈Çad-segmentacja-klient√≥w-sklepu",
    "href": "cheatsheets/05-kmeans.html#praktyczny-przyk≈Çad-segmentacja-klient√≥w-sklepu",
    "title": "K-Means ‚Äî grupowanie klient√≥w bez etykiet",
    "section": "üõçÔ∏è Praktyczny przyk≈Çad: segmentacja klient√≥w sklepu",
    "text": "üõçÔ∏è Praktyczny przyk≈Çad: segmentacja klient√≥w sklepu\nJak pogrupowaƒá klient√≥w do targetowanych kampanii marketingowych?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Tworzenie realistycznych danych klient√≥w e-commerce\nnp.random.seed(42)\nn_customers = 1000\n\n# Generujemy r√≥≈ºne typy klient√≥w z wyra≈∫nymi wzorcami\ncustomer_types = np.random.choice(['premium', 'average', 'bargain', 'sporadic'], n_customers, \n                                 p=[0.15, 0.35, 0.35, 0.15])\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_customers),\n    'roczny_dochod': np.random.normal(50000, 20000, n_customers),\n    'wydatki_roczne': np.random.normal(25000, 15000, n_customers),\n    'liczba_zakupow': np.random.randint(1, 50, n_customers),\n    'sredni_koszt_zakupu': np.random.normal(200, 100, n_customers),\n    'lata_jako_klient': np.random.randint(0, 10, n_customers),\n    'typ_klienta': customer_types\n})\n\n# Realistyczne korelacje miƒôdzy zmiennymi\nfor i, typ in enumerate(data['typ_klienta']):\n    if typ == 'premium':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(80000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(45000, 10000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(20, 50)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(400, 100)\n    elif typ == 'bargain':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(30000, 10000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(8000, 3000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(15, 40)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(80, 30)\n    elif typ == 'sporadic':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(45000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(5000, 2000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(1, 8)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(300, 150)\n\n# Czy≈õƒá dane - usu≈Ñ warto≈õci ujemne\ndata['roczny_dochod'] = np.maximum(data['roczny_dochod'], 15000)\ndata['wydatki_roczne'] = np.maximum(data['wydatki_roczne'], 1000)\ndata['sredni_koszt_zakupu'] = np.maximum(data['sredni_koszt_zakupu'], 20)\n\nprint(\"Statystyki klient√≥w:\")\nprint(data[['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', 'sredni_koszt_zakupu']].describe())\nprint(f\"\\nRozk≈Çad typ√≥w klient√≥w:\")\nprint(data['typ_klienta'].value_counts())\n\n\n\n\n\n\n\nPoka≈º statystyki klient√≥w\n\n\n\n\n\n\n\nStatystyki klient√≥w:\n              wiek  roczny_dochod  wydatki_roczne  liczba_zakupow  \\\ncount  1000.000000    1000.000000     1000.000000      1000.00000   \nmean     43.359000   46988.933946    19816.768355        24.00700   \nstd      14.817276   22175.621844    17169.303009        13.41898   \nmin      18.000000   15000.000000     1000.000000         1.00000   \n25%      30.000000   29854.699281     6462.783773        15.00000   \n50%      43.000000   42533.564242    11041.836565        25.00000   \n75%      56.000000   62388.416902    33188.043332        35.00000   \nmax      69.000000  122502.434178    74817.934938        49.00000   \n\n       sredni_koszt_zakupu  \ncount          1000.000000  \nmean            203.883750  \nstd             143.706490  \nmin              20.000000  \n25%              83.839117  \n50%             162.389656  \n75%             303.607754  \nmax             763.483441  \n\nRozk≈Çad typ√≥w klient√≥w:\ntyp_klienta\nbargain     344\naverage     337\npremium     166\nsporadic    153\nName: count, dtype: int64"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#budowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/05-kmeans.html#budowanie-modelu-krok-po-kroku",
    "title": "K-Means ‚Äî grupowanie klient√≥w bez etykiet",
    "section": "üîß Budowanie modelu krok po kroku",
    "text": "üîß Budowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Wybierz features do clusteringu (bez typ_klienta - to chcemy odkryƒá!)\nfeatures = ['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', \n           'sredni_koszt_zakupu', 'lata_jako_klient']\nX = data[features]\n\n# Standaryzacja - BARDZO WA≈ªNE w K-Means!\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(\"Przed standaryzacjƒÖ:\")\nprint(X.describe())\n\nprint(\"\\nPo standaryzacji (pierwsze 5 wierszy):\")\nX_scaled_df = pd.DataFrame(X_scaled, columns=features)\nprint(X_scaled_df.head())\nprint(\"\\n≈örednie po standaryzacji:\", X_scaled_df.mean().round(3))\nprint(\"Odchylenia po standaryzacji:\", X_scaled_df.std().round(3))\n\n\n\n\n\n\n\nPoka≈º wyniki standaryzacji\n\n\n\n\n\n\n\nPrzed standaryzacjƒÖ:\n              wiek  roczny_dochod  wydatki_roczne  liczba_zakupow  \\\ncount  1000.000000    1000.000000     1000.000000      1000.00000   \nmean     43.359000   46988.933946    19816.768355        24.00700   \nstd      14.817276   22175.621844    17169.303009        13.41898   \nmin      18.000000   15000.000000     1000.000000         1.00000   \n25%      30.000000   29854.699281     6462.783773        15.00000   \n50%      43.000000   42533.564242    11041.836565        25.00000   \n75%      56.000000   62388.416902    33188.043332        35.00000   \nmax      69.000000  122502.434178    74817.934938        49.00000   \n\n       sredni_koszt_zakupu  lata_jako_klient  \ncount          1000.000000       1000.000000  \nmean            203.883750          4.550000  \nstd             143.706490          2.914789  \nmin              20.000000          0.000000  \n25%              83.839117          2.000000  \n50%             162.389656          5.000000  \n75%             303.607754          7.000000  \nmax             763.483441          9.000000  \n\nPo standaryzacji (pierwsze 5 wierszy):\n       wiek  roczny_dochod  wydatki_roczne  liczba_zakupow  \\\n0  1.393733       0.346806        0.136732        0.894181   \n1 -0.969556       0.499661       -0.675331       -1.342577   \n2 -0.699466      -1.432630       -0.608789       -0.224198   \n3 -0.159286      -1.443249       -0.634140        0.223154   \n4 -0.496898      -1.443249       -0.966417        0.894181   \n\n   sredni_koszt_zakupu  lata_jako_klient  \n0             0.053442          1.184211  \n1             1.809673         -1.561786  \n2            -0.914825          0.154462  \n3            -1.101092         -1.561786  \n4            -0.652771         -0.875287  \n\n≈örednie po standaryzacji: wiek                  -0.0\nroczny_dochod         -0.0\nwydatki_roczne         0.0\nliczba_zakupow        -0.0\nsredni_koszt_zakupu    0.0\nlata_jako_klient       0.0\ndtype: float64\nOdchylenia po standaryzacji: wiek                   1.001\nroczny_dochod          1.001\nwydatki_roczne         1.001\nliczba_zakupow         1.001\nsredni_koszt_zakupu    1.001\nlata_jako_klient       1.001\ndtype: float64\n\n\n\n\n\n\n\n2) Znajdowanie optymalnej liczby klastr√≥w\n\n# Metoda ≈Çokcia (Elbow Method)\ninertias = []\nsilhouette_scores = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n\n# Znajd≈∫ optymalne K\nbest_k = K_range[np.argmax(silhouette_scores)]\nprint(f\"Optymalna liczba klastr√≥w (Silhouette): {best_k}\")\n\nprint(\"\\nWyniki dla r√≥≈ºnych K:\")\nfor k, inertia, sil_score in zip(K_range, inertias, silhouette_scores):\n    print(f\"K={k}: Inertia={inertia:.0f}, Silhouette={sil_score:.3f}\")\n\n\n\n\n\n\n\nPoka≈º optymalne K\n\n\n\n\n\n\n\nOptymalna liczba klastr√≥w (Silhouette): 2\n\nWyniki dla r√≥≈ºnych K:\nK=2: Inertia=4416, Silhouette=0.281\nK=3: Inertia=3691, Silhouette=0.232\nK=4: Inertia=3296, Silhouette=0.200\nK=5: Inertia=3018, Silhouette=0.196\nK=6: Inertia=2828, Silhouette=0.195\nK=7: Inertia=2665, Silhouette=0.176\nK=8: Inertia=2526, Silhouette=0.177\nK=9: Inertia=2403, Silhouette=0.181\nK=10: Inertia=2286, Silhouette=0.176\n\n\n\n\n\n\n\n3) Trenowanie finalnego modelu\n\n# Trenuj model z optymalnym K\nfinal_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\ncluster_labels = final_kmeans.fit_predict(X_scaled)\n\n# Dodaj etykiety klastr√≥w do danych\ndata['klaster'] = cluster_labels\n\nprint(f\"Model K-Means wytrenowany z K={best_k}\")\nprint(f\"Silhouette Score: {silhouette_score(X_scaled, cluster_labels):.3f}\")\n\n# Analiza klastr√≥w\nprint(\"\\nRozmiary klastr√≥w:\")\ncluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\nfor i, count in enumerate(cluster_counts):\n    print(f\"Klaster {i}: {count} klient√≥w ({count/len(data)*100:.1f}%)\")\n\n\n\n\n\n\n\nPoka≈º wyniki finalnego modelu\n\n\n\n\n\n\n\nModel K-Means wytrenowany z K=2\nSilhouette Score: 0.281\n\nRozmiary klastr√≥w:\nKlaster 0: 275 klient√≥w (27.5%)\nKlaster 1: 725 klient√≥w (72.5%)"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#analiza-i-interpretacja-klastr√≥w",
    "href": "cheatsheets/05-kmeans.html#analiza-i-interpretacja-klastr√≥w",
    "title": "K-Means ‚Äî grupowanie klient√≥w bez etykiet",
    "section": "üìä Analiza i interpretacja klastr√≥w",
    "text": "üìä Analiza i interpretacja klastr√≥w\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Przygotuj dane (powtarzamy dla kompletno≈õci)\nnp.random.seed(42)\nn_customers = 1000\n\ncustomer_types = np.random.choice(['premium', 'average', 'bargain', 'sporadic'], n_customers, \n                                 p=[0.15, 0.35, 0.35, 0.15])\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_customers),\n    'roczny_dochod': np.random.normal(50000, 20000, n_customers),\n    'wydatki_roczne': np.random.normal(25000, 15000, n_customers),\n    'liczba_zakupow': np.random.randint(1, 50, n_customers),\n    'sredni_koszt_zakupu': np.random.normal(200, 100, n_customers),\n    'lata_jako_klient': np.random.randint(0, 10, n_customers),\n    'typ_klienta': customer_types\n})\n\n# Popraw dane wed≈Çug typ√≥w\nfor i, typ in enumerate(data['typ_klienta']):\n    if typ == 'premium':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(80000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(45000, 10000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(20, 50)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(400, 100)\n    elif typ == 'bargain':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(30000, 10000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(8000, 3000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(15, 40)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(80, 30)\n    elif typ == 'sporadic':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(45000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(5000, 2000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(1, 8)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(300, 150)\n\ndata['roczny_dochod'] = np.maximum(data['roczny_dochod'], 15000)\ndata['wydatki_roczne'] = np.maximum(data['wydatki_roczne'], 1000)\ndata['sredni_koszt_zakupu'] = np.maximum(data['sredni_koszt_zakupu'], 20)\n\n# Przygotuj model\nfeatures = ['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', \n           'sredni_koszt_zakupu', 'lata_jako_klient']\nX = data[features]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Znajd≈∫ optymalne K\nsilhouette_scores = []\nK_range = range(2, 11)\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n\nbest_k = K_range[np.argmax(silhouette_scores)]\n\n# Trenuj finalny model\nfinal_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\ncluster_labels = final_kmeans.fit_predict(X_scaled)\ndata['klaster'] = cluster_labels\n\n# Analiza charakterystyk klastr√≥w\ncluster_analysis = data.groupby('klaster')[features].mean()\n\nprint(\"CHARAKTERYSTYKI KLASTR√ìW:\")\nprint(\"=\" * 50)\n\nfor cluster_id in range(best_k):\n    print(f\"\\nKLASTER {cluster_id}:\")\n    cluster_data = data[data['klaster'] == cluster_id]\n    \n    print(f\"Liczba klient√≥w: {len(cluster_data)} ({len(cluster_data)/len(data)*100:.1f}%)\")\n    print(f\"≈öredni wiek: {cluster_data['wiek'].mean():.0f} lat\")\n    print(f\"≈öredni doch√≥d: {cluster_data['roczny_dochod'].mean():.0f} z≈Ç\")\n    print(f\"≈örednie wydatki: {cluster_data['wydatki_roczne'].mean():.0f} z≈Ç\")\n    print(f\"≈örednia liczba zakup√≥w: {cluster_data['liczba_zakupow'].mean():.0f}\")\n    print(f\"≈öredni koszt zakupu: {cluster_data['sredni_koszt_zakupu'].mean():.0f} z≈Ç\")\n    \n    # Nadaj nazwƒô klastrowi na podstawie charakterystyk\n    avg_income = cluster_data['roczny_dochod'].mean()\n    avg_spending = cluster_data['wydatki_roczne'].mean()\n    avg_frequency = cluster_data['liczba_zakupow'].mean()\n    \n    if avg_income &gt; 60000 and avg_spending &gt; 30000:\n        cluster_name = \"üåü PREMIUM CUSTOMERS\"\n    elif avg_frequency &lt; 10 and avg_spending &lt; 10000:\n        cluster_name = \"üò¥ SPORADYCZNI KLIENCI\"\n    elif avg_spending / avg_income &lt; 0.3 and avg_frequency &gt; 20:\n        cluster_name = \"üí∞ BARGAIN HUNTERS\"\n    else:\n        cluster_name = \"üîÑ ≈öREDNI KLIENCI\"\n    \n    print(f\"Typ: {cluster_name}\")\n\n# Przygotuj wizualizacjƒô\nplt.figure(figsize=(12, 8))\n\n# Wykres 1: Doch√≥d vs Wydatki\nplt.subplot(2, 2, 1)\nscatter = plt.scatter(data['roczny_dochod'], data['wydatki_roczne'], \n                     c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Roczny doch√≥d')\nplt.ylabel('Wydatki roczne')\nplt.title('Doch√≥d vs Wydatki')\nplt.colorbar(scatter)\n\n# Wykres 2: Wiek vs Liczba zakup√≥w\nplt.subplot(2, 2, 2)\nscatter2 = plt.scatter(data['wiek'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Wiek')\nplt.ylabel('Liczba zakup√≥w')\nplt.title('Wiek vs Liczba zakup√≥w')\nplt.colorbar(scatter2)\n\n# Wykres 3: ≈öredni koszt vs Liczba zakup√≥w\nplt.subplot(2, 2, 3)\nscatter3 = plt.scatter(data['sredni_koszt_zakupu'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('≈öredni koszt zakupu')\nplt.ylabel('Liczba zakup√≥w')\nplt.title('Koszt vs Czƒôstotliwo≈õƒá')\nplt.colorbar(scatter3)\n\n# Wykres 4: Rozk≈Çad klastr√≥w\nplt.subplot(2, 2, 4)\ncluster_counts = data['klaster'].value_counts().sort_index()\nplt.bar(range(len(cluster_counts)), cluster_counts.values, \n        color=plt.cm.viridis(np.linspace(0, 1, len(cluster_counts))))\nplt.xlabel('Klaster')\nplt.ylabel('Liczba klient√≥w')\nplt.title('Rozmiary klastr√≥w')\nplt.xticks(range(len(cluster_counts)))\n\nplt.tight_layout()\nplt.close()\n\n# Por√≥wnanie z rzeczywistymi typami klient√≥w\nif 'typ_klienta' in data.columns:\n    print(\"\\n\" + \"=\"*50)\n    print(\"POR√ìWNANIE Z RZECZYWISTYMI TYPAMI:\")\n    comparison = pd.crosstab(data['klaster'], data['typ_klienta'])\n    print(comparison)\n\n# Odtw√≥rz wykresy\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nscatter = plt.scatter(data['roczny_dochod'], data['wydatki_roczne'], \n                     c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Roczny doch√≥d')\nplt.ylabel('Wydatki roczne')\nplt.title('Doch√≥d vs Wydatki')\nplt.colorbar(scatter)\n\nplt.subplot(2, 2, 2)\nscatter2 = plt.scatter(data['wiek'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Wiek')\nplt.ylabel('Liczba zakup√≥w')\nplt.title('Wiek vs Liczba zakup√≥w')\nplt.colorbar(scatter2)\n\nplt.subplot(2, 2, 3)\nscatter3 = plt.scatter(data['sredni_koszt_zakupu'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('≈öredni koszt zakupu')\nplt.ylabel('Liczba zakup√≥w')\nplt.title('Koszt vs Czƒôstotliwo≈õƒá')\nplt.colorbar(scatter3)\n\nplt.subplot(2, 2, 4)\ncluster_counts = data['klaster'].value_counts().sort_index()\nplt.bar(range(len(cluster_counts)), cluster_counts.values, \n        color=plt.cm.viridis(np.linspace(0, 1, len(cluster_counts))))\nplt.xlabel('Klaster')\nplt.ylabel('Liczba klient√≥w')\nplt.title('Rozmiary klastr√≥w')\nplt.xticks(range(len(cluster_counts)))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nPoka≈º analizƒô klastr√≥w i wizualizacje\n\n\n\n\n\n\n\nCHARAKTERYSTYKI KLASTR√ìW:\n==================================================\n\nKLASTER 0:\nLiczba klient√≥w: 275 (27.5%)\n≈öredni wiek: 43 lat\n≈öredni doch√≥d: 71194 z≈Ç\n≈örednie wydatki: 41779 z≈Ç\n≈örednia liczba zakup√≥w: 34\n≈öredni koszt zakupu: 332 z≈Ç\nTyp: üåü PREMIUM CUSTOMERS\n\nKLASTER 1:\nLiczba klient√≥w: 725 (72.5%)\n≈öredni wiek: 44 lat\n≈öredni doch√≥d: 37808 z≈Ç\n≈örednie wydatki: 11486 z≈Ç\n≈örednia liczba zakup√≥w: 20\n≈öredni koszt zakupu: 155 z≈Ç\nTyp: üîÑ ≈öREDNI KLIENCI\n\n==================================================\nPOR√ìWNANIE Z RZECZYWISTYMI TYPAMI:\ntyp_klienta  average  bargain  premium  sporadic\nklaster                                         \n0                109        0      166         0\n1                228      344        0       153\n\n\n\n\n\nAnaliza klastr√≥w klient√≥w"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#praktyczne-zastosowania-k-means",
    "href": "cheatsheets/05-kmeans.html#praktyczne-zastosowania-k-means",
    "title": "K-Means ‚Äî grupowanie klient√≥w bez etykiet",
    "section": "üéØ Praktyczne zastosowania K-Means",
    "text": "üéØ Praktyczne zastosowania K-Means\n\n1) Marketing - personalizacja kampanii\n\n# Strategia marketingowa na podstawie klastr√≥w\ndef marketing_strategy(cluster_id, cluster_data):\n    avg_income = cluster_data['roczny_dochod'].mean()\n    avg_spending = cluster_data['wydatki_roczne'].mean()\n    avg_frequency = cluster_data['liczba_zakupow'].mean()\n    \n    if avg_income &gt; 60000 and avg_spending &gt; 30000:\n        return {\n            'segment': 'Premium Customers',\n            'strategy': 'Produkty luksusowe, VIP program, personal shopping',\n            'budget_allocation': '40%',\n            'channels': 'Email premium, personal calls, exclusive events'\n        }\n    elif avg_frequency &lt; 10 and avg_spending &lt; 10000:\n        return {\n            'segment': 'Sporadic Customers', \n            'strategy': 'Reaktywacja, oferty specjalne, przypomnienia',\n            'budget_allocation': '15%',\n            'channels': 'SMS, push notifications, retargeting ads'\n        }\n    elif avg_spending / avg_income &lt; 0.3 and avg_frequency &gt; 20:\n        return {\n            'segment': 'Bargain Hunters',\n            'strategy': 'Promocje, wyprzeda≈ºe, programy lojalno≈õciowe',\n            'budget_allocation': '25%', \n            'channels': 'Newsletter z promocjami, social media deals'\n        }\n    else:\n        return {\n            'segment': 'Average Customers',\n            'strategy': 'Standardowe produkty, cross-selling, up-selling',\n            'budget_allocation': '20%',\n            'channels': 'Email marketing, social media, display ads'\n        }\n\nprint(\"STRATEGIA MARKETINGOWA DLA KA≈ªDEGO KLASTRA:\")\nprint(\"=\" * 60)\n\nfor cluster_id in range(best_k):\n    cluster_data = data[data['klaster'] == cluster_id]\n    strategy = marketing_strategy(cluster_id, cluster_data)\n    \n    print(f\"\\nKLASTER {cluster_id}: {strategy['segment']}\")\n    print(f\"Strategia: {strategy['strategy']}\")\n    print(f\"Bud≈ºet: {strategy['budget_allocation']}\")\n    print(f\"Kana≈Çy: {strategy['channels']}\")\n\n\n\n\n\n\n\nPoka≈º strategiƒô marketingowƒÖ\n\n\n\n\n\n\n\nSTRATEGIA MARKETINGOWA DLA KA≈ªDEGO KLASTRA:\n============================================================\n\nKLASTER 0: Premium Customers\nStrategia: Produkty luksusowe, VIP program, personal shopping\nBud≈ºet: 40%\nKana≈Çy: Email premium, personal calls, exclusive events\n\nKLASTER 1: Average Customers\nStrategia: Standardowe produkty, cross-selling, up-selling\nBud≈ºet: 20%\nKana≈Çy: Email marketing, social media, display ads\n\n\n\n\n\n\n\n2) Inne bran≈ºe\n\n# Healthcare - grupowanie pacjent√≥w\nhealthcare_features = ['wiek', 'BMI', 'ci≈õnienie', 'cholesterol', 'aktywno≈õƒá_fizyczna']\n# Wynik: programy profilaktyczne dostosowane do grup ryzyka\n\n# Finanse - portfolio management  \nfinance_features = ['doch√≥d', 'tolerancja_ryzyka', 'horyzont_inwestycji', 'do≈õwiadczenie']\n# Wynik: personalizowane porady inwestycyjne\n\n# Retail - optymalizacja sklep√≥w\nretail_features = ['lokalizacja', 'demografia', 'konkurencja', 'ruch_pieszy']  \n# Wynik: optymalne rozmieszczenie produkt√≥w w r√≥≈ºnych lokalizacjach"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#tuning-parametr√≥w-k-means",
    "href": "cheatsheets/05-kmeans.html#tuning-parametr√≥w-k-means",
    "title": "K-Means ‚Äî grupowanie klient√≥w bez etykiet",
    "section": "‚öôÔ∏è Tuning parametr√≥w K-Means",
    "text": "‚öôÔ∏è Tuning parametr√≥w K-Means\n\nfrom sklearn.cluster import KMeans\n\n# 1) R√≥≈ºne metody inicjalizacji\ninit_methods = ['k-means++', 'random']\nresults = {}\n\nfor init_method in init_methods:\n    kmeans = KMeans(n_clusters=best_k, init=init_method, n_init=10, random_state=42)\n    labels = kmeans.fit_predict(X_scaled)\n    sil_score = silhouette_score(X_scaled, labels)\n    results[init_method] = sil_score\n\nprint(\"Por√≥wnanie metod inicjalizacji:\")\nfor method, score in results.items():\n    print(f\"{method}: Silhouette = {score:.3f}\")\n\n# 2) Wp≈Çyw liczby inicjalizacji\nn_init_values = [1, 5, 10, 20]\nfor n_init in n_init_values:\n    kmeans = KMeans(n_clusters=best_k, n_init=n_init, random_state=42)\n    start_time = pd.Timestamp.now()\n    kmeans.fit(X_scaled)\n    end_time = pd.Timestamp.now()\n    duration = (end_time - start_time).total_seconds()\n    sil_score = silhouette_score(X_scaled, kmeans.labels_)\n    print(f\"n_init={n_init}: Silhouette={sil_score:.3f}, Czas={duration:.2f}s\")\n\n# 3) R√≥≈ºne algorytmy\nalgorithms = ['lloyd', 'elkan']  # 'auto' wybiera automatycznie\nfor algorithm in algorithms:\n    try:\n        kmeans = KMeans(n_clusters=best_k, algorithm=algorithm, random_state=42)\n        start_time = pd.Timestamp.now()\n        kmeans.fit(X_scaled)\n        end_time = pd.Timestamp.now()\n        duration = (end_time - start_time).total_seconds()\n        print(f\"Algorytm {algorithm}: Czas={duration:.2f}s\")\n    except:\n        print(f\"Algorytm {algorithm}: Niedostƒôpny w tej wersji\")\n\n\n\n\n\n\n\nPoka≈º wyniki tuningu\n\n\n\n\n\n\n\nPor√≥wnanie metod inicjalizacji:\nk-means++: Silhouette = 0.281\nrandom: Silhouette = 0.281\nn_init=1: Silhouette=0.280, Czas=0.00s\nn_init=5: Silhouette=0.281, Czas=0.00s\nn_init=10: Silhouette=0.281, Czas=0.01s\nn_init=20: Silhouette=0.281, Czas=0.03s"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#pu≈Çapki-i-rozwiƒÖzania",
    "href": "cheatsheets/05-kmeans.html#pu≈Çapki-i-rozwiƒÖzania",
    "title": "K-Means ‚Äî grupowanie klient√≥w bez etykiet",
    "section": "‚ö†Ô∏è Pu≈Çapki i rozwiƒÖzania",
    "text": "‚ö†Ô∏è Pu≈Çapki i rozwiƒÖzania\n\n1) Brak standaryzacji danych\n\n# Problem: r√≥≈ºne skale zmiennych\nprint(\"PROBLEM: Bez standaryzacji\")\nprint(\"Doch√≥d (tysiƒÖce): 20-80\")  \nprint(\"Wiek (lata): 18-70\")\nprint(\"‚Üí K-Means bƒôdzie skupiaƒá siƒô g≈Ç√≥wnie na dochodzie!\")\n\n# Demonstracja\nkmeans_unscaled = KMeans(n_clusters=3, random_state=42)\nlabels_unscaled = kmeans_unscaled.fit_predict(X[['roczny_dochod', 'wiek']])\n\nkmeans_scaled = KMeans(n_clusters=3, random_state=42)  \nX_subset_scaled = scaler.fit_transform(X[['roczny_dochod', 'wiek']])\nlabels_scaled = kmeans_scaled.fit_predict(X_subset_scaled)\n\nsil_unscaled = silhouette_score(X[['roczny_dochod', 'wiek']], labels_unscaled)\nsil_scaled = silhouette_score(X_subset_scaled, labels_scaled)\n\nprint(f\"\\nWyniki:\")\nprint(f\"Bez standaryzacji: Silhouette = {sil_unscaled:.3f}\")\nprint(f\"Ze standaryzacjƒÖ: Silhouette = {sil_scaled:.3f}\")\nprint(\"‚úÖ Standaryzacja ZAWSZE poprawia wyniki!\")\n\n\n\n\n\n\n\nPoka≈º problem standaryzacji\n\n\n\n\n\n\n\n\nWyniki:\nBez standaryzacji: Silhouette = 0.562\nZe standaryzacjƒÖ: Silhouette = 0.406\n‚úÖ Standaryzacja ZAWSZE poprawia wyniki!\n\n\n\n\n\n\n\n2) Outliers - warto≈õci odstajƒÖce\n\n# Problem: outliers zak≈Ç√≥cajƒÖ centroidy klastr√≥w\nfrom sklearn.preprocessing import RobustScaler\n\n# Dodaj kilka outlier√≥w\ndata_with_outliers = data.copy()\ndata_with_outliers.loc[0, 'roczny_dochod'] = 500000  # milioner!\ndata_with_outliers.loc[1, 'wydatki_roczne'] = 200000  # mega wydatki\n\nX_outliers = data_with_outliers[features]\n\n# Por√≥wnaj r√≥≈ºne skalery\nscalers = {\n    'StandardScaler': StandardScaler(),\n    'RobustScaler': RobustScaler()  # odporny na outliers\n}\n\nprint(\"Wp≈Çyw outliers:\")\nfor scaler_name, scaler_obj in scalers.items():\n    X_scaled_comp = scaler_obj.fit_transform(X_outliers)\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    labels = kmeans.fit_predict(X_scaled_comp)\n    sil_score = silhouette_score(X_scaled_comp, labels)\n    print(f\"{scaler_name}: Silhouette = {sil_score:.3f}\")\n\nprint(\"\\n‚úÖ RobustScaler lepiej radzi sobie z outliers!\")\n\n\n\n\n\n\n\nPoka≈º wp≈Çyw outliers\n\n\n\n\n\n\n\nWp≈Çyw outliers:\nStandardScaler: Silhouette = 0.193\nRobustScaler: Silhouette = 0.246\n\n‚úÖ RobustScaler lepiej radzi sobie z outliers!\n\n\n\n\n\n\n\n3) Curse of dimensionality\n\n# Problem: za du≈ºo wymiar√≥w\nfrom sklearn.decomposition import PCA\n\nprint(\"Problem wielu wymiar√≥w:\")\ndimensions = [2, 5, 10, 20, 50]\n\nfor n_dims in dimensions:\n    if n_dims &lt;= X_scaled.shape[1]:\n        X_subset = X_scaled[:, :n_dims]\n    else:\n        # Dodaj sztuczne wymiary\n        extra_dims = np.random.randn(X_scaled.shape[0], n_dims - X_scaled.shape[1])\n        X_subset = np.hstack([X_scaled, extra_dims])\n    \n    kmeans = KMeans(n_clusters=3, random_state=42)\n    labels = kmeans.fit_predict(X_subset)\n    \n    if len(np.unique(labels)) &gt; 1:  # sprawd≈∫ czy sƒÖ r√≥≈ºne klastry\n        sil_score = silhouette_score(X_subset, labels)\n        print(f\"{n_dims} wymiar√≥w: Silhouette = {sil_score:.3f}\")\n    else:\n        print(f\"{n_dims} wymiar√≥w: Wszystkie punkty w jednym klastrze!\")\n\nprint(\"\\nRozwiƒÖzanie: PCA dimensionality reduction\")\npca = PCA(n_components=0.95)  # zachowaj 95% wariancji\nX_pca = pca.fit_transform(X_scaled)\nprint(f\"Redukcja z {X_scaled.shape[1]} do {X_pca.shape[1]} wymiar√≥w\")\n\nkmeans_pca = KMeans(n_clusters=3, random_state=42)\nlabels_pca = kmeans_pca.fit_predict(X_pca)\nsil_pca = silhouette_score(X_pca, labels_pca)\nprint(f\"Po PCA: Silhouette = {sil_pca:.3f}\")\n\n\n\n\n\n\n\nPoka≈º problem wymiarowo≈õci\n\n\n\n\n\n\n\nProblem wielu wymiar√≥w:\n2 wymiar√≥w: Silhouette = 0.406\n5 wymiar√≥w: Silhouette = 0.285\n10 wymiar√≥w: Silhouette = 0.133\n20 wymiar√≥w: Silhouette = 0.055\n50 wymiar√≥w: Silhouette = 0.024\n\nRozwiƒÖzanie: PCA dimensionality reduction\nRedukcja z 6 do 6 wymiar√≥w\nPo PCA: Silhouette = 0.233"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#real-world-przypadki-u≈ºycia",
    "href": "cheatsheets/05-kmeans.html#real-world-przypadki-u≈ºycia",
    "title": "K-Means ‚Äî grupowanie klient√≥w bez etykiet",
    "section": "üåç Real-world przypadki u≈ºycia",
    "text": "üåç Real-world przypadki u≈ºycia\n\nE-commerce: Segmentacja klient√≥w, personalizacja, dynamic pricing\nMarketing: Customer personas, campaign optimization, market research\n\nFinance: Portfolio optimization, risk assessment, fraud detection\nHealthcare: Patient stratification, treatment personalization, drug discovery\nOperations: Supply chain optimization, demand forecasting, quality control\n\n\n\n\n\n\n\nüí° Kiedy u≈ºywaƒá K-Means?\n\n\n\n‚úÖ U≈ªYJ GDY:\n\nChcesz odkryƒá ukryte grupy w danych bez etykiet\nDane majƒÖ podobne gƒôsto≈õci i sƒÖ ‚ÄúokrƒÖg≈Çe‚Äù (sferyczne klastry)\nPotrzebujesz szybkiego i skalowalnego algorytmu\nWiesz w przybli≈ºeniu ile mo≈ºe byƒá grup (K)\nChcesz segmentowaƒá klient√≥w, produkty, rynki\n\n‚ùå NIE U≈ªYWAJ GDY:\n\nKlastry majƒÖ r√≥≈ºne rozmiary lub gƒôsto≈õci (u≈ºyj DBSCAN)\nKlastry majƒÖ nieregularne kszta≈Çty (u≈ºyj Hierarchical Clustering)\nNie wiesz wcale ile mo≈ºe byƒá grup (u≈ºyj DBSCAN/HDBSCAN)\nMasz kategoryczne zmienne (u≈ºyj K-Modes)\nDane majƒÖ du≈ºo outliers (u≈ºyj DBSCAN)\n\n\n\nNastƒôpna ≈õciƒÖgawka: Support Vector Machines - znajdowanie optymalnych granic! üéØ"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html",
    "href": "cheatsheets/03-decision-trees.html",
    "title": "Decision Trees ‚Äî proste drzewa decyzyjne",
    "section": "",
    "text": "Decision Trees to jeden z najbardziej intuicyjnych algorytm√≥w ML, kt√≥ry podejmuje decyzje jak cz≈Çowiek - zadajƒÖc szereg pyta≈Ñ typu ‚Äútak/nie‚Äù i na podstawie odpowiedzi klasyfikuje lub przewiduje warto≈õci.\n\n\n\n\n\n\nüí° Intuicja\n\n\n\nWyobra≈∫ sobie lekarza, kt√≥ry diagnozuje chorobƒô: ‚ÄúCzy ma gorƒÖczkƒô? TAK ‚Üí Czy boli gard≈Ço? TAK ‚Üí Czy ma katar? NIE ‚Üí Prawdopodobnie angina‚Äù. Decision Tree dzia≈Ça dok≈Çadnie tak samo!"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#czym-sƒÖ-decision-trees",
    "href": "cheatsheets/03-decision-trees.html#czym-sƒÖ-decision-trees",
    "title": "Decision Trees ‚Äî proste drzewa decyzyjne",
    "section": "",
    "text": "Decision Trees to jeden z najbardziej intuicyjnych algorytm√≥w ML, kt√≥ry podejmuje decyzje jak cz≈Çowiek - zadajƒÖc szereg pyta≈Ñ typu ‚Äútak/nie‚Äù i na podstawie odpowiedzi klasyfikuje lub przewiduje warto≈õci.\n\n\n\n\n\n\nüí° Intuicja\n\n\n\nWyobra≈∫ sobie lekarza, kt√≥ry diagnozuje chorobƒô: ‚ÄúCzy ma gorƒÖczkƒô? TAK ‚Üí Czy boli gard≈Ço? TAK ‚Üí Czy ma katar? NIE ‚Üí Prawdopodobnie angina‚Äù. Decision Tree dzia≈Ça dok≈Çadnie tak samo!"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#praktyczny-przyk≈Çad-klasyfikacja-klient√≥w-banku",
    "href": "cheatsheets/03-decision-trees.html#praktyczny-przyk≈Çad-klasyfikacja-klient√≥w-banku",
    "title": "Decision Trees ‚Äî proste drzewa decyzyjne",
    "section": "üéØ Praktyczny przyk≈Çad: klasyfikacja klient√≥w banku",
    "text": "üéØ Praktyczny przyk≈Çad: klasyfikacja klient√≥w banku\nCzy klient we≈∫mie kredyt na podstawie jego profilu?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Tworzenie realistycznych danych klient√≥w banku\nnp.random.seed(42)\nn_clients = 2000\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_clients),\n    'dochod': np.random.lognormal(10, 0.5, n_clients),  # log-normal dla dochod√≥w\n    'historia_kredytowa': np.random.choice(['dobra', '≈õrednia', 's≈Çaba'], n_clients, p=[0.6, 0.3, 0.1]),\n    'zatrudnienie': np.random.choice(['sta≈Çe', 'tymczasowe', 'bezrobotny'], n_clients, p=[0.7, 0.25, 0.05]),\n    'oszczednosci': np.random.exponential(20000, n_clients),\n    'liczba_dzieci': np.random.randint(0, 4, n_clients)\n})\n\n# Realistyczna logika decyzyjna banku\ndef czy_kredyt(row):\n    score = 0\n    \n    # Wiek (30-50 lat = najlepsi klienci)\n    if 30 &lt;= row['wiek'] &lt;= 50:\n        score += 2\n    elif row['wiek'] &lt; 25 or row['wiek'] &gt; 60:\n        score -= 1\n    \n    # Doch√≥d\n    if row['dochod'] &gt; 50000:\n        score += 3\n    elif row['dochod'] &gt; 30000:\n        score += 1\n    else:\n        score -= 2\n    \n    # Historia kredytowa\n    if row['historia_kredytowa'] == 'dobra':\n        score += 2\n    elif row['historia_kredytowa'] == 's≈Çaba':\n        score -= 3\n    \n    # Zatrudnienie\n    if row['zatrudnienie'] == 'sta≈Çe':\n        score += 2\n    elif row['zatrudnienie'] == 'bezrobotny':\n        score -= 4\n    \n    # Oszczƒôdno≈õci\n    if row['oszczednosci'] &gt; 50000:\n        score += 1\n    \n    # Dzieci (wiƒôcej dzieci = wiƒôksze ryzyko)\n    score -= row['liczba_dzieci'] * 0.5\n    \n    # Ko≈Ñcowa decyzja z odrobinƒÖ losowo≈õci\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 0.5)))\n    return probability &gt; 0.5\n\ndata['kredyt_przyznany'] = data.apply(czy_kredyt, axis=1)\n\nprint(\"Statystyki klient√≥w:\")\nprint(data.describe())\nprint(f\"\\nOdsetek przyznanych kredyt√≥w: {data['kredyt_przyznany'].mean():.1%}\")\n\n\n\n\n\n\n\nPoka≈º statystyki i odsetki przyznanych kredyt√≥w\n\n\n\n\n\n\n\nStatystyki klient√≥w:\n              wiek         dochod   oszczednosci  liczba_dzieci\ncount  2000.000000    2000.000000    2000.000000    2000.000000\nmean     43.805500   25726.361013   19950.270464       1.523000\nstd      14.929203   14090.406882   20299.940268       1.130535\nmin      18.000000    4047.221838      27.128881       0.000000\n25%      31.000000   15922.761710    5923.833818       1.000000\n50%      44.000000   22782.391426   13844.909016       2.000000\n75%      56.000000   31922.003675   27566.091135       3.000000\nmax      69.000000  112447.929023  165194.684774       3.000000\n\nOdsetek przyznanych kredyt√≥w: 63.8%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#budowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/03-decision-trees.html#budowanie-modelu-krok-po-kroku",
    "title": "Decision Trees ‚Äî proste drzewa decyzyjne",
    "section": "üîß Budowanie modelu krok po kroku",
    "text": "üîß Budowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Encoding kategorycznych zmiennych\nfrom sklearn.preprocessing import LabelEncoder\n\ndata_encoded = data.copy()\nle_historia = LabelEncoder()\nle_zatrudnienie = LabelEncoder()\n\ndata_encoded['historia_kredytowa_num'] = le_historia.fit_transform(data['historia_kredytowa'])\ndata_encoded['zatrudnienie_num'] = le_zatrudnienie.fit_transform(data['zatrudnienie'])\n\n# Features do modelu\nfeature_columns = ['wiek', 'dochod', 'historia_kredytowa_num', 'zatrudnienie_num', 'oszczednosci', 'liczba_dzieci']\nX = data_encoded[feature_columns]\ny = data_encoded['kredyt_przyznany']\n\n# Podzia≈Ç train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Dane treningowe: {len(X_train)} klient√≥w\")\nprint(f\"Dane testowe: {len(X_test)} klient√≥w\")\n\n\n\n\n\n\n\nPoka≈º wyniki dane treningowe i testowe\n\n\n\n\n\n\n\nDane treningowe: 1600 klient√≥w\nDane testowe: 400 klient√≥w\n\n\n\n\n\n\n\n2) Trenowanie Decision Tree\n\n# Tworzenie modelu z ograniczeniami (≈ºeby nie by≈Ç za g≈Çƒôboki)\ndt_model = DecisionTreeClassifier(\n    max_depth=5,        # maksymalna g≈Çƒôboko≈õƒá drzewa\n    min_samples_split=50,  # min. pr√≥bek do podzia≈Çu wƒôz≈Ça\n    min_samples_leaf=20,   # min. pr√≥bek w li≈õciu\n    random_state=42\n)\n\n# Trenowanie\ndt_model.fit(X_train, y_train)\n\nprint(\"Model wytrenowany!\")\nprint(f\"G≈Çƒôboko≈õƒá drzewa: {dt_model.tree_.max_depth}\")\nprint(f\"Liczba li≈õci: {dt_model.tree_.n_leaves}\")\n\n\n\n\n\n\n\nPoka≈º parametry wytrenowanego modelu\n\n\n\n\n\n\n\nModel wytrenowany!\nG≈Çƒôboko≈õƒá drzewa: 5\nLiczba li≈õci: 20\n\n\n\n\n\n\n\n3) Ewaluacja modelu\n\n# Predykcje\ny_pred = dt_model.predict(X_test)\ny_pred_proba = dt_model.predict_proba(X_test)[:, 1]\n\n# Metryki\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nDok≈Çadno≈õƒá modelu: {accuracy:.1%}\")\n\n# Szczeg√≥≈Çowy raport\nprint(\"\\nRaport klasyfikacji:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:    Nie    Tak\")\nprint(f\"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}\")\n\n\n\n\n\n\n\nPoka≈º wyniki\n\n\n\n\n\n\n\n\nConfusion Matrix:\nPrzewidywane:    Nie    Tak\nRzeczywiste Nie: 122    34\nRzeczywiste Tak:  27   217"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#wizualizacja-drzewa-decyzyjnego",
    "href": "cheatsheets/03-decision-trees.html#wizualizacja-drzewa-decyzyjnego",
    "title": "Decision Trees ‚Äî proste drzewa decyzyjne",
    "section": "üé® Wizualizacja drzewa decyzyjnego",
    "text": "üé® Wizualizacja drzewa decyzyjnego\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Tworzenie przyk≈Çadowych danych (kompletny przyk≈Çad)\nnp.random.seed(42)\nn_clients = 2000\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_clients),\n    'dochod': np.random.lognormal(10, 0.5, n_clients),\n    'historia_kredytowa': np.random.choice(['dobra', '≈õrednia', 's≈Çaba'], n_clients, p=[0.6, 0.3, 0.1]),\n    'zatrudnienie': np.random.choice(['sta≈Çe', 'tymczasowe', 'bezrobotny'], n_clients, p=[0.7, 0.25, 0.05]),\n    'oszczednosci': np.random.exponential(20000, n_clients),\n    'liczba_dzieci': np.random.randint(0, 4, n_clients)\n})\n\n# Prosta logika przyznawania kredytu\ndef czy_kredyt(row):\n    score = 0\n    if 30 &lt;= row['wiek'] &lt;= 50:\n        score += 2\n    if row['dochod'] &gt; 50000:\n        score += 3\n    if row['historia_kredytowa'] == 'dobra':\n        score += 2\n    if row['zatrudnienie'] == 'sta≈Çe':\n        score += 2\n    if row['oszczednosci'] &gt; 50000:\n        score += 1\n    score -= row['liczba_dzieci'] * 0.5\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 0.5)))\n    return probability &gt; 0.5\n\ndata['kredyt_przyznany'] = data.apply(czy_kredyt, axis=1)\n\n# Encoding i model\ndata_encoded = data.copy()\nle_historia = LabelEncoder()\nle_zatrudnienie = LabelEncoder()\ndata_encoded['historia_kredytowa_num'] = le_historia.fit_transform(data['historia_kredytowa'])\ndata_encoded['zatrudnienie_num'] = le_zatrudnienie.fit_transform(data['zatrudnienie'])\n\nfeature_columns = ['wiek', 'dochod', 'historia_kredytowa_num', 'zatrudnienie_num', 'oszczednosci', 'liczba_dzieci']\nX = data_encoded[feature_columns]\ny = data_encoded['kredyt_przyznany']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie drzewa\ndt_model = DecisionTreeClassifier(max_depth=5, min_samples_split=50, min_samples_leaf=20, random_state=42)\ndt_model.fit(X_train, y_train)\n\n# Ewaluacja\ny_pred = dt_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\n# Wa≈ºno≈õƒá features\nfeature_names = ['wiek', 'doch√≥d', 'historia_kred.', 'zatrudnienie', 'oszczƒôdno≈õci', 'liczba_dzieci']\nimportance = dt_model.feature_importances_\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importance\n}).sort_values('importance', ascending=False)\n\n# Przygotuj wykres drzewa\nplt.figure(figsize=(20, 10))\ntree.plot_tree(dt_model, \n               feature_names=feature_names,\n               class_names=['Nie', 'Tak'],\n               filled=True,\n               rounded=True,\n               fontsize=10)\nplt.title(\"Drzewo decyzyjne - Przyznanie kredytu\")\nplt.close()\n\nprint(f\"Wyniki modelu Decision Tree:\")\nprint(f\"G≈Çƒôboko≈õƒá drzewa: {dt_model.tree_.max_depth}\")\nprint(f\"Liczba li≈õci: {dt_model.tree_.n_leaves}\")\nprint(f\"Dok≈Çadno≈õƒá modelu: {accuracy:.1%}\")\n\nprint(f\"\\nStatystyki danych:\")\nprint(f\"Dane treningowe: {len(X_train)} klient√≥w\")\nprint(f\"Dane testowe: {len(X_test)} klient√≥w\")\nprint(f\"Odsetek przyznanych kredyt√≥w: {data['kredyt_przyznany'].mean():.1%}\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:    Nie    Tak\")\nprint(f\"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}\")\n\nprint(\"\\nWa≈ºno≈õƒá cech:\")\nfor _, row in feature_importance.iterrows():\n    print(f\"{row['feature']}: {row['importance']:.3f}\")\n\n# Odtw√≥rz wykres drzewa\nplt.figure(figsize=(20, 10))\ntree.plot_tree(dt_model, \n               feature_names=feature_names,\n               class_names=['Nie', 'Tak'],\n               filled=True,\n               rounded=True,\n               fontsize=10)\nplt.title(\"Drzewo decyzyjne - Przyznanie kredytu\")\nplt.show()\n\n\n\n\n\n\n\nPoka≈º wyniki i drzewo decyzyjne\n\n\n\n\n\n\n\nWyniki modelu Decision Tree:\nG≈Çƒôboko≈õƒá drzewa: 5\nLiczba li≈õci: 13\nDok≈Çadno≈õƒá modelu: 95.0%\n\nStatystyki danych:\nDane treningowe: 1600 klient√≥w\nDane testowe: 400 klient√≥w\nOdsetek przyznanych kredyt√≥w: 93.2%\n\nConfusion Matrix:\nPrzewidywane:    Nie    Tak\nRzeczywiste Nie:  19    11\nRzeczywiste Tak:   9   361\n\nWa≈ºno≈õƒá cech:\nzatrudnienie: 0.441\nwiek: 0.317\nhistoria_kred.: 0.178\nliczba_dzieci: 0.055\noszczƒôdno≈õci: 0.008\ndoch√≥d: 0.001\n\n\n\n\n\nWizualizacja drzewa decyzyjnego dla kredyt√≥w"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#interpretacja-decyzji-dla-konkretnego-klienta",
    "href": "cheatsheets/03-decision-trees.html#interpretacja-decyzji-dla-konkretnego-klienta",
    "title": "Decision Trees ‚Äî proste drzewa decyzyjne",
    "section": "üîç Interpretacja decyzji dla konkretnego klienta",
    "text": "üîç Interpretacja decyzji dla konkretnego klienta\n\n# Funkcja do interpretacji ≈õcie≈ºki decyzyjnej\ndef explain_decision(model, X_sample, feature_names):\n    # Pobierz ≈õcie≈ºkƒô w drzewie\n    leaf_id = model.decision_path(X_sample.reshape(1, -1)).toarray()[0]\n    feature = model.tree_.feature\n    threshold = model.tree_.threshold\n    \n    print(\"≈öcie≈ºka decyzyjna:\")\n    for node_id in range(len(leaf_id)):\n        if leaf_id[node_id] == 1:  # je≈õli wƒôze≈Ç jest na ≈õcie≈ºce\n            if feature[node_id] != -2:  # je≈õli nie jest li≈õciem\n                feature_name = feature_names[feature[node_id]]\n                threshold_val = threshold[node_id]\n                feature_val = X_sample[feature[node_id]]\n                \n                if feature_val &lt;= threshold_val:\n                    condition = \"&lt;=\"\n                else:\n                    condition = \"&gt;\"\n                    \n                print(f\"  {feature_name} ({feature_val:.0f}) {condition} {threshold_val:.0f}\")\n\n# Przyk≈Çad dla konkretnego klienta\nsample_client = X_test.iloc[0]\nprediction = dt_model.predict([sample_client])[0]\nprobability = dt_model.predict_proba([sample_client])[0]\n\nprint(f\"Klient testowy:\")\nprint(f\"Wiek: {sample_client['wiek']}\")\nprint(f\"Doch√≥d: {sample_client['dochod']:.0f}\")\nprint(f\"Oszczƒôdno≈õci: {sample_client['oszczednosci']:.0f}\")\nprint(f\"\\nDecyzja: {'KREDYT PRZYZNANY' if prediction else 'KREDYT ODRZUCONY'}\")\nprint(f\"Prawdopodobie≈Ñstwo: {probability[1]:.1%}\")\n\nexplain_decision(dt_model, sample_client.values, feature_names)\n\n\n\n\n\n\n\nPoka≈º dane klienta testowego\n\n\n\n\n\n\n\nKlient testowy:\nWiek: 56.0\nDoch√≥d: 7927\nOszczƒôdno≈õci: 32270\n\nDecyzja: KREDYT ODRZUCONY\nPrawdopodobie≈Ñstwo: 3.0%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#r√≥≈ºne-zastosowania-decision-trees",
    "href": "cheatsheets/03-decision-trees.html#r√≥≈ºne-zastosowania-decision-trees",
    "title": "Decision Trees ‚Äî proste drzewa decyzyjne",
    "section": "üéØ R√≥≈ºne zastosowania Decision Trees",
    "text": "üéØ R√≥≈ºne zastosowania Decision Trees\n\n1) Medyczna diagnoza\n\n# Przyk≈Çad klasyfikacji ryzyka chor√≥b serca\nmedical_features = ['wiek', 'cholesterol', 'ci≈õnienie', 'BMI', 'pali_papierosy']\n# Target: 'ryzyko_chor√≥b_serca' (wysokie/niskie)\n\nmedical_tree = DecisionTreeClassifier(max_depth=4)\n# Model automatycznie znajdzie progi: \"Je≈õli cholesterol &gt; 240 I BMI &gt; 30 ORAZ wiek &gt; 50...\"\n\n\n\n2) Marketing - segmentacja klient√≥w\n\n# Przewidywanie czy klient kupi produkt premium\nmarketing_features = ['doch√≥d_roczny', 'wiek', 'wykszta≈Çcenie', 'poprzednie_zakupy']\n# Target: 'kupi_premium' (tak/nie)\n\nmarketing_tree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=100)\n# Wynik: jasne regu≈Çy marketingowe do targetowania reklam\n\n\n\n3) HR - decyzje o zatrudnieniu\n\n# Przewidywanie sukcesu kandydata w rekrutacji\nhr_features = ['do≈õwiadczenie_lat', 'wykszta≈Çcenie', 'wynik_test√≥w', 'referencje']\n# Target: 'zatrudniony' (tak/nie)\n\nhr_tree = DecisionTreeClassifier(max_depth=5)\n# Wynik: automatyczne zasady rekrutacyjne"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#tuning-parametr√≥w",
    "href": "cheatsheets/03-decision-trees.html#tuning-parametr√≥w",
    "title": "Decision Trees ‚Äî proste drzewa decyzyjne",
    "section": "‚öôÔ∏è Tuning parametr√≥w",
    "text": "‚öôÔ∏è Tuning parametr√≥w\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Najwa≈ºniejsze parametry do tuningu\nparam_grid = {\n    'max_depth': [3, 5, 7, 10, None],\n    'min_samples_split': [20, 50, 100],\n    'min_samples_leaf': [10, 20, 50],\n    'criterion': ['gini', 'entropy']\n}\n\n# Grid search z cross-validation\ngrid_search = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(\"Najlepsze parametry:\")\nprint(grid_search.best_params_)\nprint(f\"Najlepsza dok≈Çadno≈õƒá CV: {grid_search.best_score_:.1%}\")\n\n# Model z najlepszymi parametrami\nbest_model = grid_search.best_estimator_\nbest_accuracy = accuracy_score(y_test, best_model.predict(X_test))\nprint(f\"Dok≈Çadno≈õƒá na test set: {best_accuracy:.1%}\")\n\n\n\n\n\n\n\nPoka≈º najlepsze parametry\n\n\n\n\n\n\n\nNajlepsze parametry:\n{'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 20}\nNajlepsza dok≈Çadno≈õƒá CV: 96.5%\nDok≈Çadno≈õƒá na test set: 95.0%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#problemy-i-rozwiƒÖzania",
    "href": "cheatsheets/03-decision-trees.html#problemy-i-rozwiƒÖzania",
    "title": "Decision Trees ‚Äî proste drzewa decyzyjne",
    "section": "‚ö†Ô∏è Problemy i rozwiƒÖzania",
    "text": "‚ö†Ô∏è Problemy i rozwiƒÖzania\n\n1) Overfitting - drzewo za g≈Çƒôbokie\n\n# Problem: drzewo \"pamiƒôta\" dane treningowe\noverfitted_tree = DecisionTreeClassifier()  # bez ogranicze≈Ñ!\noverfitted_tree.fit(X_train, y_train)\n\ntrain_acc = accuracy_score(y_train, overfitted_tree.predict(X_train))\ntest_acc = accuracy_score(y_test, overfitted_tree.predict(X_test))\n\nprint(f\"Overfitted model:\")\nprint(f\"Train accuracy: {train_acc:.1%}\")\nprint(f\"Test accuracy: {test_acc:.1%}\")\nprint(f\"R√≥≈ºnica: {train_acc - test_acc:.1%} - to overfitting!\")\n\n# RozwiƒÖzanie: ograniczenia\npruned_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=20)\npruned_tree.fit(X_train, y_train)\n\ntrain_acc_pruned = accuracy_score(y_train, pruned_tree.predict(X_train))\ntest_acc_pruned = accuracy_score(y_test, pruned_tree.predict(X_test))\n\nprint(f\"\\nPruned model:\")\nprint(f\"Train accuracy: {train_acc_pruned:.1%}\")\nprint(f\"Test accuracy: {test_acc_pruned:.1%}\")\nprint(f\"R√≥≈ºnica: {train_acc_pruned - test_acc_pruned:.1%} - znacznie lepiej!\")\n\n\n\n\n\n\n\nPoka≈º wyniki\n\n\n\n\n\n\n\nOverfitted model:\nTrain accuracy: 100.0%\nTest accuracy: 94.8%\nR√≥≈ºnica: 5.2% - to overfitting!\n\nPruned model:\nTrain accuracy: 96.7%\nTest accuracy: 95.0%\nR√≥≈ºnica: 1.7% - znacznie lepiej!\n\n\n\n\n\n\n\n2) Brak stabilno≈õci - ma≈Çe zmiany = r√≥≈ºne drzewa\n\n# Problem demonstracji\nresults = []\nfor i in range(10):\n    # R√≥≈ºne random_state = r√≥≈ºne drzewa\n    tree_test = DecisionTreeClassifier(random_state=i, max_depth=5)\n    tree_test.fit(X_train, y_train)\n    acc = accuracy_score(y_test, tree_test.predict(X_test))\n    results.append(acc)\n\nprint(f\"Dok≈Çadno≈õƒá r√≥≈ºnych drzew: {min(results):.1%} - {max(results):.1%}\")\nprint(f\"Rozrzut: {max(results) - min(results):.1%}\")\nprint(\"RozwiƒÖzanie: Random Forest (nastƒôpna ≈õciƒÖgawka!)\")\n\n\n\n\n\n\n\nPoka≈º wyniki\n\n\n\n\n\n\n\nDok≈Çadno≈õƒá r√≥≈ºnych drzew: 96.2% - 96.2%\nRozrzut: 0.0%\nRozwiƒÖzanie: Random Forest (nastƒôpna ≈õciƒÖgawka!)"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#real-world-przypadki-u≈ºycia",
    "href": "cheatsheets/03-decision-trees.html#real-world-przypadki-u≈ºycia",
    "title": "Decision Trees ‚Äî proste drzewa decyzyjne",
    "section": "üåç Real-world przypadki u≈ºycia",
    "text": "üåç Real-world przypadki u≈ºycia\n\nBankowo≈õƒá: Ocena ryzyka kredytowego, wykrywanie fraud√≥w\nMedycyna: Systemy wspomagania diagnostyki, triage pacjent√≥w\n\nE-commerce: Rekomendacje produkt√≥w, ustalanie cen dynamicznych\nHR: Automatyzacja proces√≥w rekrutacyjnych\nMarketing: Segmentacja klient√≥w, personalizacja oferowania\n\n\n\n\n\n\n\nüí° Kiedy u≈ºywaƒá Decision Trees?\n\n\n\n‚úÖ U≈ªYJ GDY:\n\nPotrzebujesz interpretowalnego modelu\nDane majƒÖ kategoryczne zmienne\n\nChcesz zrozumieƒá ‚Äúdlaczego‚Äù model podjƒÖ≈Ç decyzjƒô\nMasz nieliniowe zale≈ºno≈õci w danych\nBrak√≥w w danych nie trzeba impute‚Äôowaƒá\n\n‚ùå NIE U≈ªYWAJ GDY:\n\nPotrzebujesz najwy≈ºszej dok≈Çadno≈õci (u≈ºyj Random Forest/XGBoost)\nMasz bardzo g≈Çƒôbokie wzorce w danych (u≈ºyj Neural Networks)\nDane sƒÖ bardzo ha≈Ça≈õliwe\nPrzewidujesz warto≈õci ciƒÖg≈Çe (lepiej Linear Regression)\n\n\n\nNastƒôpna ≈õciƒÖgawka: Random Forest - zesp√≥≈Ç drzew (wkr√≥tce)! üå≤üå≤üå≤"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html",
    "href": "cheatsheets/01-intro-ml.html",
    "title": "Wprowadzenie do Machine Learning ‚Äî fundament",
    "section": "",
    "text": "Machine Learning (ML) to dziedzina informatyki, kt√≥ra pozwala komputerom uczyƒá siƒô i podejmowaƒá decyzje na podstawie danych, bez konieczno≈õci programowania ka≈ºdej regu≈Çy z g√≥ry.\n\n\n\n\n\n\nüí° Intuicja\n\n\n\nZamiast pisaƒá kod ‚Äúje≈õli temperatura &gt; 25¬∞C, to bƒôdzie s≈Çonecznie‚Äù, ML pozwala algorytmowi samemu odkryƒá te zale≈ºno≈õci z historycznych danych pogodowych."
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#czym-jest-machine-learning",
    "href": "cheatsheets/01-intro-ml.html#czym-jest-machine-learning",
    "title": "Wprowadzenie do Machine Learning ‚Äî fundament",
    "section": "",
    "text": "Machine Learning (ML) to dziedzina informatyki, kt√≥ra pozwala komputerom uczyƒá siƒô i podejmowaƒá decyzje na podstawie danych, bez konieczno≈õci programowania ka≈ºdej regu≈Çy z g√≥ry.\n\n\n\n\n\n\nüí° Intuicja\n\n\n\nZamiast pisaƒá kod ‚Äúje≈õli temperatura &gt; 25¬∞C, to bƒôdzie s≈Çonecznie‚Äù, ML pozwala algorytmowi samemu odkryƒá te zale≈ºno≈õci z historycznych danych pogodowych."
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#g≈Ç√≥wne-rodzaje-ml",
    "href": "cheatsheets/01-intro-ml.html#g≈Ç√≥wne-rodzaje-ml",
    "title": "Wprowadzenie do Machine Learning ‚Äî fundament",
    "section": "üìä G≈Ç√≥wne rodzaje ML",
    "text": "üìä G≈Ç√≥wne rodzaje ML\n\n1) Supervised Learning (Uczenie nadzorowane)\nMamy dane + znamy prawid≈Çowe odpowiedzi\n\n# Przyk≈Çad: predykcja ceny domu\n# Dane wej≈õciowe: powierzchnia, lokalizacja, rok budowy\n# Cel: przewidzieƒá cenƒô\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Przyk≈Çadowe dane\ndata = pd.DataFrame({\n    'powierzchnia': [50, 75, 100, 120, 150],\n    'rok_budowy': [1990, 2000, 2010, 2015, 2020],\n    'cena': [300000, 400000, 550000, 650000, 800000]  # znamy prawdziwe ceny!\n})\n\n# Trenowanie modelu\nmodel = LinearRegression()\nX = data[['powierzchnia', 'rok_budowy']]\ny = data['cena']\nmodel.fit(X, y)\n\n# Predykcja dla nowego domu\nnowy_dom = [[90, 2005]]\nprzewidywana_cena = model.predict(nowy_dom)\n\n# Zapisz wyniki do zmiennych dla expandera\ndane_treningowe = data.to_string()\nwynik_predykcji = f\"Przewidywana cena: {przewidywana_cena[0]:.0f} z≈Ç\"\n\nprint(\"Dane treningowe:\")\nprint(dane_treningowe)\nprint(f\"\\nPredykcja dla domu 90m¬≤, rok 2005:\")\nprint(wynik_predykcji)\n\n\n\n\n\n\n\nPoka≈º wyniki predykcji cen\n\n\n\n\n\n\n\nDane treningowe:\n   powierzchnia  rok_budowy    cena\n0            50        1990  300000\n1            75        2000  400000\n2           100        2010  550000\n3           120        2015  650000\n4           150        2020  800000\n\nPredykcja dla domu 90m¬≤, rok 2005:\nPrzewidywana cena: 493819 z≈Ç\n\n\n\n\n\nReal-world zastosowania:\n\nPredykcja cen akcji/nieruchomo≈õci\nDiagnoza medyczna (klasyfikacja chor√≥b)\nFiltrowanie spamu w emailach\nRozpoznawanie mowy/obraz√≥w\n\n\n\n\n2) Unsupervised Learning (Uczenie nienadzorowane)\nMamy tylko dane, szukamy ukrytych wzorc√≥w\n\n# Przyk≈Çad: segmentacja klient√≥w sklepu\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Dane klient√≥w: wiek i wydatki miesiƒôczne\nklienci = pd.DataFrame({\n    'wiek': [25, 30, 35, 22, 28, 45, 50, 55, 60, 65],\n    'wydatki': [2000, 2500, 3000, 1800, 2200, 4000, 4500, 3500, 3000, 2800]\n})\n\n# Grupowanie klient√≥w w 3 segmenty\nkmeans = KMeans(n_clusters=3, random_state=42)\nklienci['segment'] = kmeans.fit_predict(klienci[['wiek', 'wydatki']])\n\n# Przygotuj wyniki do wy≈õwietlenia\ndane_klientow = klienci.head().to_string()\nsegmenty_info = []\nfor i in range(3):\n    segment = klienci[klienci['segment'] == i]\n    segmenty_info.append(f\"Segment {i}: ≈õredni wiek {segment['wiek'].mean():.0f}, ≈õrednie wydatki {segment['wydatki'].mean():.0f}\")\n\nprzyklad_predykcji = kmeans.predict([[30, 2500]])[0]\n\nprint(\"Dane klient√≥w:\")\nprint(dane_klientow)\nprint(\"\\nSegmenty klient√≥w:\")\nfor info in segmenty_info:\n    print(info)\nprint(f\"\\nPrzyk≈Çad: klient 30 lat, wydaje 2500z≈Ç -&gt; segment {przyklad_predykcji}\")\n\n\n\n\n\n\n\nPoka≈º wyniki segmentacji klient√≥w\n\n\n\n\n\n\n\nDane klient√≥w:\n   wiek  wydatki  segment\n0    25     2000        0\n1    30     2500        2\n2    35     3000        2\n3    22     1800        0\n4    28     2200        0\n\nSegmenty klient√≥w:\nSegment 0: ≈õredni wiek 25, ≈õrednie wydatki 2000\nSegment 1: ≈õredni wiek 50, ≈õrednie wydatki 4000\nSegment 2: ≈õredni wiek 48, ≈õrednie wydatki 2825\n\nPrzyk≈Çad: klient 30 lat, wydaje 2500z≈Ç -&gt; segment 2\n\n\n\n\n\nReal-world zastosowania:\n\nSegmentacja klient√≥w (marketing)\nWykrywanie anomalii (cyberbezpiecze≈Ñstwo)\nAnaliza koszykowa (co kupujƒÖ razem)\nKompresja danych\n\n\n\n\n3) Reinforcement Learning (Uczenie ze wzmocnieniem)\nAgent uczy siƒô przez interakcjƒô i nagrody/kary\n\n# Przyk≈Çad koncepcyjny: optymalizacja reklam\nclass SimpleAgent:\n    def __init__(self):\n        # Jakie reklamy pokazywaƒá: [\"sportowe\", \"technologiczne\", \"modowe\"]\n        self.ad_types = [\"sportowe\", \"technologiczne\", \"modowe\"]\n        self.rewards = [0, 0, 0]  # nagrody za ka≈ºdy typ\n        self.counts = [0, 0, 0]   # ile razy pokazane\n    \n    def choose_ad(self):\n        # Wybierz reklamƒô z najwy≈ºszƒÖ ≈õredniƒÖ nagrodƒÖ\n        avg_rewards = [r/max(c,1) for r, c in zip(self.rewards, self.counts)]\n        return avg_rewards.index(max(avg_rewards))\n    \n    def update_reward(self, ad_type, clicked):\n        # Aktualizuj nagrody na podstawie klikniƒôƒá\n        self.counts[ad_type] += 1\n        if clicked:\n            self.rewards[ad_type] += 1\n    \n    def get_stats(self):\n        stats_list = []\n        for i, ad_type in enumerate(self.ad_types):\n            rate = self.rewards[i] / max(self.counts[i], 1) * 100\n            stats_list.append(f\"{ad_type}: {self.counts[i]} pokaza≈Ñ, {self.rewards[i]} klikniƒôƒá ({rate:.1f}%)\")\n        return stats_list\n\n# Symulacja\nimport numpy as np\nnp.random.seed(42)\n\nagent = SimpleAgent()\nsymulacja_wyniki = []\nsymulacja_wyniki.append(\"üéØ Symulacja optymalizacji reklam:\")\nsymulacja_wyniki.append(\"Agent uczy siƒô, kt√≥re reklamy dzia≈ÇajƒÖ najlepiej...\")\n\nfor day in range(10):\n    ad = agent.choose_ad()\n    # R√≥≈ºne prawdopodobie≈Ñstwa klikniƒôƒá dla r√≥≈ºnych typ√≥w reklam\n    click_probs = [0.1, 0.3, 0.2]  # technologiczne najlepsze\n    clicked = np.random.random() &lt; click_probs[ad]\n    agent.update_reward(ad, clicked)\n    symulacja_wyniki.append(f\"Dzie≈Ñ {day+1}: pokazano {agent.ad_types[ad]}, klikniƒôta: {clicked}\")\n\nkoncowe_statystyki = agent.get_stats()\n\nfor wynik in symulacja_wyniki:\n    print(wynik)\nprint(\"\\nüìä Ko≈Ñcowe statystyki:\")\nfor stat in koncowe_statystyki:\n    print(stat)\n\n\n\n\n\n\n\nPoka≈º symulacjƒô agenta reklamowego\n\n\n\n\n\n\n\nüéØ Symulacja optymalizacji reklam:\nAgent uczy siƒô, kt√≥re reklamy dzia≈ÇajƒÖ najlepiej...\nDzie≈Ñ 1: pokazano sportowe, klikniƒôta: False\nDzie≈Ñ 2: pokazano sportowe, klikniƒôta: False\nDzie≈Ñ 3: pokazano sportowe, klikniƒôta: False\nDzie≈Ñ 4: pokazano sportowe, klikniƒôta: False\nDzie≈Ñ 5: pokazano sportowe, klikniƒôta: False\nDzie≈Ñ 6: pokazano sportowe, klikniƒôta: False\nDzie≈Ñ 7: pokazano sportowe, klikniƒôta: True\nDzie≈Ñ 8: pokazano sportowe, klikniƒôta: False\nDzie≈Ñ 9: pokazano sportowe, klikniƒôta: False\nDzie≈Ñ 10: pokazano sportowe, klikniƒôta: False\n\nüìä Ko≈Ñcowe statystyki:\nsportowe: 10 pokaza≈Ñ, 1 klikniƒôƒá (10.0%)\ntechnologiczne: 0 pokaza≈Ñ, 0 klikniƒôƒá (0.0%)\nmodowe: 0 pokaza≈Ñ, 0 klikniƒôƒá (0.0%)\n\n\n\n\n\nReal-world zastosowania:\n\nGry komputerowe (AI graczy)\nAutonomiczne pojazdy\nOptymalizacja reklam online\nRoboty przemys≈Çowe"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#jak-wybraƒá-odpowiedni-typ-ml",
    "href": "cheatsheets/01-intro-ml.html#jak-wybraƒá-odpowiedni-typ-ml",
    "title": "Wprowadzenie do Machine Learning ‚Äî fundament",
    "section": "üéØ Jak wybraƒá odpowiedni typ ML?",
    "text": "üéØ Jak wybraƒá odpowiedni typ ML?\n\n\n\n\n\n\n\n\nSytuacja\nTyp ML\nPrzyk≈Çad\n\n\n\n\nMasz dane z prawid≈Çowymi odpowiedziami\nSupervised\nSpam/nie-spam w emailach\n\n\nChcesz znale≈∫ƒá ukryte grup—ã\nUnsupervised\nSegmentacja klient√≥w\n\n\nSystem ma siƒô uczyƒá przez trial & error\nReinforcement\nBot do gier"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#podstawowe-kroki-projektu-ml",
    "href": "cheatsheets/01-intro-ml.html#podstawowe-kroki-projektu-ml",
    "title": "Wprowadzenie do Machine Learning ‚Äî fundament",
    "section": "üîß Podstawowe kroki projektu ML",
    "text": "üîß Podstawowe kroki projektu ML\n\n# Kompletny workflow ML na przyk≈Çadzie klasyfikacji iris\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# 1. Za≈Çaduj i poznaj dane\niris = load_iris()\ndata = pd.DataFrame(iris.data, columns=iris.feature_names)\ndata['target'] = iris.target\ntarget_names = iris.target_names\n\n# 2. Przygotuj dane (czyszczenie, encoding)\nX = data.drop('target', axis=1)\ny = data['target']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 3. Podziel na train/test\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# 4. Wybierz i wytrenuj model\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# 5. Oce≈Ñ wyniki\npredictions = model.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\n\n# Przyk≈Çad predykcji\nsample_flower = X_test[0:1]\npredicted_class = model.predict(sample_flower)[0]\npredicted_name = target_names[predicted_class]\nactual_name = target_names[y_test.iloc[0]]\n\n# Przygotuj wyniki do wy≈õwietlenia\nwyniki_workflow = []\nwyniki_workflow.append(\"üî¨ Kompletny workflow projektu ML\")\nwyniki_workflow.append(\"=\" * 40)\nwyniki_workflow.append(\"1Ô∏è‚É£ Dane za≈Çadowane:\")\nwyniki_workflow.append(f\"Kszta≈Çt: {data.shape}\")\nwyniki_workflow.append(f\"Klasy: {target_names}\")\nwyniki_workflow.append(f\"Pierwsze 3 wiersze:\\n{data.head(3).to_string()}\")\nwyniki_workflow.append(f\"\\n2Ô∏è‚É£ Dane przeskalowane (pierwsze 3 cechy pierwszej pr√≥bki):\")\nwyniki_workflow.append(f\"Przed: {X.iloc[0, :3].values}\")\nwyniki_workflow.append(f\"Po: {X_scaled[0, :3]}\")\nwyniki_workflow.append(f\"\\n3Ô∏è‚É£ Podzia≈Ç danych:\")\nwyniki_workflow.append(f\"Train: {len(X_train)} pr√≥bek\")\nwyniki_workflow.append(f\"Test: {len(X_test)} pr√≥bek\")\nwyniki_workflow.append(f\"\\n4Ô∏è‚É£ Model wytrenowany: RandomForestClassifier\")\nwyniki_workflow.append(f\"\\n5Ô∏è‚É£ Wyniki:\")\nwyniki_workflow.append(f\"Dok≈Çadno≈õƒá: {accuracy:.2%}\")\nwyniki_workflow.append(f\"\\nPrzyk≈Çad predykcji:\")\nwyniki_workflow.append(f\"Przewidywana klasa: {predicted_name}\")\nwyniki_workflow.append(f\"Rzeczywista klasa: {actual_name}\")\nwyniki_workflow.append(\"‚úÖ Poprawnie!\" if predicted_name == actual_name else \"‚ùå B≈ÇƒÖd\")\n\nfor wynik in wyniki_workflow:\n    print(wynik)\n\n\n\n\n\n\n\nPoka≈º kompletny workflow ML\n\n\n\n\n\n\n\nüî¨ Kompletny workflow projektu ML\n========================================\n1Ô∏è‚É£ Dane za≈Çadowane:\nKszta≈Çt: (150, 5)\nKlasy: ['setosa' 'versicolor' 'virginica']\nPierwsze 3 wiersze:\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n0                5.1               3.5                1.4               0.2       0\n1                4.9               3.0                1.4               0.2       0\n2                4.7               3.2                1.3               0.2       0\n\n2Ô∏è‚É£ Dane przeskalowane (pierwsze 3 cechy pierwszej pr√≥bki):\nPrzed: [5.1 3.5 1.4]\nPo: [-0.90068117  1.01900435 -1.34022653]\n\n3Ô∏è‚É£ Podzia≈Ç danych:\nTrain: 120 pr√≥bek\nTest: 30 pr√≥bek\n\n4Ô∏è‚É£ Model wytrenowany: RandomForestClassifier\n\n5Ô∏è‚É£ Wyniki:\nDok≈Çadno≈õƒá: 100.00%\n\nPrzyk≈Çad predykcji:\nPrzewidywana klasa: versicolor\nRzeczywista klasa: versicolor\n‚úÖ Poprawnie!"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#najwa≈ºniejsze-biblioteki-python-dla-ml",
    "href": "cheatsheets/01-intro-ml.html#najwa≈ºniejsze-biblioteki-python-dla-ml",
    "title": "Wprowadzenie do Machine Learning ‚Äî fundament",
    "section": "üí° Najwa≈ºniejsze biblioteki Python dla ML",
    "text": "üí° Najwa≈ºniejsze biblioteki Python dla ML\n# Podstawowe przetwarzanie danych\nimport pandas as pd      # DataFrames, CSV, analiza\nimport numpy as np       # obliczenia numeryczne\n\n# Machine Learning\nfrom sklearn import *    # algorytmy ML, preprocessing, metryki\nimport xgboost as xgb   # zaawansowane drzewa decyzyjne\n\n# Wizualizacja\nimport matplotlib.pyplot as plt  # wykresy\nimport seaborn as sns           # piƒôkne wykresy statystyczne\n\n# Deep Learning\nimport tensorflow as tf  # sieci neuronowe (Google)\nimport torch            # sieci neuronowe (Facebook)\n\n\n\n\n\n\n\nüéØ Pro tips dla poczƒÖtkujƒÖcych\n\n\n\n\nZacznij od prostych algorytm√≥w - Linear Regression, Decision Trees\n80% czasu to przygotowanie danych - czyszczenie, eksploracja, feature engineering\nZawsze sprawd≈∫ czy model nie jest overfitted - u≈ºyj validation set\nRozumiej swoje dane przed wyborem algorytmu\nPraktyka &gt; teoria - r√≥b du≈ºo projekt√≥w na r√≥≈ºnych danych!\n\n\n\nNastƒôpna ≈õciƒÖgawka: Linear Regression w praktyce üöÄ"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html",
    "href": "cheatsheets/02-linear-regression.html",
    "title": "Linear Regression ‚Äî przewiduj warto≈õci liczbowe",
    "section": "",
    "text": "Linear Regression to jeden z najprostszych i najczƒô≈õciej u≈ºywanych algorytm√≥w ML do przewidywania warto≈õci liczbowych (np. ceny, temperatury, sprzeda≈ºy). Szuka najlepszej linii prostej, kt√≥ra opisuje zale≈ºno≈õƒá miƒôdzy zmiennymi.\n\n\n\n\n\n\nüí° Intuicja matematyczna\n\n\n\nSzukamy takiej linii y = ax + b, kt√≥ra najlepiej ‚Äúprzechodzi‚Äù przez nasze punkty danych. Algorytm automatycznie dobiera optymalne warto≈õci a (slope) i b (intercept)."
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#czym-jest-linear-regression",
    "href": "cheatsheets/02-linear-regression.html#czym-jest-linear-regression",
    "title": "Linear Regression ‚Äî przewiduj warto≈õci liczbowe",
    "section": "",
    "text": "Linear Regression to jeden z najprostszych i najczƒô≈õciej u≈ºywanych algorytm√≥w ML do przewidywania warto≈õci liczbowych (np. ceny, temperatury, sprzeda≈ºy). Szuka najlepszej linii prostej, kt√≥ra opisuje zale≈ºno≈õƒá miƒôdzy zmiennymi.\n\n\n\n\n\n\nüí° Intuicja matematyczna\n\n\n\nSzukamy takiej linii y = ax + b, kt√≥ra najlepiej ‚Äúprzechodzi‚Äù przez nasze punkty danych. Algorytm automatycznie dobiera optymalne warto≈õci a (slope) i b (intercept)."
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#praktyczny-przyk≈Çad-predykcja-cen-mieszka≈Ñ",
    "href": "cheatsheets/02-linear-regression.html#praktyczny-przyk≈Çad-predykcja-cen-mieszka≈Ñ",
    "title": "Linear Regression ‚Äî przewiduj warto≈õci liczbowe",
    "section": "üè† Praktyczny przyk≈Çad: predykcja cen mieszka≈Ñ",
    "text": "üè† Praktyczny przyk≈Çad: predykcja cen mieszka≈Ñ\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Tworzenie przyk≈Çadowych danych mieszka≈Ñ\nnp.random.seed(42)\nn_mieszkan = 1000\n\ndata = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_mieszkan),\n    'liczba_pokoi': np.random.randint(1, 5, n_mieszkan),\n    'wiek_budynku': np.random.randint(0, 50, n_mieszkan),\n    'odleglosc_centrum': np.random.exponential(5, n_mieszkan)\n})\n\n# Realistyczna formu≈Ça ceny (z szumem)\ndata['cena'] = (\n    data['powierzchnia'] * 8000 +           # 8k za m¬≤\n    data['liczba_pokoi'] * 15000 +          # 15k za pok√≥j\n    (50 - data['wiek_budynku']) * 2000 +    # starsze = ta≈Ñsze\n    data['odleglosc_centrum'] * (-3000) +   # dalej = ta≈Ñsze\n    np.random.normal(0, 50000, n_mieszkan)  # szum losowy\n)\n\nprint(\"Podstawowe statystyki:\")\nprint(data.describe())\n\n\n\n\n\n\n\nPoka≈º statystyki\n\n\n\n\n\n\n\nPodstawowe statystyki:\n       powierzchnia  liczba_pokoi  wiek_budynku  odleglosc_centrum  \\\ncount   1000.000000   1000.000000   1000.000000        1000.000000   \nmean      70.483301      2.473000     25.049000           4.971457   \nstd       24.480398      1.128958     14.384001           4.883716   \nmin      -11.031684      1.000000      0.000000           0.000058   \n25%       53.810242      1.000000     13.000000           1.459988   \n50%       70.632515      2.000000     25.000000           3.505541   \n75%       86.198597      4.000000     38.000000           6.954361   \nmax      166.318287      4.000000     49.000000          34.028756   \n\n               cena  \ncount  1.000000e+03  \nmean   6.363187e+05  \nstd    2.004394e+05  \nmin    1.686504e+04  \n25%    5.049745e+05  \n50%    6.356690e+05  \n75%    7.647808e+05  \nmax    1.440259e+06"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#trenowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/02-linear-regression.html#trenowanie-modelu-krok-po-kroku",
    "title": "Linear Regression ‚Äî przewiduj warto≈õci liczbowe",
    "section": "üîß Trenowanie modelu krok po kroku",
    "text": "üîß Trenowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Sprawdzenie korelacji - kt√≥re zmienne sƒÖ najwa≈ºniejsze?\ncorrelation = data.corr()['cena'].sort_values(ascending=False)\n\n# Podzia≈Ç na features (X) i target (y)\nX = data[['powierzchnia', 'liczba_pokoi', 'wiek_budynku', 'odleglosc_centrum']]\ny = data['cena']\n\n# Podzia≈Ç train/test (80/20)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"Korelacja z cenƒÖ:\")\nprint(correlation)\n\nprint(f\"Dane treningowe: {X_train.shape[0]} mieszka≈Ñ\")\nprint(f\"Dane testowe: {X_test.shape[0]} mieszka≈Ñ\")\n\n\n\n\n\n\n\nPoka≈º korelacje oraz dane treningowe\n\n\n\n\n\n\n\nKorelacja z cenƒÖ:\ncena                 1.000000\npowierzchnia         0.949080\nliczba_pokoi         0.025094\nodleglosc_centrum   -0.042040\nwiek_budynku        -0.115566\nName: cena, dtype: float64\nDane treningowe: 800 mieszka≈Ñ\nDane testowe: 200 mieszka≈Ñ\n\n\n\n\n\n\n\n2) Trenowanie modelu\n\n# Utworzenie i trenowanie modelu\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Sprawdzenie wsp√≥≈Çczynnik√≥w\nprint(\"Wsp√≥≈Çczynniki modelu:\")\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"{feature}: {coef:.0f} z≈Ç\")\nprint(f\"Intercept (sta≈Ça): {model.intercept_:.0f} z≈Ç\")\n\n\n\n\n\n\n\nPoka≈º wsp√≥≈Çczynniki\n\n\n\n\n\n\n\nWsp√≥≈Çczynniki modelu:\npowierzchnia: 7924 z≈Ç\nliczba_pokoi: 15811 z≈Ç\nwiek_budynku: -2108 z≈Ç\nodleglosc_centrum: -2698 z≈Ç\nIntercept (sta≈Ça): 104042 z≈Ç\n\n\n\n\n\n\n\n3) Ewaluacja wynik√≥w\n\n# Predykcje na zbiorze testowym\ny_pred = model.predict(X_test)\n\n# Metryki\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"\\nWyniki modelu:\")\nprint(f\"Mean Absolute Error: {mae:.0f} z≈Ç\")\nprint(f\"R¬≤ Score: {r2:.3f}\")\nprint(f\"≈öredni b≈ÇƒÖd to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania\")\n\n# Przyk≈Çadowe predykcje\nfor i in range(5):\n    rzeczywista = y_test.iloc[i]\n    przewidywana = y_pred[i]\n    b≈ÇƒÖd = abs(rzeczywista - przewidywana)\n    print(f\"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}z≈Ç, \"\n          f\"przewidywana={przewidywana:.0f}z≈Ç, b≈ÇƒÖd={b≈ÇƒÖd:.0f}z≈Ç\")\n\n\n\n\n\n\n\nPoka≈º wyniki modelu\n\n\n\n\n\n\n\n\nWyniki modelu:\nMean Absolute Error: 38912 z≈Ç\nR¬≤ Score: 0.934\n≈öredni b≈ÇƒÖd to 6.1% ceny mieszkania\nMieszkanie 1: rzeczywista=702313z≈Ç, przewidywana=771489z≈Ç, b≈ÇƒÖd=69177z≈Ç\nMieszkanie 2: rzeczywista=791174z≈Ç, przewidywana=816756z≈Ç, b≈ÇƒÖd=25582z≈Ç\nMieszkanie 3: rzeczywista=326702z≈Ç, przewidywana=274297z≈Ç, b≈ÇƒÖd=52405z≈Ç\nMieszkanie 4: rzeczywista=441380z≈Ç, przewidywana=456054z≈Ç, b≈ÇƒÖd=14674z≈Ç\nMieszkanie 5: rzeczywista=445427z≈Ç, przewidywana=383199z≈Ç, b≈ÇƒÖd=62228z≈Ç"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#wizualizacja-wynik√≥w",
    "href": "cheatsheets/02-linear-regression.html#wizualizacja-wynik√≥w",
    "title": "Linear Regression ‚Äî przewiduj warto≈õci liczbowe",
    "section": "üìä Wizualizacja wynik√≥w",
    "text": "üìä Wizualizacja wynik√≥w\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Przyk≈Çadowe dane mieszka≈Ñ (powtarzamy dla demonstracji)\nnp.random.seed(42)\nn_mieszkan = 1000\n\ndata = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_mieszkan),\n    'liczba_pokoi': np.random.randint(1, 5, n_mieszkan),\n    'wiek_budynku': np.random.randint(0, 50, n_mieszkan),\n    'odleglosc_centrum': np.random.exponential(5, n_mieszkan)\n})\n\ndata['cena'] = (\n    data['powierzchnia'] * 8000 +\n    data['liczba_pokoi'] * 15000 +\n    (50 - data['wiek_budynku']) * 2000 +\n    data['odleglosc_centrum'] * (-3000) +\n    np.random.normal(0, 50000, n_mieszkan)\n)\n\n# Przygotowanie i trenowanie modelu\nX = data[['powierzchnia', 'liczba_pokoi', 'wiek_budynku', 'odleglosc_centrum']]\ny = data['cena']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Oblicz metryki\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Przygotuj wykresy\nplt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Rzeczywiste ceny')\nplt.ylabel('Przewidywane ceny')\nplt.title('Rzeczywiste vs Przewidywane')\n\nplt.subplot(1, 2, 2)\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Przewidywane ceny')\nplt.ylabel('Residuals (b≈Çƒôdy)')\nplt.title('Analiza residuals')\n\nplt.tight_layout()\n\n# Zapisz wykres do zmiennej\nimport io\nimport base64\nbuf = io.BytesIO()\nplt.savefig(buf, format='png', dpi=100, bbox_inches='tight')\nbuf.seek(0)\nplt.close()\n\n# Przygotuj przyk≈Çadowe predykcje\nprzykladowe_predykcje = []\nfor i in range(3):\n    rzeczywista = y_test.iloc[i]\n    przewidywana = y_pred[i]\n    blad = abs(rzeczywista - przewidywana)\n    przykladowe_predykcje.append(f\"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}z≈Ç, przewidywana={przewidywana:.0f}z≈Ç, b≈ÇƒÖd={blad:.0f}z≈Ç\")\n\nprint(f\"Wyniki modelu Linear Regression:\")\nprint(f\"Mean Absolute Error: {mae:.0f} z≈Ç\")\nprint(f\"R¬≤ Score: {r2:.3f}\")\nprint(f\"≈öredni b≈ÇƒÖd to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania\")\nprint(f\"\\nPrzyk≈Çadowe predykcje:\")\nfor pred in przykladowe_predykcje:\n    print(pred)\n\n# Odtw√≥rz wykres\nplt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Rzeczywiste ceny')\nplt.ylabel('Przewidywane ceny')\nplt.title('Rzeczywiste vs Przewidywane')\n\nplt.subplot(1, 2, 2)\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Przewidywane ceny')\nplt.ylabel('Residuals (b≈Çƒôdy)')\nplt.title('Analiza residuals')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nPoka≈º wyniki i wykresy analizy\n\n\n\n\n\n\n\nWyniki modelu Linear Regression:\nMean Absolute Error: 38912 z≈Ç\nR¬≤ Score: 0.934\n≈öredni b≈ÇƒÖd to 6.1% ceny mieszkania\n\nPrzyk≈Çadowe predykcje:\nMieszkanie 1: rzeczywista=702313z≈Ç, przewidywana=771489z≈Ç, b≈ÇƒÖd=69177z≈Ç\nMieszkanie 2: rzeczywista=791174z≈Ç, przewidywana=816756z≈Ç, b≈ÇƒÖd=25582z≈Ç\nMieszkanie 3: rzeczywista=326702z≈Ç, przewidywana=274297z≈Ç, b≈ÇƒÖd=52405z≈Ç\n\n\n\n\n\nAnaliza wynik√≥w modelu Linear Regression"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#praktyczne-zastosowania-linear-regression",
    "href": "cheatsheets/02-linear-regression.html#praktyczne-zastosowania-linear-regression",
    "title": "Linear Regression ‚Äî przewiduj warto≈õci liczbowe",
    "section": "üéØ Praktyczne zastosowania Linear Regression",
    "text": "üéØ Praktyczne zastosowania Linear Regression\n\n1) Biznesowe\n\n# Przewidywanie sprzeda≈ºy na podstawie bud≈ºetu marketingowego\nsales_data = pd.DataFrame({\n    'marketing_budget': [10000, 15000, 20000, 25000, 30000],\n    'sales': [100000, 140000, 180000, 220000, 260000]\n})\n\nmodel_sales = LinearRegression()\nmodel_sales.fit(sales_data[['marketing_budget']], sales_data['sales'])\n\n# \"Je≈õli zwiƒôkszƒô bud≈ºet do 35k, sprzeda≈º wzro≈õnie do:\"\nnew_sales = model_sales.predict([[35000]])\nprint(f\"Przewidywana sprzeda≈º przy bud≈ºecie 35k: {new_sales[0]:.0f}\")\n\n\n\n\n\n\n\nPoka≈º przewidywanƒÖ sprzeda≈º\n\n\n\n\n\n\n\nPrzewidywana sprzeda≈º przy bud≈ºecie 35k: 300000\n\n\n\n\n\n\n\n2) Analizy finansowe\n\n# Relacja miƒôdzy PKB a konsumpcjƒÖ\neconomics_data = pd.DataFrame({\n    'pkb_per_capita': [25000, 30000, 35000, 40000, 45000],\n    'konsumpcja': [18000, 21000, 24000, 27000, 30000]\n})\n\nmodel_econ = LinearRegression()\nmodel_econ.fit(economics_data[['pkb_per_capita']], economics_data['konsumpcja'])\n\n# Wsp√≥≈Çczynnik sk≈Çonno≈õci do konsumpcji\nprint(f\"Na ka≈ºde dodatkowe 1000z≈Ç PKB, konsumpcja ro≈õnie o: {model_econ.coef_[0]:.0f}z≈Ç\")\n\n\n\n\n\n\n\nPoka≈º wsp√≥≈Çczynnik sk≈Çonno≈õci do konsumpcji\n\n\n\n\n\n\n\nNa ka≈ºde dodatkowe 1000z≈Ç PKB, konsumpcja ro≈õnie o: 1z≈Ç"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#najczƒôstsze-pu≈Çapki-i-jak-ich-unikaƒá",
    "href": "cheatsheets/02-linear-regression.html#najczƒôstsze-pu≈Çapki-i-jak-ich-unikaƒá",
    "title": "Linear Regression ‚Äî przewiduj warto≈õci liczbowe",
    "section": "‚ö†Ô∏è Najczƒôstsze pu≈Çapki i jak ich unikaƒá",
    "text": "‚ö†Ô∏è Najczƒôstsze pu≈Çapki i jak ich unikaƒá\n\n1) Overfitting z wieloma zmiennymi\n\n# Z≈ÅO: za du≈ºo features wzglƒôdem danych\n# Je≈õli masz 100 mieszka≈Ñ, nie u≈ºywaj 50 features!\n\n# DOBRZE: zasada kciuka\ndef check_features_ratio(X, y):\n    ratio = len(y) / X.shape[1]\n    if ratio &lt; 10:\n        print(f\"‚ö†Ô∏è Uwaga: masz tylko {ratio:.1f} obserwacji na feature!\")\n        print(\"Rozwa≈º: wiƒôcej danych lub mniej features\")\n    else:\n        print(f\"‚úÖ OK: {ratio:.1f} obserwacji na feature\")\n\ncheck_features_ratio(X_train, y_train)\n\n\n\n\n\n\n\nPoka≈º wyniki\n\n\n\n\n\n\n\n‚úÖ OK: 200.0 obserwacji na feature\n\n\n\n\n\n\n\n2) Sprawdzanie za≈Ço≈ºe≈Ñ linearno≈õci\n\n# Sprawd≈∫ czy zale≈ºno≈õci sƒÖ rzeczywi≈õcie liniowe\nfrom scipy import stats\n\nfor col in X.columns:\n    correlation, p_value = stats.pearsonr(data[col], data['cena'])\n    print(f\"{col}: korelacja={correlation:.3f}, p-value={p_value:.3f}\")\n    \n    if abs(correlation) &lt; 0.1:\n        print(f\"‚ö†Ô∏è {col} ma s≈ÇabƒÖ korelacjƒô - mo≈ºe nie byƒá przydatna\")\n\n\n\n\n\n\n\nPoka≈º wyniki korelacji\n\n\n\n\n\n\n\npowierzchnia: korelacja=0.949, p-value=0.000\nliczba_pokoi: korelacja=0.025, p-value=0.428\n‚ö†Ô∏è liczba_pokoi ma s≈ÇabƒÖ korelacjƒô - mo≈ºe nie byƒá przydatna\nwiek_budynku: korelacja=-0.116, p-value=0.000\nodleglosc_centrum: korelacja=-0.042, p-value=0.184\n‚ö†Ô∏è odleglosc_centrum ma s≈ÇabƒÖ korelacjƒô - mo≈ºe nie byƒá przydatna\n\n\n\n\n\n\n\n3) Wielokoliniowo≈õƒá (features korelujƒÖ miƒôdzy sobƒÖ)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Przyk≈Çadowe dane features (demonstracja)\nnp.random.seed(42)\nn_samples = 1000\n\nX = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_samples),\n    'liczba_pokoi': np.random.randint(1, 5, n_samples),\n    'wiek_budynku': np.random.randint(0, 50, n_samples),\n    'odleglosc_centrum': np.random.exponential(5, n_samples)\n})\n\n# Oblicz korelacje miƒôdzy features\ncorrelation_matrix = X.corr()\n\n# Przygotuj wykres\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Korelacje miƒôdzy features')\nplt.close()\n\nprint(\"Macierz korelacji miƒôdzy zmiennymi:\")\nprint(correlation_matrix)\nprint(\"\\nInterpretacja:\")\nprint(\"‚Ä¢ Warto≈õci blisko 1.0 = silna korelacja pozytywna\")\nprint(\"‚Ä¢ Warto≈õci blisko -1.0 = silna korelacja negatywna\") \nprint(\"‚Ä¢ Warto≈õci blisko 0.0 = brak korelacji\")\nprint(\"\\nJe≈õli korelacja miƒôdzy features &gt; 0.8, usu≈Ñ jednƒÖ z nich (multicollinearity)!\")\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Korelacje miƒôdzy features')\nplt.show()\n\n\n\n\n\n\n\nPoka≈º mapƒô korelacji\n\n\n\n\n\n\n\nMacierz korelacji miƒôdzy zmiennymi:\n                   powierzchnia  liczba_pokoi  wiek_budynku  odleglosc_centrum\npowierzchnia           1.000000     -0.064763      0.033920           0.029856\nliczba_pokoi          -0.064763      1.000000     -0.003894          -0.057555\nwiek_budynku           0.033920     -0.003894      1.000000          -0.037242\nodleglosc_centrum      0.029856     -0.057555     -0.037242           1.000000\n\nInterpretacja:\n‚Ä¢ Warto≈õci blisko 1.0 = silna korelacja pozytywna\n‚Ä¢ Warto≈õci blisko -1.0 = silna korelacja negatywna\n‚Ä¢ Warto≈õci blisko 0.0 = brak korelacji\n\nJe≈õli korelacja miƒôdzy features &gt; 0.8, usu≈Ñ jednƒÖ z nich (multicollinearity)!\n\n\n\n\n\nMapa korelacji miƒôdzy zmiennymi"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#parametry-do-tuningu",
    "href": "cheatsheets/02-linear-regression.html#parametry-do-tuningu",
    "title": "Linear Regression ‚Äî przewiduj warto≈õci liczbowe",
    "section": "üîß Parametry do tuningu",
    "text": "üîß Parametry do tuningu\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n# 1) Standardization - gdy features majƒÖ r√≥≈ºne skale\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train)\n\n# 2) Polynomial Features - dla nieliniowych zale≈ºno≈õci\npoly_model = Pipeline([\n    ('poly', PolynomialFeatures(degree=2)),\n    ('linear', LinearRegression())\n])\npoly_model.fit(X_train, y_train)\n\npoly_pred = poly_model.predict(X_test)\npoly_r2 = r2_score(y_test, poly_pred)\nprint(f\"Polynomial Regression R¬≤: {poly_r2:.3f}\")\n\n# 3) Regularization - Ridge i Lasso\nfrom sklearn.linear_model import Ridge, Lasso\n\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\nridge_r2 = r2_score(y_test, ridge.predict(X_test))\n\nlasso = Lasso(alpha=1.0)\nlasso.fit(X_train, y_train)\nlasso_r2 = r2_score(y_test, lasso.predict(X_test))\n\nprint(f\"Ridge R¬≤: {ridge_r2:.3f}\")\nprint(f\"Lasso R¬≤: {lasso_r2:.3f}\")\n\n\n\n\n\n\n\nPoka≈º wyniki predykcji cen\n\n\n\n\n\n\n\nPolynomial Regression R¬≤: 0.933\nRidge R¬≤: 0.934\nLasso R¬≤: 0.934"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#real-world-przyk≈Çady-zastosowa≈Ñ",
    "href": "cheatsheets/02-linear-regression.html#real-world-przyk≈Çady-zastosowa≈Ñ",
    "title": "Linear Regression ‚Äî przewiduj warto≈õci liczbowe",
    "section": "üåç Real-world przyk≈Çady zastosowa≈Ñ",
    "text": "üåç Real-world przyk≈Çady zastosowa≈Ñ\n\nE-commerce: Przewidywanie warto≈õci ≈ºyciowej klienta (LTV)\nNieruchomo≈õci: Automatyczna wycena dom√≥w (Zillow)\nFinanse: Scoring kredytowy, przewidywanie cen akcji\nMarketing: ROI kampanii reklamowych\nSupply Chain: Prognozowanie popytu na produkty\n\n\n\n\n\n\n\nüí° Kiedy u≈ºywaƒá Linear Regression?\n\n\n\n‚úÖ U≈ªYJ GDY:\n\nPrzewidujesz warto≈õci liczbowe (continuous target)\nZale≈ºno≈õci wydajƒÖ siƒô liniowe\nChcesz interpretowalny model\nMasz stosunkowo ma≈Ço features\n\n‚ùå NIE U≈ªYWAJ GDY:\n\nTarget jest kategoryczny (u≈ºyj klasyfikacji)\nZale≈ºno≈õci sƒÖ bardzo nieliniowe (u≈ºyj Random Forest, XGBoost)\nMasz tysiƒÖce features (u≈ºyj regularization)\n\n\n\nNastƒôpna ≈õciƒÖgawka: Decision Trees - klasyfikacja üå≥"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html",
    "href": "cheatsheets/04-random-forest.html",
    "title": "Random Forest ‚Äî las drzew decyzyjnych",
    "section": "",
    "text": "Random Forest to zesp√≥≈Ç wielu drzew decyzyjnych, kt√≥re g≈ÇosujƒÖ razem nad ko≈ÑcowƒÖ decyzjƒÖ. Jeden Decision Tree mo≈ºe siƒô myliƒá, ale gdy masz 100 drzew i wiƒôkszo≈õƒá m√≥wi ‚ÄúTAK‚Äù - to prawdopodobnie dobra odpowied≈∫!\n\n\n\n\n\n\nüí° Intuicja\n\n\n\nWyobra≈∫ sobie konsylium lekarskim: jeden lekarz mo≈ºe siƒô pomyliƒá w diagnozie, ale gdy 7 z 10 ekspert√≥w zgadza siƒô - diagnoza jest znacznie bardziej pewna. Random Forest dzia≈Ça dok≈Çadnie tak samo!"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#czym-jest-random-forest",
    "href": "cheatsheets/04-random-forest.html#czym-jest-random-forest",
    "title": "Random Forest ‚Äî las drzew decyzyjnych",
    "section": "",
    "text": "Random Forest to zesp√≥≈Ç wielu drzew decyzyjnych, kt√≥re g≈ÇosujƒÖ razem nad ko≈ÑcowƒÖ decyzjƒÖ. Jeden Decision Tree mo≈ºe siƒô myliƒá, ale gdy masz 100 drzew i wiƒôkszo≈õƒá m√≥wi ‚ÄúTAK‚Äù - to prawdopodobnie dobra odpowied≈∫!\n\n\n\n\n\n\nüí° Intuicja\n\n\n\nWyobra≈∫ sobie konsylium lekarskim: jeden lekarz mo≈ºe siƒô pomyliƒá w diagnozie, ale gdy 7 z 10 ekspert√≥w zgadza siƒô - diagnoza jest znacznie bardziej pewna. Random Forest dzia≈Ça dok≈Çadnie tak samo!"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#praktyczny-przyk≈Çad-przewidywanie-sukcesu-produktu",
    "href": "cheatsheets/04-random-forest.html#praktyczny-przyk≈Çad-przewidywanie-sukcesu-produktu",
    "title": "Random Forest ‚Äî las drzew decyzyjnych",
    "section": "üéØ Praktyczny przyk≈Çad: przewidywanie sukcesu produktu",
    "text": "üéØ Praktyczny przyk≈Çad: przewidywanie sukcesu produktu\nCzy nowy produkt odniesie sukces na rynku?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Tworzenie realistycznych danych produkt√≥w\nnp.random.seed(42)\nn_products = 2000\n\ndata = pd.DataFrame({\n    'cena': np.random.lognormal(6, 1, n_products),  # log-normal dla cen\n    'jakosc': np.random.randint(1, 11, n_products),  # 1-10 skala jako≈õci\n    'marketing_budget': np.random.exponential(50000, n_products),\n    'konkurencja': np.random.randint(1, 21, n_products),  # liczba konkurent√≥w\n    'sezon': np.random.choice(['wiosna', 'lato', 'jesien', 'zima'], n_products),\n    'kategoria': np.random.choice(['elektronika', 'moda', 'dom', 'sport', 'ksiazki'], n_products),\n    'ocena_testowa': np.random.normal(7, 2, n_products),  # oceny focus group\n    'innowacyjnosc': np.random.randint(1, 11, n_products),\n    'marka_znana': np.random.choice([0, 1], n_products, p=[0.3, 0.7])  # czy znana marka\n})\n\n# Realistyczna logika sukcesu produktu\ndef czy_sukces(row):\n    score = 0\n    \n    # Cena (sweet spot 50-500)\n    if 50 &lt;= row['cena'] &lt;= 500:\n        score += 2\n    elif row['cena'] &gt; 1000:\n        score -= 2\n    \n    # Jako≈õƒá (najwa≈ºniejsze!)\n    score += row['jakosc'] * 0.8\n    \n    # Marketing\n    if row['marketing_budget'] &gt; 100000:\n        score += 3\n    elif row['marketing_budget'] &gt; 50000:\n        score += 1\n    \n    # Konkurencja (mniej = lepiej)\n    score -= row['konkurencja'] * 0.2\n    \n    # Sezon (lato i zima lepsze)\n    if row['sezon'] in ['lato', 'zima']:\n        score += 1\n    \n    # Kategoria\n    if row['kategoria'] == 'elektronika':\n        score += 1\n    elif row['kategoria'] == 'ksiazki':\n        score -= 1\n    \n    # Ocena testowa\n    score += (row['ocena_testowa'] - 5) * 0.5\n    \n    # Innowacyjno≈õƒá\n    score += row['innowacyjnosc'] * 0.3\n    \n    # Znana marka\n    if row['marka_znana']:\n        score += 2\n    \n    # Ko≈Ñcowa decyzja z odrobinƒÖ losowo≈õci\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 1)))\n    return probability &gt; 0.5\n\ndata['sukces'] = data.apply(czy_sukces, axis=1)\n\nprint(\"Statystyki produkt√≥w:\")\nprint(data.describe())\nprint(f\"\\nOdsetek udanych produkt√≥w: {data['sukces'].mean():.1%}\")\n\n\n\n\n\n\n\nPoka≈º statystyki produkt√≥w\n\n\n\n\n\n\n\nStatystyki produkt√≥w:\n               cena       jakosc  marketing_budget  konkurencja  \\\ncount   2000.000000  2000.000000       2000.000000   2000.00000   \nmean     692.530569     5.534500      49376.934612     10.53950   \nstd      929.701850     2.871922      49876.166534      5.76594   \nmin       15.779832     1.000000         11.353200      1.00000   \n25%      216.445352     3.000000      13957.783468      6.00000   \n50%      421.867820     6.000000      33526.684882     11.00000   \n75%      798.695019     8.000000      67692.822351     16.00000   \nmax    19010.210160    10.000000     376260.171802     20.00000   \n\n       ocena_testowa  innowacyjnosc  marka_znana  \ncount    2000.000000    2000.000000  2000.000000  \nmean        7.011461       5.500000     0.711000  \nstd         1.957420       2.909679     0.453411  \nmin        -0.844801       1.000000     0.000000  \n25%         5.700781       3.000000     0.000000  \n50%         7.041344       6.000000     1.000000  \n75%         8.307413       8.000000     1.000000  \nmax        13.754766      10.000000     1.000000  \n\nOdsetek udanych produkt√≥w: 99.2%"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#budowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/04-random-forest.html#budowanie-modelu-krok-po-kroku",
    "title": "Random Forest ‚Äî las drzew decyzyjnych",
    "section": "üîß Budowanie modelu krok po kroku",
    "text": "üîß Budowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Encoding kategorycznych zmiennych\nfrom sklearn.preprocessing import LabelEncoder\n\ndata_encoded = data.copy()\nle_sezon = LabelEncoder()\nle_kategoria = LabelEncoder()\n\ndata_encoded['sezon_num'] = le_sezon.fit_transform(data['sezon'])\ndata_encoded['kategoria_num'] = le_kategoria.fit_transform(data['kategoria'])\n\n# Features do modelu\nfeature_columns = ['cena', 'jakosc', 'marketing_budget', 'konkurencja', \n                  'sezon_num', 'kategoria_num', 'ocena_testowa', \n                  'innowacyjnosc', 'marka_znana']\nX = data_encoded[feature_columns]\ny = data_encoded['sukces']\n\n# Podzia≈Ç train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Dane treningowe: {len(X_train)} produkt√≥w\")\nprint(f\"Dane testowe: {len(X_test)} produkt√≥w\")\n\n\n\n\n\n\n\nPoka≈º podzia≈Ç danych\n\n\n\n\n\n\n\nDane treningowe: 1600 produkt√≥w\nDane testowe: 400 produkt√≥w\n\n\n\n\n\n\n\n2) Trenowanie Random Forest\n\n# Tworzenie modelu Random Forest\nrf_model = RandomForestClassifier(\n    n_estimators=100,       # liczba drzew w lesie\n    max_depth=10,          # maksymalna g≈Çƒôboko≈õƒá ka≈ºdego drzewa\n    min_samples_split=20,   # min. pr√≥bek do podzia≈Çu\n    min_samples_leaf=10,    # min. pr√≥bek w li≈õciu\n    random_state=42,\n    n_jobs=-1              # u≈ºyj wszystkie rdzenie CPU\n)\n\n# Trenowanie\nrf_model.fit(X_train, y_train)\n\nprint(\"Random Forest wytrenowany!\")\nprint(f\"Liczba drzew: {rf_model.n_estimators}\")\nprint(f\"≈örednia g≈Çƒôboko≈õƒá drzew: {np.mean([tree.tree_.max_depth for tree in rf_model.estimators_]):.1f}\")\n\n\n\n\n\n\n\nPoka≈º parametry wytrenowanego modelu\n\n\n\n\n\n\n\nRandom Forest wytrenowany!\nLiczba drzew: 100\n≈örednia g≈Çƒôboko≈õƒá drzew: 5.2\n\n\n\n\n\n\n\n3) Ewaluacja modelu\n\n# Predykcje\ny_pred = rf_model.predict(X_test)\ny_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n\n# Metryki\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nDok≈Çadno≈õƒá Random Forest: {accuracy:.1%}\")\n\n# Szczeg√≥≈Çowy raport\nprint(\"\\nRaport klasyfikacji:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:     Pora≈ºka  Sukces\")\nprint(f\"Rzeczywiste Pora≈ºka: {cm[0,0]:3d}     {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Sukces:  {cm[1,0]:3d}     {cm[1,1]:3d}\")\n\n\n\n\n\n\n\nPoka≈º wyniki Random Forest\n\n\n\n\n\n\n\n\nDok≈Çadno≈õƒá Random Forest: 99.8%\n\nConfusion Matrix:\nPrzewidywane:     Pora≈ºka  Sukces\nRzeczywiste Pora≈ºka:   0       1\nRzeczywiste Sukces:    0     399"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#analiza-wa≈ºno≈õci-cech-feature-importance",
    "href": "cheatsheets/04-random-forest.html#analiza-wa≈ºno≈õci-cech-feature-importance",
    "title": "Random Forest ‚Äî las drzew decyzyjnych",
    "section": "üìä Analiza wa≈ºno≈õci cech (Feature Importance)",
    "text": "üìä Analiza wa≈ºno≈õci cech (Feature Importance)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Przygotuj dane (powtarzamy dla kompletno≈õci)\nnp.random.seed(42)\nn_products = 2000\n\ndata = pd.DataFrame({\n    'cena': np.random.lognormal(6, 1, n_products),\n    'jakosc': np.random.randint(1, 11, n_products),\n    'marketing_budget': np.random.exponential(50000, n_products),\n    'konkurencja': np.random.randint(1, 21, n_products),\n    'sezon': np.random.choice(['wiosna', 'lato', 'jesien', 'zima'], n_products),\n    'kategoria': np.random.choice(['elektronika', 'moda', 'dom', 'sport', 'ksiazki'], n_products),\n    'ocena_testowa': np.random.normal(7, 2, n_products),\n    'innowacyjnosc': np.random.randint(1, 11, n_products),\n    'marka_znana': np.random.choice([0, 1], n_products, p=[0.3, 0.7])\n})\n\ndef czy_sukces(row):\n    score = 0\n    if 50 &lt;= row['cena'] &lt;= 500:\n        score += 2\n    elif row['cena'] &gt; 1000:\n        score -= 2\n    score += row['jakosc'] * 0.8\n    if row['marketing_budget'] &gt; 100000:\n        score += 3\n    elif row['marketing_budget'] &gt; 50000:\n        score += 1\n    score -= row['konkurencja'] * 0.2\n    if row['sezon'] in ['lato', 'zima']:\n        score += 1\n    if row['kategoria'] == 'elektronika':\n        score += 1\n    elif row['kategoria'] == 'ksiazki':\n        score -= 1\n    score += (row['ocena_testowa'] - 5) * 0.5\n    score += row['innowacyjnosc'] * 0.3\n    if row['marka_znana']:\n        score += 2\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 1)))\n    return probability &gt; 0.5\n\ndata['sukces'] = data.apply(czy_sukces, axis=1)\n\n# Encoding i model\ndata_encoded = data.copy()\nle_sezon = LabelEncoder()\nle_kategoria = LabelEncoder()\ndata_encoded['sezon_num'] = le_sezon.fit_transform(data['sezon'])\ndata_encoded['kategoria_num'] = le_kategoria.fit_transform(data['kategoria'])\n\nfeature_columns = ['cena', 'jakosc', 'marketing_budget', 'konkurencja', \n                  'sezon_num', 'kategoria_num', 'ocena_testowa', \n                  'innowacyjnosc', 'marka_znana']\nX = data_encoded[feature_columns]\ny = data_encoded['sukces']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenuj model\nrf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\nrf_model.fit(X_train, y_train)\n\n# Ewaluacja\ny_pred = rf_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\n# Wa≈ºno≈õƒá cech\nfeature_names = ['cena', 'jako≈õƒá', 'marketing_budget', 'konkurencja', \n                'sezon', 'kategoria', 'ocena_testowa', 'innowacyjno≈õƒá', 'marka_znana']\nimportance = rf_model.feature_importances_\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importance\n}).sort_values('importance', ascending=False)\n\n# Wykres wa≈ºno≈õci cech\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance['feature'], feature_importance['importance'])\nplt.xlabel('Wa≈ºno≈õƒá cechy')\nplt.title('Wa≈ºno≈õƒá cech w przewidywaniu sukcesu produktu')\nplt.gca().invert_yaxis()\nplt.close()\n\nprint(f\"Wyniki Random Forest:\")\nprint(f\"Dok≈Çadno≈õƒá modelu: {accuracy:.1%}\")\nprint(f\"Liczba drzew: {rf_model.n_estimators}\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:     Pora≈ºka  Sukces\")\nprint(f\"Rzeczywiste Pora≈ºka: {cm[0,0]:3d}     {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Sukces:  {cm[1,0]:3d}     {cm[1,1]:3d}\")\n\nprint(\"\\nWa≈ºno≈õƒá cech:\")\nfor _, row in feature_importance.iterrows():\n    print(f\"{row['feature']}: {row['importance']:.3f}\")\n\n# Odtw√≥rz wykres\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance['feature'], feature_importance['importance'])\nplt.xlabel('Wa≈ºno≈õƒá cechy')\nplt.title('Wa≈ºno≈õƒá cech w przewidywaniu sukcesu produktu')\nplt.gca().invert_yaxis()\nplt.show()\n\n\n\n\n\n\n\nPoka≈º wyniki i wykres wa≈ºno≈õci cech\n\n\n\n\n\n\n\nWyniki Random Forest:\nDok≈Çadno≈õƒá modelu: 99.8%\nLiczba drzew: 100\n\nConfusion Matrix:\nPrzewidywane:     Pora≈ºka  Sukces\nRzeczywiste Pora≈ºka:   0       1\nRzeczywiste Sukces:    0     399\n\nWa≈ºno≈õƒá cech:\ncena: 0.294\nmarketing_budget: 0.168\nocena_testowa: 0.143\njako≈õƒá: 0.117\nkonkurencja: 0.105\ninnowacyjno≈õƒá: 0.069\nsezon: 0.042\nkategoria: 0.033\nmarka_znana: 0.028\n\n\n\n\n\nWa≈ºno≈õƒá cech w Random Forest"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#random-forest-vs-decision-tree---por√≥wnanie",
    "href": "cheatsheets/04-random-forest.html#random-forest-vs-decision-tree---por√≥wnanie",
    "title": "Random Forest ‚Äî las drzew decyzyjnych",
    "section": "üîç Random Forest vs Decision Tree - por√≥wnanie",
    "text": "üîç Random Forest vs Decision Tree - por√≥wnanie\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Por√≥wnanie z pojedynczym Decision Tree\ndt_model = DecisionTreeClassifier(max_depth=10, random_state=42)\ndt_model.fit(X_train, y_train)\n\ndt_pred = dt_model.predict(X_test)\ndt_accuracy = accuracy_score(y_test, dt_pred)\n\nprint(\"POR√ìWNANIE MODELI:\")\nprint(f\"Decision Tree:    {dt_accuracy:.1%}\")\nprint(f\"Random Forest:    {accuracy:.1%}\")\nprint(f\"Poprawa:          +{(accuracy - dt_accuracy)*100:.1f} punkt√≥w procentowych\")\n\n# Test stabilno≈õci - r√≥≈ºne random_state\ndt_results = []\nrf_results = []\n\nfor rs in range(10):\n    # Decision Tree\n    dt_temp = DecisionTreeClassifier(max_depth=10, random_state=rs)\n    dt_temp.fit(X_train, y_train)\n    dt_results.append(accuracy_score(y_test, dt_temp.predict(X_test)))\n    \n    # Random Forest  \n    rf_temp = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=rs)\n    rf_temp.fit(X_train, y_train)\n    rf_results.append(accuracy_score(y_test, rf_temp.predict(X_test)))\n\nprint(f\"\\nSTABILNO≈öƒÜ (10 r√≥≈ºnych random_state):\")\nprint(f\"Decision Tree rozrzut: {max(dt_results)-min(dt_results):.1%}\")\nprint(f\"Random Forest rozrzut: {max(rf_results)-min(rf_results):.1%}\")\nprint(\"Random Forest jest znacznie bardziej stabilny!\")\n\n\n\n\n\n\n\nPoka≈º por√≥wnanie modeli\n\n\n\n\n\n\n\nPOR√ìWNANIE MODELI:\nDecision Tree:    98.5%\nRandom Forest:    99.8%\nPoprawa:          +1.3 punkt√≥w procentowych\n\nSTABILNO≈öƒÜ (10 r√≥≈ºnych random_state):\nDecision Tree rozrzut: 1.0%\nRandom Forest rozrzut: 0.0%\nRandom Forest jest znacznie bardziej stabilny!"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#zastosowania-random-forest",
    "href": "cheatsheets/04-random-forest.html#zastosowania-random-forest",
    "title": "Random Forest ‚Äî las drzew decyzyjnych",
    "section": "üéØ Zastosowania Random Forest",
    "text": "üéØ Zastosowania Random Forest\n\n1) E-commerce - rekomendacje produkt√≥w\n\n# Przewidywanie czy u≈ºytkownik kupi produkt\necommerce_features = ['historia_zakup√≥w', 'czas_na_stronie', 'kategoria_preferowana', \n                     'cena_produktu', 'oceny_produktu', 'sezon']\n# Target: 'kupi_produkt' (tak/nie)\n\necommerce_rf = RandomForestClassifier(n_estimators=200)\n# Wynik: system rekomendacji uwzglƒôdniajƒÖcy kompleksowe zachowania u≈ºytkownik√≥w\n\n\n\n2) Finanse - wykrywanie fraud√≥w\n\n# Identyfikacja podejrzanych transakcji\nfraud_features = ['kwota', 'godzina', 'lokalizacja', 'typ_karty', \n                 'historia_klienta', 'czƒôstotliwo≈õƒá_transakcji']\n# Target: 'fraud' (tak/nie)\n\nfraud_rf = RandomForestClassifier(n_estimators=500, class_weight='balanced')\n# Wynik: system antyfaudowy z wysokƒÖ dok≈Çadno≈õciƒÖ\n\n\n\n3) HR - przewidywanie odej≈õƒá pracownik√≥w\n\n# Employee churn prediction\nhr_features = ['satisfaction', 'last_evaluation', 'projects', 'salary', \n              'time_company', 'work_accident', 'promotion']\n# Target: 'left_company' (tak/nie)\n\nhr_rf = RandomForestClassifier(n_estimators=300)\n# Wynik: wczesne ostrze≈ºenia przed odej≈õciami kluczowych pracownik√≥w"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#tuning-parametr√≥w-random-forest",
    "href": "cheatsheets/04-random-forest.html#tuning-parametr√≥w-random-forest",
    "title": "Random Forest ‚Äî las drzew decyzyjnych",
    "section": "‚öôÔ∏è Tuning parametr√≥w Random Forest",
    "text": "‚öôÔ∏è Tuning parametr√≥w Random Forest\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Najwa≈ºniejsze parametry do tuningu\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [5, 10, 15, None],\n    'min_samples_split': [10, 20, 50],\n    'min_samples_leaf': [5, 10, 20],\n    'max_features': ['sqrt', 'log2', None]\n}\n\n# Grid search z cross-validation (uwaga: mo≈ºe potrwaƒá!)\nprint(\"Rozpoczynam Grid Search - mo≈ºe potrwaƒá kilka minut...\")\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_grid,\n    cv=3,  # redukcja CV dla szybko≈õci\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(\"Najlepsze parametry:\")\nprint(grid_search.best_params_)\nprint(f\"Najlepsza dok≈Çadno≈õƒá CV: {grid_search.best_score_:.1%}\")\n\n# Model z najlepszymi parametrami\nbest_rf = grid_search.best_estimator_\nbest_accuracy = accuracy_score(y_test, best_rf.predict(X_test))\nprint(f\"Dok≈Çadno≈õƒá na test set: {best_accuracy:.1%}\")\n\n\n\n\n\n\n\nPoka≈º najlepsze parametry\n\n\n\n\n\n\n\nNajlepsze parametry:\n{'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 50}\nNajlepsza dok≈Çadno≈õƒá CV: 99.0%\nDok≈Çadno≈õƒá na test set: 99.8%"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#pu≈Çapki-i-rozwiƒÖzania",
    "href": "cheatsheets/04-random-forest.html#pu≈Çapki-i-rozwiƒÖzania",
    "title": "Random Forest ‚Äî las drzew decyzyjnych",
    "section": "‚ö†Ô∏è Pu≈Çapki i rozwiƒÖzania",
    "text": "‚ö†Ô∏è Pu≈Çapki i rozwiƒÖzania\n\n1) Overfitting mimo ensemble\n\n# Problem: za g≈Çƒôbokie drzewa mogƒÖ nadal prowadziƒá do overfittingu\noverfitted_rf = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=None,    # bez limitu g≈Çƒôboko≈õci!\n    min_samples_leaf=1  # pozw√≥l na li≈õcie z 1 pr√≥bkƒÖ!\n)\noverfitted_rf.fit(X_train, y_train)\n\ntrain_acc_over = accuracy_score(y_train, overfitted_rf.predict(X_train))\ntest_acc_over = accuracy_score(y_test, overfitted_rf.predict(X_test))\n\nprint(f\"Overfitted Random Forest:\")\nprint(f\"Train accuracy: {train_acc_over:.1%}\")\nprint(f\"Test accuracy: {test_acc_over:.1%}\")\nprint(f\"R√≥≈ºnica: {train_acc_over - test_acc_over:.1%}\")\n\n# RozwiƒÖzanie: odpowiednie parametry\nbalanced_rf = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,      # ogranicz g≈Çƒôboko≈õƒá\n    min_samples_leaf=10  # wymu≈õ wiƒôcej pr√≥bek w li≈õciach\n)\nbalanced_rf.fit(X_train, y_train)\n\ntrain_acc_bal = accuracy_score(y_train, balanced_rf.predict(X_train))\ntest_acc_bal = accuracy_score(y_test, balanced_rf.predict(X_test))\n\nprint(f\"\\nBalanced Random Forest:\")\nprint(f\"Train accuracy: {train_acc_bal:.1%}\")\nprint(f\"Test accuracy: {test_acc_bal:.1%}\")\nprint(f\"R√≥≈ºnica: {train_acc_bal - test_acc_bal:.1%} - znacznie lepiej!\")\n\n\n\n\n\n\n\nPoka≈º wyniki overfittingu\n\n\n\n\n\n\n\nOverfitted Random Forest:\nTrain accuracy: 100.0%\nTest accuracy: 99.8%\nR√≥≈ºnica: 0.2%\n\nBalanced Random Forest:\nTrain accuracy: 99.0%\nTest accuracy: 99.8%\nR√≥≈ºnica: -0.8% - znacznie lepiej!\n\n\n\n\n\n\n\n2) Niezbalansowane klasy\n\n# Problem: gdy jedna klasa jest rzadka (np. 5% fraud√≥w, 95% normalnych transakcji)\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Sprawd≈∫ balans klas w naszych danych\nprint(f\"Rozk≈Çad klas:\")\nprint(f\"Pora≈ºka: {(~y_train).sum()} ({(~y_train).mean():.1%})\")\nprint(f\"Sukces: {y_train.sum()} ({y_train.mean():.1%})\")\n\n# RozwiƒÖzanie 1: class_weight='balanced'\nbalanced_class_rf = RandomForestClassifier(\n    n_estimators=100,\n    class_weight='balanced',  # automatyczne wa≈ºenie klas\n    random_state=42\n)\nbalanced_class_rf.fit(X_train, y_train)\n\n# RozwiƒÖzanie 2: bootstrap sampling\nbalanced_rf = RandomForestClassifier(\n    n_estimators=100,\n    max_samples=0.8,  # u≈ºyj tylko 80% pr√≥bek do ka≈ºdego drzewa\n    random_state=42\n)\nbalanced_rf.fit(X_train, y_train)\n\nprint(\"RozwiƒÖzania zosta≈Çy zaimplementowane!\")\n\n\n\n\n\n\n\nPoka≈º rozk≈Çad klas\n\n\n\n\n\n\n\nRozk≈Çad klas:\nPora≈ºka: 16 (1.0%)\nSukces: 1584 (99.0%)\nRozwiƒÖzania zosta≈Çy zaimplementowane!"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#real-world-przypadki-u≈ºycia",
    "href": "cheatsheets/04-random-forest.html#real-world-przypadki-u≈ºycia",
    "title": "Random Forest ‚Äî las drzew decyzyjnych",
    "section": "üåç Real-world przypadki u≈ºycia",
    "text": "üåç Real-world przypadki u≈ºycia\n\nFinanse: Credit scoring, wykrywanie prania pieniƒôdzy, trading algorytmiczny\nE-commerce: Systemy rekomendacji, dynamic pricing, churn prediction\nHealthcare: Diagnoza medyczna, drug discovery, analiza obraz√≥w medycznych\nMarketing: Customer segmentation, campaign optimization, A/B testing\nCybersecurity: Intrusion detection, malware classification, anomaly detection\n\n\n\n\n\n\n\nüí° Kiedy u≈ºywaƒá Random Forest?\n\n\n\n‚úÖ U≈ªYJ GDY:\n\nPotrzebujesz wysokiej dok≈Çadno≈õci z interpretowalnym modelem\nMasz mixed features (liczbowe + kategoryczne)\nDane zawierajƒÖ missing values (RF radzi sobie z nimi dobrze)\nChcesz feature importance bez complex feature engineering\nPotrzebujesz stabilnego modelu (ma≈Ça wariancja)\n\n‚ùå NIE U≈ªYWAJ GDY:\n\nMasz bardzo du≈ºe datasety (u≈ºyj XGBoost/LightGBM)\nPotrzebujesz bardzo prostego modelu (u≈ºyj Decision Tree)\nTarget jest continuous i potrzebujesz liniowej interpretacji (u≈ºyj Linear Regression)\nMasz bardzo wysokowymiarowe dane (u≈ºyj SVM lub Neural Networks)\n\n\n\nNastƒôpna ≈õciƒÖgawka: K-Means Clustering - grupowanie bez etykiet! üéØ"
  },
  {
    "objectID": "cheatsheets/06-svm.html",
    "href": "cheatsheets/06-svm.html",
    "title": "Support Vector Machines ‚Äî optymalne granice decyzyjne",
    "section": "",
    "text": "SVM to algorytm, kt√≥ry znajduje optymalnƒÖ granicƒô decyzyjnƒÖ miƒôdzy klasami. Zamiast szukaƒá ‚Äújakiejkolwiek‚Äù linii podzia≈Çu, SVM znajduje tƒÖ, kt√≥ra maksymalizuje margines - odleg≈Ço≈õƒá do najbli≈ºszych punkt√≥w z ka≈ºdej klasy.\n\n\n\n\n\n\nüí° Intuicja\n\n\n\nWyobra≈∫ sobie sƒôdziego, kt√≥ry musi wyznaczyƒá granicƒô miƒôdzy dwoma dru≈ºynami. SVM nie tylko znajdzie granicƒô, ale postawi jƒÖ tak, ≈ºeby by≈Ça jak najdalej od najbli≈ºszych zawodnik√≥w z obu stron - dla maksymalnego bezpiecze≈Ñstwa!"
  },
  {
    "objectID": "cheatsheets/06-svm.html#czym-jest-support-vector-machine-svm",
    "href": "cheatsheets/06-svm.html#czym-jest-support-vector-machine-svm",
    "title": "Support Vector Machines ‚Äî optymalne granice decyzyjne",
    "section": "",
    "text": "SVM to algorytm, kt√≥ry znajduje optymalnƒÖ granicƒô decyzyjnƒÖ miƒôdzy klasami. Zamiast szukaƒá ‚Äújakiejkolwiek‚Äù linii podzia≈Çu, SVM znajduje tƒÖ, kt√≥ra maksymalizuje margines - odleg≈Ço≈õƒá do najbli≈ºszych punkt√≥w z ka≈ºdej klasy.\n\n\n\n\n\n\nüí° Intuicja\n\n\n\nWyobra≈∫ sobie sƒôdziego, kt√≥ry musi wyznaczyƒá granicƒô miƒôdzy dwoma dru≈ºynami. SVM nie tylko znajdzie granicƒô, ale postawi jƒÖ tak, ≈ºeby by≈Ça jak najdalej od najbli≈ºszych zawodnik√≥w z obu stron - dla maksymalnego bezpiecze≈Ñstwa!"
  },
  {
    "objectID": "cheatsheets/06-svm.html#praktyczny-przyk≈Çad-klasyfikacja-emaili-spamham",
    "href": "cheatsheets/06-svm.html#praktyczny-przyk≈Çad-klasyfikacja-emaili-spamham",
    "title": "Support Vector Machines ‚Äî optymalne granice decyzyjne",
    "section": "üéØ Praktyczny przyk≈Çad: klasyfikacja emaili spam/ham",
    "text": "üéØ Praktyczny przyk≈Çad: klasyfikacja emaili spam/ham\nCzy email to spam czy prawdziwa wiadomo≈õƒá?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\n\n# Tworzenie realistycznych danych emaili\nnp.random.seed(42)\nn_emails = 2000\n\n# Generuj r√≥≈ºne typy emaili\nemail_types = np.random.choice(['spam', 'ham'], n_emails, p=[0.3, 0.7])\n\ndata = pd.DataFrame({\n    'dlugosc_tematu': np.random.randint(5, 100, n_emails),\n    'liczba_wielkich_liter': np.random.randint(0, 50, n_emails),\n    'liczba_wykrzyknikow': np.random.randint(0, 10, n_emails),\n    'liczba_linkow': np.random.randint(0, 20, n_emails),\n    'slowa_promocyjne': np.random.randint(0, 15, n_emails),\n    'dlugosc_wiadomosci': np.random.randint(50, 5000, n_emails),\n    'godzina_wyslania': np.random.randint(0, 24, n_emails),\n    'typ': email_types\n})\n\n# Realistyczne wzorce dla spam vs ham\nfor i, typ in enumerate(data['typ']):\n    if typ == 'spam':\n        # Spam charakterystyki\n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(20, 80)  # d≈Çu≈ºsze tematy\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(10, 50)  # DU≈ªO WIELKIMI\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(3, 10)  # wiƒôcej !!!\n        data.loc[i, 'liczba_linkow'] = np.random.randint(5, 20)  # du≈ºo link√≥w\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(5, 15)  # PROMOCJA, GRATIS\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(200, 1000)  # ≈õrednie d≈Çugo≈õci\n        data.loc[i, 'godzina_wyslania'] = np.random.choice([2, 3, 22, 23, 1])  # dziwne godziny\n    else:\n        # Ham (prawdziwe) charakterystyki  \n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(5, 40)  # kr√≥tsze tematy\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(0, 8)  # normalne u≈ºywanie\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(0, 3)  # rzadko !\n        data.loc[i, 'liczba_linkow'] = np.random.randint(0, 3)  # ma≈Ço link√≥w\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(0, 2)  # prawie brak\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(50, 3000)  # r√≥≈ºne d≈Çugo≈õci\n        data.loc[i, 'godzina_wyslania'] = np.random.randint(8, 22)  # normalne godziny\n\n# Konwertuj target na binary\ndata['is_spam'] = (data['typ'] == 'spam').astype(int)\n\nprint(\"Statystyki emaili:\")\nprint(data.describe())\nprint(f\"\\nRozk≈Çad spam/ham:\")\nprint(data['typ'].value_counts())\nprint(f\"Procent spam: {data['is_spam'].mean():.1%}\")\n\n\n\n\n\n\n\nPoka≈º statystyki emaili\n\n\n\n\n\n\n\nStatystyki emaili:\n       dlugosc_tematu  liczba_wielkich_liter  liczba_wykrzyknikow  \\\ncount     2000.000000            2000.000000           2000.00000   \nmean        30.482500              11.392500              2.60800   \nstd         18.078609              13.675451              2.69369   \nmin          5.000000               0.000000              0.00000   \n25%         17.000000               2.000000              1.00000   \n50%         27.000000               5.000000              2.00000   \n75%         38.000000              17.000000              4.00000   \nmax         79.000000              49.000000              9.00000   \n\n       liczba_linkow  slowa_promocyjne  dlugosc_wiadomosci  godzina_wyslania  \\\ncount    2000.000000       2000.000000         2000.000000       2000.000000   \nmean        4.360000          3.302500         1240.088000         13.031000   \nstd         5.642279          4.524183          834.469147          6.857445   \nmin         0.000000          0.000000           56.000000          1.000000   \n25%         1.000000          0.000000          539.000000          9.000000   \n50%         2.000000          1.000000          958.000000         14.000000   \n75%         7.000000          6.000000         1905.500000         19.000000   \nmax        19.000000         14.000000         2998.000000         23.000000   \n\n           is_spam  \ncount  2000.000000  \nmean      0.307500  \nstd       0.461574  \nmin       0.000000  \n25%       0.000000  \n50%       0.000000  \n75%       1.000000  \nmax       1.000000  \n\nRozk≈Çad spam/ham:\ntyp\nham     1385\nspam     615\nName: count, dtype: int64\nProcent spam: 30.8%"
  },
  {
    "objectID": "cheatsheets/06-svm.html#budowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/06-svm.html#budowanie-modelu-krok-po-kroku",
    "title": "Support Vector Machines ‚Äî optymalne granice decyzyjne",
    "section": "üîß Budowanie modelu krok po kroku",
    "text": "üîß Budowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Features do modelu\nfeatures = ['dlugosc_tematu', 'liczba_wielkich_liter', 'liczba_wykrzyknikow', \n           'liczba_linkow', 'slowa_promocyjne', 'dlugosc_wiadomosci', 'godzina_wyslania']\nX = data[features]\ny = data['is_spam']\n\n# Podzia≈Ç train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standaryzacja - KLUCZOWA dla SVM!\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"Dane treningowe: {len(X_train)} emaili\")\nprint(f\"Dane testowe: {len(X_test)} emaili\")\nprint(f\"Spam w train: {y_train.mean():.1%}\")\nprint(f\"Spam w test: {y_test.mean():.1%}\")\n\nprint(\"\\nPrzyk≈Çad standaryzacji:\")\nprint(\"Przed:\", X_train.iloc[0].values)\nprint(\"Po:\", X_train_scaled[0])\n\n\n\n\n\n\n\nPoka≈º podzia≈Ç danych\n\n\n\n\n\n\n\nDane treningowe: 1600 emaili\nDane testowe: 400 emaili\nSpam w train: 30.9%\nSpam w test: 30.0%\n\nPrzyk≈Çad standaryzacji:\nPrzed: [   6    1    0    2    0 2057   21]\nPo: [-1.36651173 -0.75972497 -0.9773396  -0.41945283 -0.73519846  1.00688364\n  1.1681456 ]\n\n\n\n\n\n\n\n2) Trenowanie r√≥≈ºnych kerneli SVM\n\n# Por√≥wnanie r√≥≈ºnych kerneli\nkernels = ['linear', 'rbf', 'poly']\nsvm_results = {}\n\nfor kernel in kernels:\n    print(f\"\\nTrenowanie SVM z kernelem: {kernel}\")\n    \n    if kernel == 'poly':\n        svm = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm = SVC(kernel=kernel, random_state=42)\n    \n    # Trenowanie\n    svm.fit(X_train_scaled, y_train)\n    \n    # Predykcje\n    y_pred = svm.predict(X_test_scaled)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    svm_results[kernel] = {\n        'model': svm,\n        'accuracy': accuracy,\n        'predictions': y_pred\n    }\n    \n    print(f\"Dok≈Çadno≈õƒá: {accuracy:.1%}\")\n    print(f\"Liczba support vectors: {svm.n_support_}\")\n\n# Znajd≈∫ najlepszy kernel\nbest_kernel = max(svm_results.keys(), key=lambda k: svm_results[k]['accuracy'])\nbest_model = svm_results[best_kernel]['model']\n\nprint(f\"\\nüèÜ Najlepszy kernel: {best_kernel}\")\nprint(f\"Najlepsza dok≈Çadno≈õƒá: {svm_results[best_kernel]['accuracy']:.1%}\")\n\n\n\n\n\n\n\nPoka≈º wyniki r√≥≈ºnych kerneli\n\n\n\n\n\n\n\n\nKernel linear:\nDok≈Çadno≈õƒá: 100.0%\nLiczba support vectors: [2 3]\n\nKernel rbf:\nDok≈Çadno≈õƒá: 100.0%\nLiczba support vectors: [11 26]\n\nKernel poly:\nDok≈Çadno≈õƒá: 100.0%\nLiczba support vectors: [16 16]\n\nüèÜ Najlepszy kernel: linear\nNajlepsza dok≈Çadno≈õƒá: 100.0%\n\n\n\n\n\n\n\n3) Ewaluacja najlepszego modelu\n\n# Szczeg√≥≈Çowa analiza najlepszego modelu\nbest_predictions = svm_results[best_kernel]['predictions']\n\nprint(f\"SZCZEG√ì≈ÅOWA ANALIZA - SVM {best_kernel.upper()}\")\nprint(\"=\" * 50)\n\n# Classification report\nprint(\"\\nRaport klasyfikacji:\")\nprint(classification_report(y_test, best_predictions, target_names=['Ham', 'Spam']))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, best_predictions)\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:     Ham    Spam\")\nprint(f\"Rzeczywiste Ham:  {cm[0,0]:3d}    {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Spam: {cm[1,0]:3d}    {cm[1,1]:3d}\")\n\n# Analiza b≈Çƒôd√≥w\nfalse_positives = np.where((y_test == 0) & (best_predictions == 1))[0]\nfalse_negatives = np.where((y_test == 1) & (best_predictions == 0))[0]\n\nprint(f\"\\nAnaliza b≈Çƒôd√≥w:\")\nprint(f\"False Positives (Ham ‚Üí Spam): {len(false_positives)}\")\nprint(f\"False Negatives (Spam ‚Üí Ham): {len(false_negatives)}\")\n\nif len(false_positives) &gt; 0:\n    print(f\"\\nPrzyk≈Çad b≈Çƒôdnie sklasyfikowanego Ham jako Spam:\")\n    fp_idx = false_positives[0]\n    print(f\"Email: {X_test.iloc[fp_idx].to_dict()}\")\n\n\n\n\n\n\n\nPoka≈º szczeg√≥≈ÇowƒÖ analizƒô\n\n\n\n\n\n\n\nSZCZEG√ì≈ÅOWA ANALIZA - SVM LINEAR\n==================================================\n\nRaport klasyfikacji:\n              precision    recall  f1-score   support\n\n         Ham       1.00      1.00      1.00       280\n        Spam       1.00      1.00      1.00       120\n\n    accuracy                           1.00       400\n   macro avg       1.00      1.00      1.00       400\nweighted avg       1.00      1.00      1.00       400\n\n\nConfusion Matrix:\nPrzewidywane:     Ham    Spam\nRzeczywiste Ham:  280      0\nRzeczywiste Spam:   0    120\n\nAnaliza b≈Çƒôd√≥w:\nFalse Positives (Ham ‚Üí Spam): 0\nFalse Negatives (Spam ‚Üí Ham): 0"
  },
  {
    "objectID": "cheatsheets/06-svm.html#wizualizacja-granic-decyzyjnych",
    "href": "cheatsheets/06-svm.html#wizualizacja-granic-decyzyjnych",
    "title": "Support Vector Machines ‚Äî optymalne granice decyzyjne",
    "section": "üìä Wizualizacja granic decyzyjnych",
    "text": "üìä Wizualizacja granic decyzyjnych\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Przygotuj dane (powtarzamy dla kompletno≈õci)\nnp.random.seed(42)\nn_emails = 2000\n\nemail_types = np.random.choice(['spam', 'ham'], n_emails, p=[0.3, 0.7])\n\ndata = pd.DataFrame({\n    'dlugosc_tematu': np.random.randint(5, 100, n_emails),\n    'liczba_wielkich_liter': np.random.randint(0, 50, n_emails),\n    'liczba_wykrzyknikow': np.random.randint(0, 10, n_emails),\n    'liczba_linkow': np.random.randint(0, 20, n_emails),\n    'slowa_promocyjne': np.random.randint(0, 15, n_emails),\n    'dlugosc_wiadomosci': np.random.randint(50, 5000, n_emails),\n    'godzina_wyslania': np.random.randint(0, 24, n_emails),\n    'typ': email_types\n})\n\n# Popraw charakterystyki\nfor i, typ in enumerate(data['typ']):\n    if typ == 'spam':\n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(20, 80)\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(10, 50)\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(3, 10)\n        data.loc[i, 'liczba_linkow'] = np.random.randint(5, 20)\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(5, 15)\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(200, 1000)\n        data.loc[i, 'godzina_wyslania'] = np.random.choice([2, 3, 22, 23, 1])\n    else:\n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(5, 40)\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(0, 8)\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(0, 3)\n        data.loc[i, 'liczba_linkow'] = np.random.randint(0, 3)\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(0, 2)\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(50, 3000)\n        data.loc[i, 'godzina_wyslania'] = np.random.randint(8, 22)\n\ndata['is_spam'] = (data['typ'] == 'spam').astype(int)\n\n# Wybierz tylko 2 features dla wizualizacji 2D\nfeatures_2d = ['liczba_wielkich_liter', 'liczba_wykrzyknikow']\nX_2d = data[features_2d]\ny = data['is_spam']\n\nX_train_2d, X_test_2d, y_train, y_test = train_test_split(X_2d, y, test_size=0.2, random_state=42)\n\nscaler_2d = StandardScaler()\nX_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)\nX_test_2d_scaled = scaler_2d.transform(X_test_2d)\n\n# Trenuj modele z r√≥≈ºnymi kernelami (na 2D)\nkernels = ['linear', 'rbf', 'poly']\nplt.figure(figsize=(15, 5))\n\nfor i, kernel in enumerate(kernels):\n    plt.subplot(1, 3, i+1)\n    \n    # Trenuj model\n    if kernel == 'poly':\n        svm = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm = SVC(kernel=kernel, random_state=42)\n    \n    svm.fit(X_train_2d_scaled, y_train)\n    \n    # Stw√≥rz mesh do wizualizacji granic\n    h = 0.02\n    x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1\n    y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    # Predykcje na mesh\n    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    # Narysuj granice i punkty\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    scatter = plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], \n                         c=y_train, cmap=plt.cm.RdYlBu, edgecolors='black')\n    \n    # Pod≈õwietl support vectors\n    if hasattr(svm, 'support_'):\n        plt.scatter(X_train_2d_scaled[svm.support_, 0], \n                   X_train_2d_scaled[svm.support_, 1],\n                   s=100, facecolors='none', edgecolors='black', linewidth=2)\n    \n    plt.xlabel('Liczba wielkich liter (scaled)')\n    plt.ylabel('Liczba wykrzyknik√≥w (scaled)')\n    plt.title(f'SVM - {kernel.upper()} kernel')\n    \n    # Dok≈Çadno≈õƒá\n    accuracy = accuracy_score(y_test, svm.predict(X_test_2d_scaled))\n    plt.text(0.02, 0.98, f'Accuracy: {accuracy:.1%}', \n             transform=plt.gca().transAxes, \n             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n             verticalalignment='top')\n\nplt.tight_layout()\nplt.close()\n\n# Przygotuj dane dla pe≈Çnego modelu\nfeatures = ['dlugosc_tematu', 'liczba_wielkich_liter', 'liczba_wykrzyknikow', \n           'liczba_linkow', 'slowa_promocyjne', 'dlugosc_wiadomosci', 'godzina_wyslania']\nX_full = data[features]\nX_train_full, X_test_full, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42)\n\nscaler_full = StandardScaler()\nX_train_full_scaled = scaler_full.fit_transform(X_train_full)\nX_test_full_scaled = scaler_full.transform(X_test_full)\n\n# Por√≥wnaj wyniki\nprint(\"POR√ìWNANIE KERNELI (2D vs 7D features):\")\nprint(\"=\" * 50)\n\nfor kernel in kernels:\n    # 2D model\n    if kernel == 'poly':\n        svm_2d = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm_2d = SVC(kernel=kernel, random_state=42)\n    svm_2d.fit(X_train_2d_scaled, y_train)\n    acc_2d = accuracy_score(y_test, svm_2d.predict(X_test_2d_scaled))\n    \n    # 7D model\n    if kernel == 'poly':\n        svm_7d = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm_7d = SVC(kernel=kernel, random_state=42)\n    svm_7d.fit(X_train_full_scaled, y_train)\n    acc_7d = accuracy_score(y_test, svm_7d.predict(X_test_full_scaled))\n    \n    print(f\"{kernel.upper():6} - 2D: {acc_2d:.1%}, 7D: {acc_7d:.1%}, Poprawa: +{(acc_7d-acc_2d)*100:.1f}pp\")\n\n# Odtw√≥rz wizualizacjƒô\nplt.figure(figsize=(15, 5))\n\nfor i, kernel in enumerate(kernels):\n    plt.subplot(1, 3, i+1)\n    \n    if kernel == 'poly':\n        svm = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm = SVC(kernel=kernel, random_state=42)\n    \n    svm.fit(X_train_2d_scaled, y_train)\n    \n    h = 0.02\n    x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1\n    y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    \n    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    scatter = plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], \n                         c=y_train, cmap=plt.cm.RdYlBu, edgecolors='black')\n    \n    if hasattr(svm, 'support_'):\n        plt.scatter(X_train_2d_scaled[svm.support_, 0], \n                   X_train_2d_scaled[svm.support_, 1],\n                   s=100, facecolors='none', edgecolors='black', linewidth=2)\n    \n    plt.xlabel('Liczba wielkich liter (scaled)')\n    plt.ylabel('Liczba wykrzyknik√≥w (scaled)')\n    plt.title(f'SVM - {kernel.upper()} kernel')\n    \n    accuracy = accuracy_score(y_test, svm.predict(X_test_2d_scaled))\n    plt.text(0.02, 0.98, f'Accuracy: {accuracy:.1%}', \n             transform=plt.gca().transAxes, \n             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n             verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nPoka≈º wizualizacjƒô granic i por√≥wnanie\n\n\n\n\n\n\n\nPOR√ìWNANIE KERNELI (2D vs 7D features):\n==================================================\nLINEAR - 2D: 100.0%, 7D: 100.0%, Poprawa: +0.0pp\nRBF    - 2D: 100.0%, 7D: 100.0%, Poprawa: +0.0pp\nPOLY   - 2D: 98.2%, 7D: 100.0%, Poprawa: +1.7pp\n\n\n\n\n\nWizualizacja granic decyzyjnych SVM dla r√≥≈ºnych kerneli"
  },
  {
    "objectID": "cheatsheets/06-svm.html#rodzaje-kerneli-i-ich-zastosowania",
    "href": "cheatsheets/06-svm.html#rodzaje-kerneli-i-ich-zastosowania",
    "title": "Support Vector Machines ‚Äî optymalne granice decyzyjne",
    "section": "üéØ Rodzaje kerneli i ich zastosowania",
    "text": "üéØ Rodzaje kerneli i ich zastosowania\n\n1) Linear Kernel - proste granice\n\n# U≈ºyj gdy:\n# - Dane sƒÖ liniowo separowalne\n# - Masz du≈ºo features (high-dimensional)\n# - Potrzebujesz szybkiego modelu\n# - Chcesz interpretowalno≈õƒá\n\nlinear_use_cases = [\n    \"Text classification (high-dimensional sparse data)\",\n    \"Gene expression analysis\", \n    \"Document categorization\",\n    \"Sentiment analysis z bag-of-words\"\n]\n\nprint(\"LINEAR KERNEL - najlepsze zastosowania:\")\nfor i, use_case in enumerate(linear_use_cases, 1):\n    print(f\"{i}. {use_case}\")\n\n# Demo: Linear SVM na prostych danych\nX_simple = np.random.randn(100, 2)\ny_simple = (X_simple[:, 0] + X_simple[:, 1] &gt; 0).astype(int)\n\nlinear_svm = SVC(kernel='linear')\nlinear_svm.fit(X_simple, y_simple)\nprint(f\"\\nLinear SVM na prostych danych: {linear_svm.score(X_simple, y_simple):.1%}\")\n\n\n\n\n\n\n\nPoka≈º zastosowania Linear Kernel\n\n\n\n\n\n\n\nLINEAR KERNEL - najlepsze zastosowania:\n1. Text classification (high-dimensional sparse data)\n2. Gene expression analysis\n3. Document categorization\n4. Sentiment analysis z bag-of-words\n\nLinear SVM na prostych danych: 98.0%\n\n\n\n\n\n\n\n2) RBF Kernel - uniwersalny wyb√≥r\n\n# RBF (Radial Basis Function) - najczƒô≈õciej u≈ºywany\n# Mo≈ºe aproksymowaƒá dowolnƒÖ granicƒô decyzyjnƒÖ!\n\nrbf_use_cases = [\n    \"Image classification\",\n    \"Medical diagnosis\", \n    \"Fraud detection\",\n    \"Customer segmentation\",\n    \"General classification tasks\"\n]\n\nprint(\"RBF KERNEL - najlepsze zastosowania:\")\nfor i, use_case in enumerate(rbf_use_cases, 1):\n    print(f\"{i}. {use_case}\")\n\n# Demo: wp≈Çyw parametru gamma\ngammas = [0.1, 1.0, 10.0]\nprint(f\"\\nWp≈Çyw parametru gamma w RBF:\")\n\nfor gamma in gammas:\n    rbf_svm = SVC(kernel='rbf', gamma=gamma, random_state=42)\n    rbf_svm.fit(X_train_full_scaled, y_train)\n    accuracy = rbf_svm.score(X_test_full_scaled, y_test)\n    print(f\"Gamma {gamma:4.1f}: Accuracy = {accuracy:.1%}, Support Vectors = {rbf_svm.n_support_.sum()}\")\n\nprint(\"\\nüí° Gamma wysoka = bardziej skomplikowane granice\")\nprint(\"üí° Gamma niska = g≈Çadsze granice\")\n\n\n\n\n\n\n\nPoka≈º zastosowania RBF Kernel\n\n\n\n\n\n\n\nRBF KERNEL - najlepsze zastosowania:\n1. Image classification\n2. Medical diagnosis\n3. Fraud detection\n4. Customer segmentation\n5. General classification tasks\n\nWp≈Çyw parametru gamma w RBF:\nGamma  0.1: Accuracy = 100.0%, Support Vectors = 26\nGamma  1.0: Accuracy = 100.0%, Support Vectors = 374\nGamma 10.0: Accuracy = 100.0%, Support Vectors = 1546\n\nüí° Gamma wysoka = bardziej skomplikowane granice\nüí° Gamma niska = g≈Çadsze granice\n\n\n\n\n\n\n\n3) Polynomial Kernel - dla z≈Ço≈ºonych wzorc√≥w\n\npoly_use_cases = [\n    \"Computer vision (shape recognition)\",\n    \"Bioinformatics (protein classification)\",\n    \"Natural language processing\",\n    \"Pattern recognition\"\n]\n\nprint(\"POLYNOMIAL KERNEL - najlepsze zastosowania:\")\nfor i, use_case in enumerate(poly_use_cases, 1):\n    print(f\"{i}. {use_case}\")\n\n# Demo: wp≈Çyw stopnia wielomianu\ndegrees = [2, 3, 4, 5]\nprint(f\"\\nWp≈Çyw stopnia wielomianu:\")\n\nfor degree in degrees:\n    poly_svm = SVC(kernel='poly', degree=degree, random_state=42)\n    poly_svm.fit(X_train_full_scaled, y_train)\n    accuracy = poly_svm.score(X_test_full_scaled, y_test)\n    print(f\"Degree {degree}: Accuracy = {accuracy:.1%}\")\n\nprint(\"\\n‚ö†Ô∏è Uwaga: wy≈ºszie stopnie = ryzyko overfittingu!\")\n\n\n\n\n\n\n\nPoka≈º zastosowania Polynomial Kernel\n\n\n\n\n\n\n\nPOLYNOMIAL KERNEL - najlepsze zastosowania:\n1. Computer vision (shape recognition)\n2. Bioinformatics (protein classification)\n3. Natural language processing\n4. Pattern recognition\n\nWp≈Çyw stopnia wielomianu:\nDegree 2: Accuracy = 99.8%\nDegree 3: Accuracy = 100.0%\nDegree 4: Accuracy = 99.5%\nDegree 5: Accuracy = 100.0%\n\n‚ö†Ô∏è Uwaga: wy≈ºszie stopnie = ryzyko overfittingu!"
  },
  {
    "objectID": "cheatsheets/06-svm.html#tuning-parametr√≥w-svm",
    "href": "cheatsheets/06-svm.html#tuning-parametr√≥w-svm",
    "title": "Support Vector Machines ‚Äî optymalne granice decyzyjne",
    "section": "‚öôÔ∏è Tuning parametr√≥w SVM",
    "text": "‚öôÔ∏è Tuning parametr√≥w SVM\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Grid Search dla RBF kernel\nparam_grid_rbf = {\n    'C': [0.1, 1, 10, 100],           # regularization strength\n    'gamma': [0.001, 0.01, 0.1, 1]   # kernel coefficient\n}\n\nprint(\"Rozpoczynam Grid Search dla RBF SVM...\")\ngrid_search = GridSearchCV(\n    SVC(kernel='rbf', random_state=42),\n    param_grid_rbf,\n    cv=3,  # 3-fold CV dla szybko≈õci\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=0\n)\n\ngrid_search.fit(X_train_full_scaled, y_train)\n\nprint(\"Najlepsze parametry RBF:\")\nprint(grid_search.best_params_)\nprint(f\"Najlepsza dok≈Çadno≈õƒá CV: {grid_search.best_score_:.1%}\")\n\n# Test na zbiorze testowym\nbest_svm = grid_search.best_estimator_\ntest_accuracy = best_svm.score(X_test_full_scaled, y_test)\nprint(f\"Dok≈Çadno≈õƒá na test set: {test_accuracy:.1%}\")\n\n# Por√≥wnanie przed/po tuningu\nbaseline_svm = SVC(kernel='rbf', random_state=42)\nbaseline_svm.fit(X_train_full_scaled, y_train)\nbaseline_accuracy = baseline_svm.score(X_test_full_scaled, y_test)\n\nprint(f\"\\nPor√≥wnanie:\")\nprint(f\"Baseline RBF:      {baseline_accuracy:.1%}\")\nprint(f\"Tuned RBF:         {test_accuracy:.1%}\")\nprint(f\"Poprawa:           +{(test_accuracy - baseline_accuracy)*100:.1f}pp\")\n\n\n\n\n\n\n\nPoka≈º wyniki tuningu\n\n\n\n\n\n\n\nNajlepsze parametry RBF:\n{'C': 0.1, 'gamma': 0.01}\nNajlepsza dok≈Çadno≈õƒá CV: 100.0%\nDok≈Çadno≈õƒá na test set: 100.0%\n\nPor√≥wnanie:\nBaseline RBF:      100.0%\nTuned RBF:         100.0%\nPoprawa:           +0.0pp"
  },
  {
    "objectID": "cheatsheets/06-svm.html#pu≈Çapki-i-rozwiƒÖzania",
    "href": "cheatsheets/06-svm.html#pu≈Çapki-i-rozwiƒÖzania",
    "title": "Support Vector Machines ‚Äî optymalne granice decyzyjne",
    "section": "‚ö†Ô∏è Pu≈Çapki i rozwiƒÖzania",
    "text": "‚ö†Ô∏è Pu≈Çapki i rozwiƒÖzania\n\n1) Brak standaryzacji\n\n# Problem: SVM jest bardzo wra≈ºliwy na skale danych\nprint(\"DEMONSTRACJA: wp≈Çyw standaryzacji\")\n\n# Bez standaryzacji\nsvm_unscaled = SVC(kernel='rbf', random_state=42)\nsvm_unscaled.fit(X_train_full, y_train)  # raw data!\nacc_unscaled = svm_unscaled.score(X_test_full, y_test)\n\n# Ze standaryzacjƒÖ\nsvm_scaled = SVC(kernel='rbf', random_state=42)\nsvm_scaled.fit(X_train_full_scaled, y_train)  # scaled data!\nacc_scaled = svm_scaled.score(X_test_full_scaled, y_test)\n\nprint(f\"Bez standaryzacji: {acc_unscaled:.1%}\")\nprint(f\"Ze standaryzacjƒÖ:  {acc_scaled:.1%}\")\nprint(f\"Poprawa:           +{(acc_scaled - acc_unscaled)*100:.1f}pp\")\nprint(\"\\n‚úÖ ZAWSZE standaryzuj dane przed SVM!\")\n\n\n\n\n\n\n\nPoka≈º wp≈Çyw standaryzacji\n\n\n\n\n\n\n\nBez standaryzacji: 88.0%\nZe standaryzacjƒÖ:  100.0%\nPoprawa:           +12.0pp\n\n‚úÖ ZAWSZE standaryzuj dane przed SVM!\n\n\n\n\n\n\n\n2) Niezbalansowane klasy\n\n# Problem: gdy jedna klasa jest bardzo rzadka\nprint(\"PROBLEM: niezbalansowane klasy\")\n\n# Sprawd≈∫ balans w naszych danych\nprint(f\"Balans klas: Spam {y_train.mean():.1%}, Ham {(1-y_train.mean()):.1%}\")\n\n# RozwiƒÖzanie 1: class_weight='balanced'\nsvm_balanced = SVC(kernel='rbf', class_weight='balanced', random_state=42)\nsvm_balanced.fit(X_train_full_scaled, y_train)\n\n# RozwiƒÖzanie 2: manual class weights\nspam_weight = len(y_train) / (2 * y_train.sum())  # 1/frequency\nham_weight = len(y_train) / (2 * (len(y_train) - y_train.sum()))\nclass_weights = {0: ham_weight, 1: spam_weight}\n\nsvm_manual = SVC(kernel='rbf', class_weight=class_weights, random_state=42)\nsvm_manual.fit(X_train_full_scaled, y_train)\n\n# Por√≥wnanie\nsvm_normal = SVC(kernel='rbf', random_state=42)\nsvm_normal.fit(X_train_full_scaled, y_train)\n\nmodels = {\n    'Normal': svm_normal,\n    'Balanced': svm_balanced, \n    'Manual weights': svm_manual\n}\n\nprint(\"\\nPor√≥wnanie podej≈õƒá:\")\nfor name, model in models.items():\n    pred = model.predict(X_test_full_scaled)\n    accuracy = accuracy_score(y_test, pred)\n    \n    # Sprawd≈∫ recall dla spam (wa≈ºne!)\n    spam_indices = y_test == 1\n    spam_recall = np.mean(pred[spam_indices] == 1) if spam_indices.sum() &gt; 0 else 0\n    \n    print(f\"{name:12}: Accuracy={accuracy:.1%}, Spam Recall={spam_recall:.1%}\")\n\n\n\n\n\n\n\nPoka≈º rozwiƒÖzania dla niezbalansowanych klas\n\n\n\n\n\n\n\nBalans klas: Spam 30.9%, Ham 69.1%\n\nPor√≥wnanie podej≈õƒá:\nNormal      : Accuracy=100.0%, Spam Recall=100.0%\nBalanced    : Accuracy=100.0%, Spam Recall=100.0%\nManual weights: Accuracy=100.0%, Spam Recall=100.0%\n\n\n\n\n\n\n\n3) Overfitting z wysokim C\n\n# Problem: zbyt wysoka warto≈õƒá C prowadzi do overfittingu\nC_values = [0.01, 0.1, 1, 10, 100, 1000]\nresults = []\n\nprint(\"Wp≈Çyw parametru C (regularization):\")\nprint(\"C      | Train Acc | Test Acc | Difference | Support Vectors\")\nprint(\"-\" * 60)\n\nfor C in C_values:\n    svm = SVC(kernel='rbf', C=C, random_state=42)\n    svm.fit(X_train_full_scaled, y_train)\n    \n    train_acc = svm.score(X_train_full_scaled, y_train)\n    test_acc = svm.score(X_test_full_scaled, y_test)\n    diff = train_acc - test_acc\n    n_sv = svm.n_support_.sum()\n    \n    print(f\"{C:6.2f} | {train_acc:8.1%} | {test_acc:7.1%} | {diff:9.1%} | {n_sv:14d}\")\n    results.append((C, train_acc, test_acc, diff, n_sv))\n\nprint(\"\\nüí° Optimalne C: wysokie test accuracy, ma≈Ça r√≥≈ºnica train-test\")\nprint(\"üí° Zbyt wysokie C: perfect train accuracy, s≈Çabe test accuracy\")\n\n\n\n\n\n\n\nPoka≈º wp≈Çyw parametru C\n\n\n\n\n\n\n\nWp≈Çyw parametru C (regularization):\nC      | Train Acc | Test Acc | Difference | Support Vectors\n------------------------------------------------------------\n  0.01 |   100.0% |  100.0% |      0.0% |            582\n  0.10 |   100.0% |  100.0% |      0.0% |             99\n  1.00 |   100.0% |  100.0% |      0.0% |             37\n 10.00 |   100.0% |  100.0% |      0.0% |             35\n100.00 |   100.0% |  100.0% |      0.0% |             35\n1000.00 |   100.0% |  100.0% |      0.0% |             35\n\nüí° Optimalne C: wysokie test accuracy, ma≈Ça r√≥≈ºnica train-test\nüí° Zbyt wysokie C: perfect train accuracy, s≈Çabe test accuracy"
  },
  {
    "objectID": "cheatsheets/06-svm.html#real-world-przypadki-u≈ºycia",
    "href": "cheatsheets/06-svm.html#real-world-przypadki-u≈ºycia",
    "title": "Support Vector Machines ‚Äî optymalne granice decyzyjne",
    "section": "üåç Real-world przypadki u≈ºycia",
    "text": "üåç Real-world przypadki u≈ºycia\n\nText Classification: Spam detection, sentiment analysis, document categorization\nComputer Vision: Image classification, face recognition, medical image analysis\nFinance: Credit scoring, algorithmic trading, fraud detection\nHealthcare: Disease diagnosis, drug discovery, medical image analysis\nCybersecurity: Intrusion detection, malware classification, network security\n\n\n\n\n\n\n\nüí° Kiedy u≈ºywaƒá SVM?\n\n\n\n‚úÖ U≈ªYJ GDY:\n\nPotrzebujesz wysokiej dok≈Çadno≈õci klasyfikacji\nMasz ≈õredniej wielko≈õci dataset (1K-100K samples)\nDane majƒÖ clear margins miƒôdzy klasami\nNie potrzebujesz probabilistic outputs\nMemory efficiency jest wa≈ºna\n\n‚ùå NIE U≈ªYWAJ GDY:\n\nMasz bardzo du≈ºe datasety (&gt;100K samples - u≈ºyj Neural Networks)\nPotrzebujesz probabilistic predictions (u≈ºyj Logistic Regression)\nDane majƒÖ du≈ºo noise‚Äôu (u≈ºyj Random Forest)\nTarget jest continuous (u≈ºyj SVR - Support Vector Regression)\nPotrzebujesz bardzo szybkiego inference (u≈ºyj Decision Tree)\n\n\n\nPodsumowanie: SVM to potƒô≈ºny algoritm do klasyfikacji z optymalnƒÖ separacjƒÖ klas. Wybierz kernel w zale≈ºno≈õci od z≈Ço≈ºono≈õci danych: Linear dla prostych, RBF dla uniwersalnych, Polynomial dla specjalnych przypadk√≥w. üéØ"
  }
]