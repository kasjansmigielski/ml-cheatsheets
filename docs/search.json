[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Witaj w świecie Machine Learning!",
    "section": "",
    "text": "Witaj w praktycznej kolekcji ściąg z Machine Learning! Znajdziesz tutaj algorytmy, teorię i real-world przykłady przedstawione w przystępny sposób dla kursantów Data Science. Każda ściągawka to kompaktowa porcja wiedzy gotowa do zastosowania.\n\n\n\nWyjaśnienie algorytmu - jak działa i dlaczego\nPraktyczne przykłady kodu gotowe do skopiowania i uruchomienia\nReal-world zastosowania - kiedy i gdzie użyć\nKluczowe parametry - co można dostroić i jak\nTipsy od praktyków - na co uważać w rzeczywistych projektach\n\n\n\n\nNajlepsza metoda nauki:\n\nPrzeczytaj teorię - zrozum podstawy algorytmu\nPrzeanalizuj przykład - zobacz jak teoria przekłada się na kod\n\nSkopiuj snippety - użyj przyciska 📋 Copy w prawym górnym rogu\nTestuj w swoim środowisku - Jupyter, Colab, VS Code\nEksperymentuj z parametrami - zmień wartości i obserwuj efekty!\n\n\n\n\n\n\n\n💡 Pro tip dla kursantów\n\n\n\nNie tylko kopiuj kod - zrozum co robi każda linia. Spróbuj zmienić dane wejściowe, parametry algorytmu lub dodać własne modyfikacje. To najlepszy sposób na naukę ML!\n\n\n\n\n\n\n\n\nWprowadzenie do ML - Czym jest ML i jakie są główne rodzaje algorytmów\n\n\n\n\n\nLinear Regression - Predykcja wartości liczbowych\n\n\n\n\n\nDecision Trees - Proste drzewa decyzyjne\n\n\n\n\n\n\n\n🔥 Kolejne ściągi w przygotowaniu!\n\n\n\n\nK-Means Clustering (segmentacja klientów)\nRandom Forest (ensembles)\nLogistic Regression (klasyfikacja binarna)\nFeature Engineering (przygotowanie danych)\nModel Evaluation (metryki i walidacja)\n\nŚledź aktualizacje!\n\n\n\n\n\n\n\nZacznij od podstaw - Wprowadzenie do ML\nWybierz algorytm który Cię interesuje z menu ML Cheatsheets\n\nPrzeczytaj teorię - zrozum jak działa algorytm\nSkopiuj kod (ikona 📋 Copy w prawym górnym rogu bloku)\nTestuj w praktyce - uruchom w Jupyter/Colab i eksperymentuj!\n\n\nPowodzenia w nauce Machine Learning! 🚀🤖"
  },
  {
    "objectID": "index.html#co-znajdziesz-w-każdej-ściągawce",
    "href": "index.html#co-znajdziesz-w-każdej-ściągawce",
    "title": "Witaj w świecie Machine Learning!",
    "section": "",
    "text": "Wyjaśnienie algorytmu - jak działa i dlaczego\nPraktyczne przykłady kodu gotowe do skopiowania i uruchomienia\nReal-world zastosowania - kiedy i gdzie użyć\nKluczowe parametry - co można dostroić i jak\nTipsy od praktyków - na co uważać w rzeczywistych projektach"
  },
  {
    "objectID": "index.html#jak-korzystać-z-ml-cheatsheets",
    "href": "index.html#jak-korzystać-z-ml-cheatsheets",
    "title": "Witaj w świecie Machine Learning!",
    "section": "",
    "text": "Najlepsza metoda nauki:\n\nPrzeczytaj teorię - zrozum podstawy algorytmu\nPrzeanalizuj przykład - zobacz jak teoria przekłada się na kod\n\nSkopiuj snippety - użyj przyciska 📋 Copy w prawym górnym rogu\nTestuj w swoim środowisku - Jupyter, Colab, VS Code\nEksperymentuj z parametrami - zmień wartości i obserwuj efekty!\n\n\n\n\n\n\n\n💡 Pro tip dla kursantów\n\n\n\nNie tylko kopiuj kod - zrozum co robi każda linia. Spróbuj zmienić dane wejściowe, parametry algorytmu lub dodać własne modyfikacje. To najlepszy sposób na naukę ML!"
  },
  {
    "objectID": "index.html#dostępne-ściągi-ml",
    "href": "index.html#dostępne-ściągi-ml",
    "title": "Witaj w świecie Machine Learning!",
    "section": "",
    "text": "Wprowadzenie do ML - Czym jest ML i jakie są główne rodzaje algorytmów\n\n\n\n\n\nLinear Regression - Predykcja wartości liczbowych\n\n\n\n\n\nDecision Trees - Proste drzewa decyzyjne\n\n\n\n\n\n\n\n🔥 Kolejne ściągi w przygotowaniu!\n\n\n\n\nK-Means Clustering (segmentacja klientów)\nRandom Forest (ensembles)\nLogistic Regression (klasyfikacja binarna)\nFeature Engineering (przygotowanie danych)\nModel Evaluation (metryki i walidacja)\n\nŚledź aktualizacje!"
  },
  {
    "objectID": "index.html#szybki-start",
    "href": "index.html#szybki-start",
    "title": "Witaj w świecie Machine Learning!",
    "section": "",
    "text": "Zacznij od podstaw - Wprowadzenie do ML\nWybierz algorytm który Cię interesuje z menu ML Cheatsheets\n\nPrzeczytaj teorię - zrozum jak działa algorytm\nSkopiuj kod (ikona 📋 Copy w prawym górnym rogu bloku)\nTestuj w praktyce - uruchom w Jupyter/Colab i eksperymentuj!\n\n\nPowodzenia w nauce Machine Learning! 🚀🤖"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html",
    "href": "cheatsheets/02-linear-regression.html",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "",
    "text": "Linear Regression to jeden z najprostszych i najczęściej używanych algorytmów ML do przewidywania wartości liczbowych (np. ceny, temperatury, sprzedaży). Szuka najlepszej linii prostej, która opisuje zależność między zmiennymi.\n\n\n\n\n\n\n💡 Intuicja matematyczna\n\n\n\nSzukamy takiej linii y = ax + b, która najlepiej “przechodzi” przez nasze punkty danych. Algorytm automatycznie dobiera optymalne wartości a (slope) i b (intercept)."
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#czym-jest-linear-regression",
    "href": "cheatsheets/02-linear-regression.html#czym-jest-linear-regression",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "",
    "text": "Linear Regression to jeden z najprostszych i najczęściej używanych algorytmów ML do przewidywania wartości liczbowych (np. ceny, temperatury, sprzedaży). Szuka najlepszej linii prostej, która opisuje zależność między zmiennymi.\n\n\n\n\n\n\n💡 Intuicja matematyczna\n\n\n\nSzukamy takiej linii y = ax + b, która najlepiej “przechodzi” przez nasze punkty danych. Algorytm automatycznie dobiera optymalne wartości a (slope) i b (intercept)."
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#praktyczny-przykład-predykcja-cen-mieszkań",
    "href": "cheatsheets/02-linear-regression.html#praktyczny-przykład-predykcja-cen-mieszkań",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "🏠 Praktyczny przykład: predykcja cen mieszkań",
    "text": "🏠 Praktyczny przykład: predykcja cen mieszkań\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Tworzenie przykładowych danych mieszkań\nnp.random.seed(42)\nn_mieszkan = 1000\n\ndata = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_mieszkan),\n    'liczba_pokoi': np.random.randint(1, 5, n_mieszkan),\n    'wiek_budynku': np.random.randint(0, 50, n_mieszkan),\n    'odleglosc_centrum': np.random.exponential(5, n_mieszkan)\n})\n\n# Realistyczna formuła ceny (z szumem)\ndata['cena'] = (\n    data['powierzchnia'] * 8000 +           # 8k za m²\n    data['liczba_pokoi'] * 15000 +          # 15k za pokój\n    (50 - data['wiek_budynku']) * 2000 +    # starsze = tańsze\n    data['odleglosc_centrum'] * (-3000) +   # dalej = tańsze\n    np.random.normal(0, 50000, n_mieszkan)  # szum losowy\n)\n\nprint(\"Podstawowe statystyki:\")\nprint(data.describe())\n\n\n\n\n\n\n\nPokaż statystyki\n\n\n\n\n\n\n\nPodstawowe statystyki:\n       powierzchnia  liczba_pokoi  wiek_budynku  odleglosc_centrum  \\\ncount   1000.000000   1000.000000   1000.000000        1000.000000   \nmean      70.483301      2.473000     25.049000           4.971457   \nstd       24.480398      1.128958     14.384001           4.883716   \nmin      -11.031684      1.000000      0.000000           0.000058   \n25%       53.810242      1.000000     13.000000           1.459988   \n50%       70.632515      2.000000     25.000000           3.505541   \n75%       86.198597      4.000000     38.000000           6.954361   \nmax      166.318287      4.000000     49.000000          34.028756   \n\n               cena  \ncount  1.000000e+03  \nmean   6.363187e+05  \nstd    2.004394e+05  \nmin    1.686504e+04  \n25%    5.049745e+05  \n50%    6.356690e+05  \n75%    7.647808e+05  \nmax    1.440259e+06"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#trenowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/02-linear-regression.html#trenowanie-modelu-krok-po-kroku",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "🔧 Trenowanie modelu krok po kroku",
    "text": "🔧 Trenowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Sprawdzenie korelacji - które zmienne są najważniejsze?\ncorrelation = data.corr()['cena'].sort_values(ascending=False)\n\n# Podział na features (X) i target (y)\nX = data[['powierzchnia', 'liczba_pokoi', 'wiek_budynku', 'odleglosc_centrum']]\ny = data['cena']\n\n# Podział train/test (80/20)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"Korelacja z ceną:\")\nprint(correlation)\n\nprint(f\"Dane treningowe: {X_train.shape[0]} mieszkań\")\nprint(f\"Dane testowe: {X_test.shape[0]} mieszkań\")\n\n\n\n\n\n\n\nPokaż korelacje oraz dane treningowe\n\n\n\n\n\n\n\nKorelacja z ceną:\ncena                 1.000000\npowierzchnia         0.949080\nliczba_pokoi         0.025094\nodleglosc_centrum   -0.042040\nwiek_budynku        -0.115566\nName: cena, dtype: float64\nDane treningowe: 800 mieszkań\nDane testowe: 200 mieszkań\n\n\n\n\n\n\n\n2) Trenowanie modelu\n\n# Utworzenie i trenowanie modelu\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Sprawdzenie współczynników\nprint(\"Współczynniki modelu:\")\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"{feature}: {coef:.0f} zł\")\nprint(f\"Intercept (stała): {model.intercept_:.0f} zł\")\n\n\n\n\n\n\n\nPokaż współczynniki\n\n\n\n\n\n\n\nWspółczynniki modelu:\npowierzchnia: 7924 zł\nliczba_pokoi: 15811 zł\nwiek_budynku: -2108 zł\nodleglosc_centrum: -2698 zł\nIntercept (stała): 104042 zł\n\n\n\n\n\n\n\n3) Ewaluacja wyników\n\n# Predykcje na zbiorze testowym\ny_pred = model.predict(X_test)\n\n# Metryki\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"\\nWyniki modelu:\")\nprint(f\"Mean Absolute Error: {mae:.0f} zł\")\nprint(f\"R² Score: {r2:.3f}\")\nprint(f\"Średni błąd to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania\")\n\n# Przykładowe predykcje\nfor i in range(5):\n    rzeczywista = y_test.iloc[i]\n    przewidywana = y_pred[i]\n    błąd = abs(rzeczywista - przewidywana)\n    print(f\"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}zł, \"\n          f\"przewidywana={przewidywana:.0f}zł, błąd={błąd:.0f}zł\")\n\n\n\n\n\n\n\nPokaż wyniki modelu\n\n\n\n\n\n\n\n\nWyniki modelu:\nMean Absolute Error: 38912 zł\nR² Score: 0.934\nŚredni błąd to 6.1% ceny mieszkania\nMieszkanie 1: rzeczywista=702313zł, przewidywana=771489zł, błąd=69177zł\nMieszkanie 2: rzeczywista=791174zł, przewidywana=816756zł, błąd=25582zł\nMieszkanie 3: rzeczywista=326702zł, przewidywana=274297zł, błąd=52405zł\nMieszkanie 4: rzeczywista=441380zł, przewidywana=456054zł, błąd=14674zł\nMieszkanie 5: rzeczywista=445427zł, przewidywana=383199zł, błąd=62228zł"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#wizualizacja-wyników",
    "href": "cheatsheets/02-linear-regression.html#wizualizacja-wyników",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "📊 Wizualizacja wyników",
    "text": "📊 Wizualizacja wyników\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Przykładowe dane mieszkań (powtarzamy dla demonstracji)\nnp.random.seed(42)\nn_mieszkan = 1000\n\ndata = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_mieszkan),\n    'liczba_pokoi': np.random.randint(1, 5, n_mieszkan),\n    'wiek_budynku': np.random.randint(0, 50, n_mieszkan),\n    'odleglosc_centrum': np.random.exponential(5, n_mieszkan)\n})\n\ndata['cena'] = (\n    data['powierzchnia'] * 8000 +\n    data['liczba_pokoi'] * 15000 +\n    (50 - data['wiek_budynku']) * 2000 +\n    data['odleglosc_centrum'] * (-3000) +\n    np.random.normal(0, 50000, n_mieszkan)\n)\n\n# Przygotowanie i trenowanie modelu\nX = data[['powierzchnia', 'liczba_pokoi', 'wiek_budynku', 'odleglosc_centrum']]\ny = data['cena']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Oblicz metryki\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Przygotuj wykresy\nplt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Rzeczywiste ceny')\nplt.ylabel('Przewidywane ceny')\nplt.title('Rzeczywiste vs Przewidywane')\n\nplt.subplot(1, 2, 2)\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Przewidywane ceny')\nplt.ylabel('Residuals (błędy)')\nplt.title('Analiza residuals')\n\nplt.tight_layout()\n\n# Zapisz wykres do zmiennej\nimport io\nimport base64\nbuf = io.BytesIO()\nplt.savefig(buf, format='png', dpi=100, bbox_inches='tight')\nbuf.seek(0)\nplt.close()\n\n# Przygotuj przykładowe predykcje\nprzykladowe_predykcje = []\nfor i in range(3):\n    rzeczywista = y_test.iloc[i]\n    przewidywana = y_pred[i]\n    blad = abs(rzeczywista - przewidywana)\n    przykladowe_predykcje.append(f\"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}zł, przewidywana={przewidywana:.0f}zł, błąd={blad:.0f}zł\")\n\nprint(f\"Wyniki modelu Linear Regression:\")\nprint(f\"Mean Absolute Error: {mae:.0f} zł\")\nprint(f\"R² Score: {r2:.3f}\")\nprint(f\"Średni błąd to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania\")\nprint(f\"\\nPrzykładowe predykcje:\")\nfor pred in przykladowe_predykcje:\n    print(pred)\n\n# Odtwórz wykres\nplt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Rzeczywiste ceny')\nplt.ylabel('Przewidywane ceny')\nplt.title('Rzeczywiste vs Przewidywane')\n\nplt.subplot(1, 2, 2)\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Przewidywane ceny')\nplt.ylabel('Residuals (błędy)')\nplt.title('Analiza residuals')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nPokaż wyniki i wykresy analizy\n\n\n\n\n\n\n\nWyniki modelu Linear Regression:\nMean Absolute Error: 38912 zł\nR² Score: 0.934\nŚredni błąd to 6.1% ceny mieszkania\n\nPrzykładowe predykcje:\nMieszkanie 1: rzeczywista=702313zł, przewidywana=771489zł, błąd=69177zł\nMieszkanie 2: rzeczywista=791174zł, przewidywana=816756zł, błąd=25582zł\nMieszkanie 3: rzeczywista=326702zł, przewidywana=274297zł, błąd=52405zł\n\n\n\n\n\nAnaliza wyników modelu Linear Regression"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#praktyczne-zastosowania-linear-regression",
    "href": "cheatsheets/02-linear-regression.html#praktyczne-zastosowania-linear-regression",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "🎯 Praktyczne zastosowania Linear Regression",
    "text": "🎯 Praktyczne zastosowania Linear Regression\n\n1) Biznesowe\n\n# Przewidywanie sprzedaży na podstawie budżetu marketingowego\nsales_data = pd.DataFrame({\n    'marketing_budget': [10000, 15000, 20000, 25000, 30000],\n    'sales': [100000, 140000, 180000, 220000, 260000]\n})\n\nmodel_sales = LinearRegression()\nmodel_sales.fit(sales_data[['marketing_budget']], sales_data['sales'])\n\n# \"Jeśli zwiększę budżet do 35k, sprzedaż wzrośnie do:\"\nnew_sales = model_sales.predict([[35000]])\nprint(f\"Przewidywana sprzedaż przy budżecie 35k: {new_sales[0]:.0f}\")\n\n\n\n\n\n\n\nPokaż przewidywaną sprzedaż\n\n\n\n\n\n\n\nPrzewidywana sprzedaż przy budżecie 35k: 300000\n\n\n\n\n\n\n\n2) Analizy finansowe\n\n# Relacja między PKB a konsumpcją\neconomics_data = pd.DataFrame({\n    'pkb_per_capita': [25000, 30000, 35000, 40000, 45000],\n    'konsumpcja': [18000, 21000, 24000, 27000, 30000]\n})\n\nmodel_econ = LinearRegression()\nmodel_econ.fit(economics_data[['pkb_per_capita']], economics_data['konsumpcja'])\n\n# Współczynnik skłonności do konsumpcji\nprint(f\"Na każde dodatkowe 1000zł PKB, konsumpcja rośnie o: {model_econ.coef_[0]:.0f}zł\")\n\n\n\n\n\n\n\nPokaż współczynnik skłonności do konsumpcji\n\n\n\n\n\n\n\nNa każde dodatkowe 1000zł PKB, konsumpcja rośnie o: 1zł"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#najczęstsze-pułapki-i-jak-ich-unikać",
    "href": "cheatsheets/02-linear-regression.html#najczęstsze-pułapki-i-jak-ich-unikać",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "⚠️ Najczęstsze pułapki i jak ich unikać",
    "text": "⚠️ Najczęstsze pułapki i jak ich unikać\n\n1) Overfitting z wieloma zmiennymi\n\n# ZŁO: za dużo features względem danych\n# Jeśli masz 100 mieszkań, nie używaj 50 features!\n\n# DOBRZE: zasada kciuka\ndef check_features_ratio(X, y):\n    ratio = len(y) / X.shape[1]\n    if ratio &lt; 10:\n        print(f\"⚠️ Uwaga: masz tylko {ratio:.1f} obserwacji na feature!\")\n        print(\"Rozważ: więcej danych lub mniej features\")\n    else:\n        print(f\"✅ OK: {ratio:.1f} obserwacji na feature\")\n\ncheck_features_ratio(X_train, y_train)\n\n\n\n\n\n\n\nPokaż wyniki\n\n\n\n\n\n\n\n✅ OK: 200.0 obserwacji na feature\n\n\n\n\n\n\n\n2) Sprawdzanie założeń linearności\n\n# Sprawdź czy zależności są rzeczywiście liniowe\nfrom scipy import stats\n\nfor col in X.columns:\n    correlation, p_value = stats.pearsonr(data[col], data['cena'])\n    print(f\"{col}: korelacja={correlation:.3f}, p-value={p_value:.3f}\")\n    \n    if abs(correlation) &lt; 0.1:\n        print(f\"⚠️ {col} ma słabą korelację - może nie być przydatna\")\n\n\n\n\n\n\n\nPokaż wyniki korelacji\n\n\n\n\n\n\n\npowierzchnia: korelacja=0.949, p-value=0.000\nliczba_pokoi: korelacja=0.025, p-value=0.428\n⚠️ liczba_pokoi ma słabą korelację - może nie być przydatna\nwiek_budynku: korelacja=-0.116, p-value=0.000\nodleglosc_centrum: korelacja=-0.042, p-value=0.184\n⚠️ odleglosc_centrum ma słabą korelację - może nie być przydatna\n\n\n\n\n\n\n\n3) Wielokoliniowość (features korelują między sobą)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Przykładowe dane features (demonstracja)\nnp.random.seed(42)\nn_samples = 1000\n\nX = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_samples),\n    'liczba_pokoi': np.random.randint(1, 5, n_samples),\n    'wiek_budynku': np.random.randint(0, 50, n_samples),\n    'odleglosc_centrum': np.random.exponential(5, n_samples)\n})\n\n# Oblicz korelacje między features\ncorrelation_matrix = X.corr()\n\n# Przygotuj wykres\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Korelacje między features')\nplt.close()\n\nprint(\"Macierz korelacji między zmiennymi:\")\nprint(correlation_matrix)\nprint(\"\\nInterpretacja:\")\nprint(\"• Wartości blisko 1.0 = silna korelacja pozytywna\")\nprint(\"• Wartości blisko -1.0 = silna korelacja negatywna\") \nprint(\"• Wartości blisko 0.0 = brak korelacji\")\nprint(\"\\nJeśli korelacja między features &gt; 0.8, usuń jedną z nich (multicollinearity)!\")\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Korelacje między features')\nplt.show()\n\n\n\n\n\n\n\nPokaż mapę korelacji\n\n\n\n\n\n\n\nMacierz korelacji między zmiennymi:\n                   powierzchnia  liczba_pokoi  wiek_budynku  odleglosc_centrum\npowierzchnia           1.000000     -0.064763      0.033920           0.029856\nliczba_pokoi          -0.064763      1.000000     -0.003894          -0.057555\nwiek_budynku           0.033920     -0.003894      1.000000          -0.037242\nodleglosc_centrum      0.029856     -0.057555     -0.037242           1.000000\n\nInterpretacja:\n• Wartości blisko 1.0 = silna korelacja pozytywna\n• Wartości blisko -1.0 = silna korelacja negatywna\n• Wartości blisko 0.0 = brak korelacji\n\nJeśli korelacja między features &gt; 0.8, usuń jedną z nich (multicollinearity)!\n\n\n\n\n\nMapa korelacji między zmiennymi"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#parametry-do-tuningu",
    "href": "cheatsheets/02-linear-regression.html#parametry-do-tuningu",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "🔧 Parametry do tuningu",
    "text": "🔧 Parametry do tuningu\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n# 1) Standardization - gdy features mają różne skale\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train)\n\n# 2) Polynomial Features - dla nieliniowych zależności\npoly_model = Pipeline([\n    ('poly', PolynomialFeatures(degree=2)),\n    ('linear', LinearRegression())\n])\npoly_model.fit(X_train, y_train)\n\npoly_pred = poly_model.predict(X_test)\npoly_r2 = r2_score(y_test, poly_pred)\nprint(f\"Polynomial Regression R²: {poly_r2:.3f}\")\n\n# 3) Regularization - Ridge i Lasso\nfrom sklearn.linear_model import Ridge, Lasso\n\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\nridge_r2 = r2_score(y_test, ridge.predict(X_test))\n\nlasso = Lasso(alpha=1.0)\nlasso.fit(X_train, y_train)\nlasso_r2 = r2_score(y_test, lasso.predict(X_test))\n\nprint(f\"Ridge R²: {ridge_r2:.3f}\")\nprint(f\"Lasso R²: {lasso_r2:.3f}\")\n\n\n\n\n\n\n\nPokaż wyniki predykcji cen\n\n\n\n\n\n\n\nPolynomial Regression R²: 0.933\nRidge R²: 0.934\nLasso R²: 0.934"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#real-world-przykłady-zastosowań",
    "href": "cheatsheets/02-linear-regression.html#real-world-przykłady-zastosowań",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "🌍 Real-world przykłady zastosowań",
    "text": "🌍 Real-world przykłady zastosowań\n\nE-commerce: Przewidywanie wartości życiowej klienta (LTV)\nNieruchomości: Automatyczna wycena domów (Zillow)\nFinanse: Scoring kredytowy, przewidywanie cen akcji\nMarketing: ROI kampanii reklamowych\nSupply Chain: Prognozowanie popytu na produkty\n\n\n\n\n\n\n\n💡 Kiedy używać Linear Regression?\n\n\n\n✅ UŻYJ GDY:\n\nPrzewidujesz wartości liczbowe (continuous target)\nZależności wydają się liniowe\nChcesz interpretowalny model\nMasz stosunkowo mało features\n\n❌ NIE UŻYWAJ GDY:\n\nTarget jest kategoryczny (użyj klasyfikacji)\nZależności są bardzo nieliniowe (użyj Random Forest, XGBoost)\nMasz tysiące features (użyj regularization)\n\n\n\nNastępna ściągawka: Decision Trees - klasyfikacja 🌳"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html",
    "href": "cheatsheets/01-intro-ml.html",
    "title": "Wprowadzenie do Machine Learning — fundament",
    "section": "",
    "text": "Machine Learning (ML) to dziedzina informatyki, która pozwala komputerom uczyć się i podejmować decyzje na podstawie danych, bez konieczności programowania każdej reguły z góry.\n\n\n\n\n\n\n💡 Intuicja\n\n\n\nZamiast pisać kod “jeśli temperatura &gt; 25°C, to będzie słonecznie”, ML pozwala algorytmowi samemu odkryć te zależności z historycznych danych pogodowych."
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#czym-jest-machine-learning",
    "href": "cheatsheets/01-intro-ml.html#czym-jest-machine-learning",
    "title": "Wprowadzenie do Machine Learning — fundament",
    "section": "",
    "text": "Machine Learning (ML) to dziedzina informatyki, która pozwala komputerom uczyć się i podejmować decyzje na podstawie danych, bez konieczności programowania każdej reguły z góry.\n\n\n\n\n\n\n💡 Intuicja\n\n\n\nZamiast pisać kod “jeśli temperatura &gt; 25°C, to będzie słonecznie”, ML pozwala algorytmowi samemu odkryć te zależności z historycznych danych pogodowych."
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#główne-rodzaje-ml",
    "href": "cheatsheets/01-intro-ml.html#główne-rodzaje-ml",
    "title": "Wprowadzenie do Machine Learning — fundament",
    "section": "📊 Główne rodzaje ML",
    "text": "📊 Główne rodzaje ML\n\n1) Supervised Learning (Uczenie nadzorowane)\nMamy dane + znamy prawidłowe odpowiedzi\n\n# Przykład: predykcja ceny domu\n# Dane wejściowe: powierzchnia, lokalizacja, rok budowy\n# Cel: przewidzieć cenę\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Przykładowe dane\ndata = pd.DataFrame({\n    'powierzchnia': [50, 75, 100, 120, 150],\n    'rok_budowy': [1990, 2000, 2010, 2015, 2020],\n    'cena': [300000, 400000, 550000, 650000, 800000]  # znamy prawdziwe ceny!\n})\n\n# Trenowanie modelu\nmodel = LinearRegression()\nX = data[['powierzchnia', 'rok_budowy']]\ny = data['cena']\nmodel.fit(X, y)\n\n# Predykcja dla nowego domu\nnowy_dom = [[90, 2005]]\nprzewidywana_cena = model.predict(nowy_dom)\n\n# Zapisz wyniki do zmiennych dla expandera\ndane_treningowe = data.to_string()\nwynik_predykcji = f\"Przewidywana cena: {przewidywana_cena[0]:.0f} zł\"\n\nprint(\"Dane treningowe:\")\nprint(dane_treningowe)\nprint(f\"\\nPredykcja dla domu 90m², rok 2005:\")\nprint(wynik_predykcji)\n\n\n\n\n\n\n\nPokaż wyniki predykcji cen\n\n\n\n\n\n\n\nDane treningowe:\n   powierzchnia  rok_budowy    cena\n0            50        1990  300000\n1            75        2000  400000\n2           100        2010  550000\n3           120        2015  650000\n4           150        2020  800000\n\nPredykcja dla domu 90m², rok 2005:\nPrzewidywana cena: 493819 zł\n\n\n\n\n\nReal-world zastosowania:\n\nPredykcja cen akcji/nieruchomości\nDiagnoza medyczna (klasyfikacja chorób)\nFiltrowanie spamu w emailach\nRozpoznawanie mowy/obrazów\n\n\n\n\n2) Unsupervised Learning (Uczenie nienadzorowane)\nMamy tylko dane, szukamy ukrytych wzorców\n\n# Przykład: segmentacja klientów sklepu\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Dane klientów: wiek i wydatki miesięczne\nklienci = pd.DataFrame({\n    'wiek': [25, 30, 35, 22, 28, 45, 50, 55, 60, 65],\n    'wydatki': [2000, 2500, 3000, 1800, 2200, 4000, 4500, 3500, 3000, 2800]\n})\n\n# Grupowanie klientów w 3 segmenty\nkmeans = KMeans(n_clusters=3, random_state=42)\nklienci['segment'] = kmeans.fit_predict(klienci[['wiek', 'wydatki']])\n\n# Przygotuj wyniki do wyświetlenia\ndane_klientow = klienci.head().to_string()\nsegmenty_info = []\nfor i in range(3):\n    segment = klienci[klienci['segment'] == i]\n    segmenty_info.append(f\"Segment {i}: średni wiek {segment['wiek'].mean():.0f}, średnie wydatki {segment['wydatki'].mean():.0f}\")\n\nprzyklad_predykcji = kmeans.predict([[30, 2500]])[0]\n\nprint(\"Dane klientów:\")\nprint(dane_klientow)\nprint(\"\\nSegmenty klientów:\")\nfor info in segmenty_info:\n    print(info)\nprint(f\"\\nPrzykład: klient 30 lat, wydaje 2500zł -&gt; segment {przyklad_predykcji}\")\n\n\n\n\n\n\n\nPokaż wyniki segmentacji klientów\n\n\n\n\n\n\n\nDane klientów:\n   wiek  wydatki  segment\n0    25     2000        0\n1    30     2500        2\n2    35     3000        2\n3    22     1800        0\n4    28     2200        0\n\nSegmenty klientów:\nSegment 0: średni wiek 25, średnie wydatki 2000\nSegment 1: średni wiek 50, średnie wydatki 4000\nSegment 2: średni wiek 48, średnie wydatki 2825\n\nPrzykład: klient 30 lat, wydaje 2500zł -&gt; segment 2\n\n\n\n\n\nReal-world zastosowania:\n\nSegmentacja klientów (marketing)\nWykrywanie anomalii (cyberbezpieczeństwo)\nAnaliza koszykowa (co kupują razem)\nKompresja danych\n\n\n\n\n3) Reinforcement Learning (Uczenie ze wzmocnieniem)\nAgent uczy się przez interakcję i nagrody/kary\n\n# Przykład koncepcyjny: optymalizacja reklam\nclass SimpleAgent:\n    def __init__(self):\n        # Jakie reklamy pokazywać: [\"sportowe\", \"technologiczne\", \"modowe\"]\n        self.ad_types = [\"sportowe\", \"technologiczne\", \"modowe\"]\n        self.rewards = [0, 0, 0]  # nagrody za każdy typ\n        self.counts = [0, 0, 0]   # ile razy pokazane\n    \n    def choose_ad(self):\n        # Wybierz reklamę z najwyższą średnią nagrodą\n        avg_rewards = [r/max(c,1) for r, c in zip(self.rewards, self.counts)]\n        return avg_rewards.index(max(avg_rewards))\n    \n    def update_reward(self, ad_type, clicked):\n        # Aktualizuj nagrody na podstawie kliknięć\n        self.counts[ad_type] += 1\n        if clicked:\n            self.rewards[ad_type] += 1\n    \n    def get_stats(self):\n        stats_list = []\n        for i, ad_type in enumerate(self.ad_types):\n            rate = self.rewards[i] / max(self.counts[i], 1) * 100\n            stats_list.append(f\"{ad_type}: {self.counts[i]} pokazań, {self.rewards[i]} kliknięć ({rate:.1f}%)\")\n        return stats_list\n\n# Symulacja\nimport numpy as np\nnp.random.seed(42)\n\nagent = SimpleAgent()\nsymulacja_wyniki = []\nsymulacja_wyniki.append(\"🎯 Symulacja optymalizacji reklam:\")\nsymulacja_wyniki.append(\"Agent uczy się, które reklamy działają najlepiej...\")\n\nfor day in range(10):\n    ad = agent.choose_ad()\n    # Różne prawdopodobieństwa kliknięć dla różnych typów reklam\n    click_probs = [0.1, 0.3, 0.2]  # technologiczne najlepsze\n    clicked = np.random.random() &lt; click_probs[ad]\n    agent.update_reward(ad, clicked)\n    symulacja_wyniki.append(f\"Dzień {day+1}: pokazano {agent.ad_types[ad]}, kliknięta: {clicked}\")\n\nkoncowe_statystyki = agent.get_stats()\n\nfor wynik in symulacja_wyniki:\n    print(wynik)\nprint(\"\\n📊 Końcowe statystyki:\")\nfor stat in koncowe_statystyki:\n    print(stat)\n\n\n\n\n\n\n\nPokaż symulację agenta reklamowego\n\n\n\n\n\n\n\n🎯 Symulacja optymalizacji reklam:\nAgent uczy się, które reklamy działają najlepiej...\nDzień 1: pokazano sportowe, kliknięta: False\nDzień 2: pokazano sportowe, kliknięta: False\nDzień 3: pokazano sportowe, kliknięta: False\nDzień 4: pokazano sportowe, kliknięta: False\nDzień 5: pokazano sportowe, kliknięta: False\nDzień 6: pokazano sportowe, kliknięta: False\nDzień 7: pokazano sportowe, kliknięta: True\nDzień 8: pokazano sportowe, kliknięta: False\nDzień 9: pokazano sportowe, kliknięta: False\nDzień 10: pokazano sportowe, kliknięta: False\n\n📊 Końcowe statystyki:\nsportowe: 10 pokazań, 1 kliknięć (10.0%)\ntechnologiczne: 0 pokazań, 0 kliknięć (0.0%)\nmodowe: 0 pokazań, 0 kliknięć (0.0%)\n\n\n\n\n\nReal-world zastosowania:\n\nGry komputerowe (AI graczy)\nAutonomiczne pojazdy\nOptymalizacja reklam online\nRoboty przemysłowe"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#jak-wybrać-odpowiedni-typ-ml",
    "href": "cheatsheets/01-intro-ml.html#jak-wybrać-odpowiedni-typ-ml",
    "title": "Wprowadzenie do Machine Learning — fundament",
    "section": "🎯 Jak wybrać odpowiedni typ ML?",
    "text": "🎯 Jak wybrać odpowiedni typ ML?\n\n\n\n\n\n\n\n\nSytuacja\nTyp ML\nPrzykład\n\n\n\n\nMasz dane z prawidłowymi odpowiedziami\nSupervised\nSpam/nie-spam w emailach\n\n\nChcesz znaleźć ukryte grupы\nUnsupervised\nSegmentacja klientów\n\n\nSystem ma się uczyć przez trial & error\nReinforcement\nBot do gier"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#podstawowe-kroki-projektu-ml",
    "href": "cheatsheets/01-intro-ml.html#podstawowe-kroki-projektu-ml",
    "title": "Wprowadzenie do Machine Learning — fundament",
    "section": "🔧 Podstawowe kroki projektu ML",
    "text": "🔧 Podstawowe kroki projektu ML\n\n# Kompletny workflow ML na przykładzie klasyfikacji iris\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# 1. Załaduj i poznaj dane\niris = load_iris()\ndata = pd.DataFrame(iris.data, columns=iris.feature_names)\ndata['target'] = iris.target\ntarget_names = iris.target_names\n\n# 2. Przygotuj dane (czyszczenie, encoding)\nX = data.drop('target', axis=1)\ny = data['target']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 3. Podziel na train/test\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# 4. Wybierz i wytrenuj model\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# 5. Oceń wyniki\npredictions = model.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\n\n# Przykład predykcji\nsample_flower = X_test[0:1]\npredicted_class = model.predict(sample_flower)[0]\npredicted_name = target_names[predicted_class]\nactual_name = target_names[y_test.iloc[0]]\n\n# Przygotuj wyniki do wyświetlenia\nwyniki_workflow = []\nwyniki_workflow.append(\"🔬 Kompletny workflow projektu ML\")\nwyniki_workflow.append(\"=\" * 40)\nwyniki_workflow.append(\"1️⃣ Dane załadowane:\")\nwyniki_workflow.append(f\"Kształt: {data.shape}\")\nwyniki_workflow.append(f\"Klasy: {target_names}\")\nwyniki_workflow.append(f\"Pierwsze 3 wiersze:\\n{data.head(3).to_string()}\")\nwyniki_workflow.append(f\"\\n2️⃣ Dane przeskalowane (pierwsze 3 cechy pierwszej próbki):\")\nwyniki_workflow.append(f\"Przed: {X.iloc[0, :3].values}\")\nwyniki_workflow.append(f\"Po: {X_scaled[0, :3]}\")\nwyniki_workflow.append(f\"\\n3️⃣ Podział danych:\")\nwyniki_workflow.append(f\"Train: {len(X_train)} próbek\")\nwyniki_workflow.append(f\"Test: {len(X_test)} próbek\")\nwyniki_workflow.append(f\"\\n4️⃣ Model wytrenowany: RandomForestClassifier\")\nwyniki_workflow.append(f\"\\n5️⃣ Wyniki:\")\nwyniki_workflow.append(f\"Dokładność: {accuracy:.2%}\")\nwyniki_workflow.append(f\"\\nPrzykład predykcji:\")\nwyniki_workflow.append(f\"Przewidywana klasa: {predicted_name}\")\nwyniki_workflow.append(f\"Rzeczywista klasa: {actual_name}\")\nwyniki_workflow.append(\"✅ Poprawnie!\" if predicted_name == actual_name else \"❌ Błąd\")\n\nfor wynik in wyniki_workflow:\n    print(wynik)\n\n\n\n\n\n\n\nPokaż kompletny workflow ML\n\n\n\n\n\n\n\n🔬 Kompletny workflow projektu ML\n========================================\n1️⃣ Dane załadowane:\nKształt: (150, 5)\nKlasy: ['setosa' 'versicolor' 'virginica']\nPierwsze 3 wiersze:\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n0                5.1               3.5                1.4               0.2       0\n1                4.9               3.0                1.4               0.2       0\n2                4.7               3.2                1.3               0.2       0\n\n2️⃣ Dane przeskalowane (pierwsze 3 cechy pierwszej próbki):\nPrzed: [5.1 3.5 1.4]\nPo: [-0.90068117  1.01900435 -1.34022653]\n\n3️⃣ Podział danych:\nTrain: 120 próbek\nTest: 30 próbek\n\n4️⃣ Model wytrenowany: RandomForestClassifier\n\n5️⃣ Wyniki:\nDokładność: 100.00%\n\nPrzykład predykcji:\nPrzewidywana klasa: versicolor\nRzeczywista klasa: versicolor\n✅ Poprawnie!"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#najważniejsze-biblioteki-python-dla-ml",
    "href": "cheatsheets/01-intro-ml.html#najważniejsze-biblioteki-python-dla-ml",
    "title": "Wprowadzenie do Machine Learning — fundament",
    "section": "💡 Najważniejsze biblioteki Python dla ML",
    "text": "💡 Najważniejsze biblioteki Python dla ML\n# Podstawowe przetwarzanie danych\nimport pandas as pd      # DataFrames, CSV, analiza\nimport numpy as np       # obliczenia numeryczne\n\n# Machine Learning\nfrom sklearn import *    # algorytmy ML, preprocessing, metryki\nimport xgboost as xgb   # zaawansowane drzewa decyzyjne\n\n# Wizualizacja\nimport matplotlib.pyplot as plt  # wykresy\nimport seaborn as sns           # piękne wykresy statystyczne\n\n# Deep Learning\nimport tensorflow as tf  # sieci neuronowe (Google)\nimport torch            # sieci neuronowe (Facebook)\n\n\n\n\n\n\n\n🎯 Pro tips dla początkujących\n\n\n\n\nZacznij od prostych algorytmów - Linear Regression, Decision Trees\n80% czasu to przygotowanie danych - czyszczenie, eksploracja, feature engineering\nZawsze sprawdź czy model nie jest overfitted - użyj validation set\nRozumiej swoje dane przed wyborem algorytmu\nPraktyka &gt; teoria - rób dużo projektów na różnych danych!\n\n\n\nNastępna ściągawka: Linear Regression w praktyce 🚀"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html",
    "href": "cheatsheets/03-decision-trees.html",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "",
    "text": "Decision Trees to jeden z najbardziej intuicyjnych algorytmów ML, który podejmuje decyzje jak człowiek - zadając szereg pytań typu “tak/nie” i na podstawie odpowiedzi klasyfikuje lub przewiduje wartości.\n\n\n\n\n\n\n💡 Intuicja\n\n\n\nWyobraź sobie lekarza, który diagnozuje chorobę: “Czy ma gorączkę? TAK → Czy boli gardło? TAK → Czy ma katar? NIE → Prawdopodobnie angina”. Decision Tree działa dokładnie tak samo!"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#czym-są-decision-trees",
    "href": "cheatsheets/03-decision-trees.html#czym-są-decision-trees",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "",
    "text": "Decision Trees to jeden z najbardziej intuicyjnych algorytmów ML, który podejmuje decyzje jak człowiek - zadając szereg pytań typu “tak/nie” i na podstawie odpowiedzi klasyfikuje lub przewiduje wartości.\n\n\n\n\n\n\n💡 Intuicja\n\n\n\nWyobraź sobie lekarza, który diagnozuje chorobę: “Czy ma gorączkę? TAK → Czy boli gardło? TAK → Czy ma katar? NIE → Prawdopodobnie angina”. Decision Tree działa dokładnie tak samo!"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#praktyczny-przykład-klasyfikacja-klientów-banku",
    "href": "cheatsheets/03-decision-trees.html#praktyczny-przykład-klasyfikacja-klientów-banku",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "🎯 Praktyczny przykład: klasyfikacja klientów banku",
    "text": "🎯 Praktyczny przykład: klasyfikacja klientów banku\nCzy klient weźmie kredyt na podstawie jego profilu?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Tworzenie realistycznych danych klientów banku\nnp.random.seed(42)\nn_clients = 2000\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_clients),\n    'dochod': np.random.lognormal(10, 0.5, n_clients),  # log-normal dla dochodów\n    'historia_kredytowa': np.random.choice(['dobra', 'średnia', 'słaba'], n_clients, p=[0.6, 0.3, 0.1]),\n    'zatrudnienie': np.random.choice(['stałe', 'tymczasowe', 'bezrobotny'], n_clients, p=[0.7, 0.25, 0.05]),\n    'oszczednosci': np.random.exponential(20000, n_clients),\n    'liczba_dzieci': np.random.randint(0, 4, n_clients)\n})\n\n# Realistyczna logika decyzyjna banku\ndef czy_kredyt(row):\n    score = 0\n    \n    # Wiek (30-50 lat = najlepsi klienci)\n    if 30 &lt;= row['wiek'] &lt;= 50:\n        score += 2\n    elif row['wiek'] &lt; 25 or row['wiek'] &gt; 60:\n        score -= 1\n    \n    # Dochód\n    if row['dochod'] &gt; 50000:\n        score += 3\n    elif row['dochod'] &gt; 30000:\n        score += 1\n    else:\n        score -= 2\n    \n    # Historia kredytowa\n    if row['historia_kredytowa'] == 'dobra':\n        score += 2\n    elif row['historia_kredytowa'] == 'słaba':\n        score -= 3\n    \n    # Zatrudnienie\n    if row['zatrudnienie'] == 'stałe':\n        score += 2\n    elif row['zatrudnienie'] == 'bezrobotny':\n        score -= 4\n    \n    # Oszczędności\n    if row['oszczednosci'] &gt; 50000:\n        score += 1\n    \n    # Dzieci (więcej dzieci = większe ryzyko)\n    score -= row['liczba_dzieci'] * 0.5\n    \n    # Końcowa decyzja z odrobiną losowości\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 0.5)))\n    return probability &gt; 0.5\n\ndata['kredyt_przyznany'] = data.apply(czy_kredyt, axis=1)\n\nprint(\"Statystyki klientów:\")\nprint(data.describe())\nprint(f\"\\nOdsetek przyznanych kredytów: {data['kredyt_przyznany'].mean():.1%}\")\n\n\n\n\n\n\n\nPokaż statystyki i odsetki przyznanych kredytów\n\n\n\n\n\n\n\nStatystyki klientów:\n              wiek         dochod   oszczednosci  liczba_dzieci\ncount  2000.000000    2000.000000    2000.000000    2000.000000\nmean     43.805500   25726.361013   19950.270464       1.523000\nstd      14.929203   14090.406882   20299.940268       1.130535\nmin      18.000000    4047.221838      27.128881       0.000000\n25%      31.000000   15922.761710    5923.833818       1.000000\n50%      44.000000   22782.391426   13844.909016       2.000000\n75%      56.000000   31922.003675   27566.091135       3.000000\nmax      69.000000  112447.929023  165194.684774       3.000000\n\nOdsetek przyznanych kredytów: 63.8%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#budowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/03-decision-trees.html#budowanie-modelu-krok-po-kroku",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "🔧 Budowanie modelu krok po kroku",
    "text": "🔧 Budowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Encoding kategorycznych zmiennych\nfrom sklearn.preprocessing import LabelEncoder\n\ndata_encoded = data.copy()\nle_historia = LabelEncoder()\nle_zatrudnienie = LabelEncoder()\n\ndata_encoded['historia_kredytowa_num'] = le_historia.fit_transform(data['historia_kredytowa'])\ndata_encoded['zatrudnienie_num'] = le_zatrudnienie.fit_transform(data['zatrudnienie'])\n\n# Features do modelu\nfeature_columns = ['wiek', 'dochod', 'historia_kredytowa_num', 'zatrudnienie_num', 'oszczednosci', 'liczba_dzieci']\nX = data_encoded[feature_columns]\ny = data_encoded['kredyt_przyznany']\n\n# Podział train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Dane treningowe: {len(X_train)} klientów\")\nprint(f\"Dane testowe: {len(X_test)} klientów\")\n\n\n\n\n\n\n\nPokaż wyniki dane treningowe i testowe\n\n\n\n\n\n\n\nDane treningowe: 1600 klientów\nDane testowe: 400 klientów\n\n\n\n\n\n\n\n2) Trenowanie Decision Tree\n\n# Tworzenie modelu z ograniczeniami (żeby nie był za głęboki)\ndt_model = DecisionTreeClassifier(\n    max_depth=5,        # maksymalna głębokość drzewa\n    min_samples_split=50,  # min. próbek do podziału węzła\n    min_samples_leaf=20,   # min. próbek w liściu\n    random_state=42\n)\n\n# Trenowanie\ndt_model.fit(X_train, y_train)\n\nprint(\"Model wytrenowany!\")\nprint(f\"Głębokość drzewa: {dt_model.tree_.max_depth}\")\nprint(f\"Liczba liści: {dt_model.tree_.n_leaves}\")\n\n\n\n\n\n\n\nPokaż parametry wytrenowanego modelu\n\n\n\n\n\n\n\nModel wytrenowany!\nGłębokość drzewa: 5\nLiczba liści: 20\n\n\n\n\n\n\n\n3) Ewaluacja modelu\n\n# Predykcje\ny_pred = dt_model.predict(X_test)\ny_pred_proba = dt_model.predict_proba(X_test)[:, 1]\n\n# Metryki\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nDokładność modelu: {accuracy:.1%}\")\n\n# Szczegółowy raport\nprint(\"\\nRaport klasyfikacji:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:    Nie    Tak\")\nprint(f\"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}\")\n\n\n\n\n\n\n\nPokaż wyniki\n\n\n\n\n\n\n\n\nConfusion Matrix:\nPrzewidywane:    Nie    Tak\nRzeczywiste Nie: 122    34\nRzeczywiste Tak:  27   217"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#wizualizacja-drzewa-decyzyjnego",
    "href": "cheatsheets/03-decision-trees.html#wizualizacja-drzewa-decyzyjnego",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "🎨 Wizualizacja drzewa decyzyjnego",
    "text": "🎨 Wizualizacja drzewa decyzyjnego\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Tworzenie przykładowych danych (kompletny przykład)\nnp.random.seed(42)\nn_clients = 2000\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_clients),\n    'dochod': np.random.lognormal(10, 0.5, n_clients),\n    'historia_kredytowa': np.random.choice(['dobra', 'średnia', 'słaba'], n_clients, p=[0.6, 0.3, 0.1]),\n    'zatrudnienie': np.random.choice(['stałe', 'tymczasowe', 'bezrobotny'], n_clients, p=[0.7, 0.25, 0.05]),\n    'oszczednosci': np.random.exponential(20000, n_clients),\n    'liczba_dzieci': np.random.randint(0, 4, n_clients)\n})\n\n# Prosta logika przyznawania kredytu\ndef czy_kredyt(row):\n    score = 0\n    if 30 &lt;= row['wiek'] &lt;= 50:\n        score += 2\n    if row['dochod'] &gt; 50000:\n        score += 3\n    if row['historia_kredytowa'] == 'dobra':\n        score += 2\n    if row['zatrudnienie'] == 'stałe':\n        score += 2\n    if row['oszczednosci'] &gt; 50000:\n        score += 1\n    score -= row['liczba_dzieci'] * 0.5\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 0.5)))\n    return probability &gt; 0.5\n\ndata['kredyt_przyznany'] = data.apply(czy_kredyt, axis=1)\n\n# Encoding i model\ndata_encoded = data.copy()\nle_historia = LabelEncoder()\nle_zatrudnienie = LabelEncoder()\ndata_encoded['historia_kredytowa_num'] = le_historia.fit_transform(data['historia_kredytowa'])\ndata_encoded['zatrudnienie_num'] = le_zatrudnienie.fit_transform(data['zatrudnienie'])\n\nfeature_columns = ['wiek', 'dochod', 'historia_kredytowa_num', 'zatrudnienie_num', 'oszczednosci', 'liczba_dzieci']\nX = data_encoded[feature_columns]\ny = data_encoded['kredyt_przyznany']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie drzewa\ndt_model = DecisionTreeClassifier(max_depth=5, min_samples_split=50, min_samples_leaf=20, random_state=42)\ndt_model.fit(X_train, y_train)\n\n# Ewaluacja\ny_pred = dt_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\n# Ważność features\nfeature_names = ['wiek', 'dochód', 'historia_kred.', 'zatrudnienie', 'oszczędności', 'liczba_dzieci']\nimportance = dt_model.feature_importances_\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importance\n}).sort_values('importance', ascending=False)\n\n# Przygotuj wykres drzewa\nplt.figure(figsize=(20, 10))\ntree.plot_tree(dt_model, \n               feature_names=feature_names,\n               class_names=['Nie', 'Tak'],\n               filled=True,\n               rounded=True,\n               fontsize=10)\nplt.title(\"Drzewo decyzyjne - Przyznanie kredytu\")\nplt.close()\n\nprint(f\"Wyniki modelu Decision Tree:\")\nprint(f\"Głębokość drzewa: {dt_model.tree_.max_depth}\")\nprint(f\"Liczba liści: {dt_model.tree_.n_leaves}\")\nprint(f\"Dokładność modelu: {accuracy:.1%}\")\n\nprint(f\"\\nStatystyki danych:\")\nprint(f\"Dane treningowe: {len(X_train)} klientów\")\nprint(f\"Dane testowe: {len(X_test)} klientów\")\nprint(f\"Odsetek przyznanych kredytów: {data['kredyt_przyznany'].mean():.1%}\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:    Nie    Tak\")\nprint(f\"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}\")\n\nprint(\"\\nWażność cech:\")\nfor _, row in feature_importance.iterrows():\n    print(f\"{row['feature']}: {row['importance']:.3f}\")\n\n# Odtwórz wykres drzewa\nplt.figure(figsize=(20, 10))\ntree.plot_tree(dt_model, \n               feature_names=feature_names,\n               class_names=['Nie', 'Tak'],\n               filled=True,\n               rounded=True,\n               fontsize=10)\nplt.title(\"Drzewo decyzyjne - Przyznanie kredytu\")\nplt.show()\n\n\n\n\n\n\n\nPokaż wyniki i drzewo decyzyjne\n\n\n\n\n\n\n\nWyniki modelu Decision Tree:\nGłębokość drzewa: 5\nLiczba liści: 13\nDokładność modelu: 95.0%\n\nStatystyki danych:\nDane treningowe: 1600 klientów\nDane testowe: 400 klientów\nOdsetek przyznanych kredytów: 93.2%\n\nConfusion Matrix:\nPrzewidywane:    Nie    Tak\nRzeczywiste Nie:  19    11\nRzeczywiste Tak:   9   361\n\nWażność cech:\nzatrudnienie: 0.441\nwiek: 0.317\nhistoria_kred.: 0.178\nliczba_dzieci: 0.055\noszczędności: 0.008\ndochód: 0.001\n\n\n\n\n\nWizualizacja drzewa decyzyjnego dla kredytów"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#interpretacja-decyzji-dla-konkretnego-klienta",
    "href": "cheatsheets/03-decision-trees.html#interpretacja-decyzji-dla-konkretnego-klienta",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "🔍 Interpretacja decyzji dla konkretnego klienta",
    "text": "🔍 Interpretacja decyzji dla konkretnego klienta\n\n# Funkcja do interpretacji ścieżki decyzyjnej\ndef explain_decision(model, X_sample, feature_names):\n    # Pobierz ścieżkę w drzewie\n    leaf_id = model.decision_path(X_sample.reshape(1, -1)).toarray()[0]\n    feature = model.tree_.feature\n    threshold = model.tree_.threshold\n    \n    print(\"Ścieżka decyzyjna:\")\n    for node_id in range(len(leaf_id)):\n        if leaf_id[node_id] == 1:  # jeśli węzeł jest na ścieżce\n            if feature[node_id] != -2:  # jeśli nie jest liściem\n                feature_name = feature_names[feature[node_id]]\n                threshold_val = threshold[node_id]\n                feature_val = X_sample[feature[node_id]]\n                \n                if feature_val &lt;= threshold_val:\n                    condition = \"&lt;=\"\n                else:\n                    condition = \"&gt;\"\n                    \n                print(f\"  {feature_name} ({feature_val:.0f}) {condition} {threshold_val:.0f}\")\n\n# Przykład dla konkretnego klienta\nsample_client = X_test.iloc[0]\nprediction = dt_model.predict([sample_client])[0]\nprobability = dt_model.predict_proba([sample_client])[0]\n\nprint(f\"Klient testowy:\")\nprint(f\"Wiek: {sample_client['wiek']}\")\nprint(f\"Dochód: {sample_client['dochod']:.0f}\")\nprint(f\"Oszczędności: {sample_client['oszczednosci']:.0f}\")\nprint(f\"\\nDecyzja: {'KREDYT PRZYZNANY' if prediction else 'KREDYT ODRZUCONY'}\")\nprint(f\"Prawdopodobieństwo: {probability[1]:.1%}\")\n\nexplain_decision(dt_model, sample_client.values, feature_names)\n\n\n\n\n\n\n\nPokaż dane klienta testowego\n\n\n\n\n\n\n\nKlient testowy:\nWiek: 56.0\nDochód: 7927\nOszczędności: 32270\n\nDecyzja: KREDYT ODRZUCONY\nPrawdopodobieństwo: 3.0%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#różne-zastosowania-decision-trees",
    "href": "cheatsheets/03-decision-trees.html#różne-zastosowania-decision-trees",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "🎯 Różne zastosowania Decision Trees",
    "text": "🎯 Różne zastosowania Decision Trees\n\n1) Medyczna diagnoza\n\n# Przykład klasyfikacji ryzyka chorób serca\nmedical_features = ['wiek', 'cholesterol', 'ciśnienie', 'BMI', 'pali_papierosy']\n# Target: 'ryzyko_chorób_serca' (wysokie/niskie)\n\nmedical_tree = DecisionTreeClassifier(max_depth=4)\n# Model automatycznie znajdzie progi: \"Jeśli cholesterol &gt; 240 I BMI &gt; 30 ORAZ wiek &gt; 50...\"\n\n\n\n2) Marketing - segmentacja klientów\n\n# Przewidywanie czy klient kupi produkt premium\nmarketing_features = ['dochód_roczny', 'wiek', 'wykształcenie', 'poprzednie_zakupy']\n# Target: 'kupi_premium' (tak/nie)\n\nmarketing_tree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=100)\n# Wynik: jasne reguły marketingowe do targetowania reklam\n\n\n\n3) HR - decyzje o zatrudnieniu\n\n# Przewidywanie sukcesu kandydata w rekrutacji\nhr_features = ['doświadczenie_lat', 'wykształcenie', 'wynik_testów', 'referencje']\n# Target: 'zatrudniony' (tak/nie)\n\nhr_tree = DecisionTreeClassifier(max_depth=5)\n# Wynik: automatyczne zasady rekrutacyjne"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#tuning-parametrów",
    "href": "cheatsheets/03-decision-trees.html#tuning-parametrów",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "⚙️ Tuning parametrów",
    "text": "⚙️ Tuning parametrów\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Najważniejsze parametry do tuningu\nparam_grid = {\n    'max_depth': [3, 5, 7, 10, None],\n    'min_samples_split': [20, 50, 100],\n    'min_samples_leaf': [10, 20, 50],\n    'criterion': ['gini', 'entropy']\n}\n\n# Grid search z cross-validation\ngrid_search = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(\"Najlepsze parametry:\")\nprint(grid_search.best_params_)\nprint(f\"Najlepsza dokładność CV: {grid_search.best_score_:.1%}\")\n\n# Model z najlepszymi parametrami\nbest_model = grid_search.best_estimator_\nbest_accuracy = accuracy_score(y_test, best_model.predict(X_test))\nprint(f\"Dokładność na test set: {best_accuracy:.1%}\")\n\n\n\n\n\n\n\nPokaż najlepsze parametry\n\n\n\n\n\n\n\nNajlepsze parametry:\n{'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 20}\nNajlepsza dokładność CV: 96.5%\nDokładność na test set: 95.0%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#problemy-i-rozwiązania",
    "href": "cheatsheets/03-decision-trees.html#problemy-i-rozwiązania",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "⚠️ Problemy i rozwiązania",
    "text": "⚠️ Problemy i rozwiązania\n\n1) Overfitting - drzewo za głębokie\n\n# Problem: drzewo \"pamięta\" dane treningowe\noverfitted_tree = DecisionTreeClassifier()  # bez ograniczeń!\noverfitted_tree.fit(X_train, y_train)\n\ntrain_acc = accuracy_score(y_train, overfitted_tree.predict(X_train))\ntest_acc = accuracy_score(y_test, overfitted_tree.predict(X_test))\n\nprint(f\"Overfitted model:\")\nprint(f\"Train accuracy: {train_acc:.1%}\")\nprint(f\"Test accuracy: {test_acc:.1%}\")\nprint(f\"Różnica: {train_acc - test_acc:.1%} - to overfitting!\")\n\n# Rozwiązanie: ograniczenia\npruned_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=20)\npruned_tree.fit(X_train, y_train)\n\ntrain_acc_pruned = accuracy_score(y_train, pruned_tree.predict(X_train))\ntest_acc_pruned = accuracy_score(y_test, pruned_tree.predict(X_test))\n\nprint(f\"\\nPruned model:\")\nprint(f\"Train accuracy: {train_acc_pruned:.1%}\")\nprint(f\"Test accuracy: {test_acc_pruned:.1%}\")\nprint(f\"Różnica: {train_acc_pruned - test_acc_pruned:.1%} - znacznie lepiej!\")\n\n\n\n\n\n\n\nPokaż wyniki\n\n\n\n\n\n\n\nOverfitted model:\nTrain accuracy: 100.0%\nTest accuracy: 94.8%\nRóżnica: 5.2% - to overfitting!\n\nPruned model:\nTrain accuracy: 96.7%\nTest accuracy: 95.0%\nRóżnica: 1.7% - znacznie lepiej!\n\n\n\n\n\n\n\n2) Brak stabilności - małe zmiany = różne drzewa\n\n# Problem demonstracji\nresults = []\nfor i in range(10):\n    # Różne random_state = różne drzewa\n    tree_test = DecisionTreeClassifier(random_state=i, max_depth=5)\n    tree_test.fit(X_train, y_train)\n    acc = accuracy_score(y_test, tree_test.predict(X_test))\n    results.append(acc)\n\nprint(f\"Dokładność różnych drzew: {min(results):.1%} - {max(results):.1%}\")\nprint(f\"Rozrzut: {max(results) - min(results):.1%}\")\nprint(\"Rozwiązanie: Random Forest (następna ściągawka!)\")\n\n\n\n\n\n\n\nPokaż wyniki\n\n\n\n\n\n\n\nDokładność różnych drzew: 96.2% - 96.2%\nRozrzut: 0.0%\nRozwiązanie: Random Forest (następna ściągawka!)"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#real-world-przypadki-użycia",
    "href": "cheatsheets/03-decision-trees.html#real-world-przypadki-użycia",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "🌍 Real-world przypadki użycia",
    "text": "🌍 Real-world przypadki użycia\n\nBankowość: Ocena ryzyka kredytowego, wykrywanie fraudów\nMedycyna: Systemy wspomagania diagnostyki, triage pacjentów\n\nE-commerce: Rekomendacje produktów, ustalanie cen dynamicznych\nHR: Automatyzacja procesów rekrutacyjnych\nMarketing: Segmentacja klientów, personalizacja oferowania\n\n\n\n\n\n\n\n💡 Kiedy używać Decision Trees?\n\n\n\n✅ UŻYJ GDY:\n\nPotrzebujesz interpretowalnego modelu\nDane mają kategoryczne zmienne\n\nChcesz zrozumieć “dlaczego” model podjął decyzję\nMasz nieliniowe zależności w danych\nBraków w danych nie trzeba impute’ować\n\n❌ NIE UŻYWAJ GDY:\n\nPotrzebujesz najwyższej dokładności (użyj Random Forest/XGBoost)\nMasz bardzo głębokie wzorce w danych (użyj Neural Networks)\nDane są bardzo hałaśliwe\nPrzewidujesz wartości ciągłe (lepiej Linear Regression)\n\n\n\nNastępna ściągawka: Random Forest - zespół drzew (wkrótce)! 🌲🌲🌲"
  },
  {
    "objectID": "cheatsheets/06-svm.html",
    "href": "cheatsheets/06-svm.html",
    "title": "Support Vector Machines — optymalne granice decyzyjne",
    "section": "",
    "text": "SVM to algorytm, który znajduje optymalną granicę decyzyjną między klasami. Zamiast szukać “jakiejkolwiek” linii podziału, SVM znajduje tą, która maksymalizuje margines - odległość do najbliższych punktów z każdej klasy.\n\n\n\n\n\n\n💡 Intuicja\n\n\n\nWyobraź sobie sędziego, który musi wyznaczyć granicę między dwoma drużynami. SVM nie tylko znajdzie granicę, ale postawi ją tak, żeby była jak najdalej od najbliższych zawodników z obu stron - dla maksymalnego bezpieczeństwa!"
  },
  {
    "objectID": "cheatsheets/06-svm.html#czym-jest-support-vector-machine-svm",
    "href": "cheatsheets/06-svm.html#czym-jest-support-vector-machine-svm",
    "title": "Support Vector Machines — optymalne granice decyzyjne",
    "section": "",
    "text": "SVM to algorytm, który znajduje optymalną granicę decyzyjną między klasami. Zamiast szukać “jakiejkolwiek” linii podziału, SVM znajduje tą, która maksymalizuje margines - odległość do najbliższych punktów z każdej klasy.\n\n\n\n\n\n\n💡 Intuicja\n\n\n\nWyobraź sobie sędziego, który musi wyznaczyć granicę między dwoma drużynami. SVM nie tylko znajdzie granicę, ale postawi ją tak, żeby była jak najdalej od najbliższych zawodników z obu stron - dla maksymalnego bezpieczeństwa!"
  },
  {
    "objectID": "cheatsheets/06-svm.html#praktyczny-przykład-klasyfikacja-emaili-spamham",
    "href": "cheatsheets/06-svm.html#praktyczny-przykład-klasyfikacja-emaili-spamham",
    "title": "Support Vector Machines — optymalne granice decyzyjne",
    "section": "🎯 Praktyczny przykład: klasyfikacja emaili spam/ham",
    "text": "🎯 Praktyczny przykład: klasyfikacja emaili spam/ham\nCzy email to spam czy prawdziwa wiadomość?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\n\n# Tworzenie realistycznych danych emaili\nnp.random.seed(42)\nn_emails = 2000\n\n# Generuj różne typy emaili\nemail_types = np.random.choice(['spam', 'ham'], n_emails, p=[0.3, 0.7])\n\ndata = pd.DataFrame({\n    'dlugosc_tematu': np.random.randint(5, 100, n_emails),\n    'liczba_wielkich_liter': np.random.randint(0, 50, n_emails),\n    'liczba_wykrzyknikow': np.random.randint(0, 10, n_emails),\n    'liczba_linkow': np.random.randint(0, 20, n_emails),\n    'slowa_promocyjne': np.random.randint(0, 15, n_emails),\n    'dlugosc_wiadomosci': np.random.randint(50, 5000, n_emails),\n    'godzina_wyslania': np.random.randint(0, 24, n_emails),\n    'typ': email_types\n})\n\n# Realistyczne wzorce dla spam vs ham\nfor i, typ in enumerate(data['typ']):\n    if typ == 'spam':\n        # Spam charakterystyki\n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(20, 80)  # dłuższe tematy\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(10, 50)  # DUŻO WIELKIMI\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(3, 10)  # więcej !!!\n        data.loc[i, 'liczba_linkow'] = np.random.randint(5, 20)  # dużo linków\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(5, 15)  # PROMOCJA, GRATIS\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(200, 1000)  # średnie długości\n        data.loc[i, 'godzina_wyslania'] = np.random.choice([2, 3, 22, 23, 1])  # dziwne godziny\n    else:\n        # Ham (prawdziwe) charakterystyki  \n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(5, 40)  # krótsze tematy\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(0, 8)  # normalne używanie\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(0, 3)  # rzadko !\n        data.loc[i, 'liczba_linkow'] = np.random.randint(0, 3)  # mało linków\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(0, 2)  # prawie brak\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(50, 3000)  # różne długości\n        data.loc[i, 'godzina_wyslania'] = np.random.randint(8, 22)  # normalne godziny\n\n# Konwertuj target na binary\ndata['is_spam'] = (data['typ'] == 'spam').astype(int)\n\nprint(\"Statystyki emaili:\")\nprint(data.describe())\nprint(f\"\\nRozkład spam/ham:\")\nprint(data['typ'].value_counts())\nprint(f\"Procent spam: {data['is_spam'].mean():.1%}\")\n\n\n\n\n\n\n\nPokaż statystyki emaili\n\n\n\n\n\n\n\nStatystyki emaili:\n       dlugosc_tematu  liczba_wielkich_liter  liczba_wykrzyknikow  \\\ncount     2000.000000            2000.000000           2000.00000   \nmean        30.482500              11.392500              2.60800   \nstd         18.078609              13.675451              2.69369   \nmin          5.000000               0.000000              0.00000   \n25%         17.000000               2.000000              1.00000   \n50%         27.000000               5.000000              2.00000   \n75%         38.000000              17.000000              4.00000   \nmax         79.000000              49.000000              9.00000   \n\n       liczba_linkow  slowa_promocyjne  dlugosc_wiadomosci  godzina_wyslania  \\\ncount    2000.000000       2000.000000         2000.000000       2000.000000   \nmean        4.360000          3.302500         1240.088000         13.031000   \nstd         5.642279          4.524183          834.469147          6.857445   \nmin         0.000000          0.000000           56.000000          1.000000   \n25%         1.000000          0.000000          539.000000          9.000000   \n50%         2.000000          1.000000          958.000000         14.000000   \n75%         7.000000          6.000000         1905.500000         19.000000   \nmax        19.000000         14.000000         2998.000000         23.000000   \n\n           is_spam  \ncount  2000.000000  \nmean      0.307500  \nstd       0.461574  \nmin       0.000000  \n25%       0.000000  \n50%       0.000000  \n75%       1.000000  \nmax       1.000000  \n\nRozkład spam/ham:\ntyp\nham     1385\nspam     615\nName: count, dtype: int64\nProcent spam: 30.8%"
  },
  {
    "objectID": "cheatsheets/06-svm.html#budowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/06-svm.html#budowanie-modelu-krok-po-kroku",
    "title": "Support Vector Machines — optymalne granice decyzyjne",
    "section": "🔧 Budowanie modelu krok po kroku",
    "text": "🔧 Budowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Features do modelu\nfeatures = ['dlugosc_tematu', 'liczba_wielkich_liter', 'liczba_wykrzyknikow', \n           'liczba_linkow', 'slowa_promocyjne', 'dlugosc_wiadomosci', 'godzina_wyslania']\nX = data[features]\ny = data['is_spam']\n\n# Podział train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standaryzacja - KLUCZOWA dla SVM!\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"Dane treningowe: {len(X_train)} emaili\")\nprint(f\"Dane testowe: {len(X_test)} emaili\")\nprint(f\"Spam w train: {y_train.mean():.1%}\")\nprint(f\"Spam w test: {y_test.mean():.1%}\")\n\nprint(\"\\nPrzykład standaryzacji:\")\nprint(\"Przed:\", X_train.iloc[0].values)\nprint(\"Po:\", X_train_scaled[0])\n\n\n\n\n\n\n\nPokaż podział danych\n\n\n\n\n\n\n\nDane treningowe: 1600 emaili\nDane testowe: 400 emaili\nSpam w train: 30.9%\nSpam w test: 30.0%\n\nPrzykład standaryzacji:\nPrzed: [   6    1    0    2    0 2057   21]\nPo: [-1.36651173 -0.75972497 -0.9773396  -0.41945283 -0.73519846  1.00688364\n  1.1681456 ]\n\n\n\n\n\n\n\n2) Trenowanie różnych kerneli SVM\n\n# Porównanie różnych kerneli\nkernels = ['linear', 'rbf', 'poly']\nsvm_results = {}\n\nfor kernel in kernels:\n    print(f\"\\nTrenowanie SVM z kernelem: {kernel}\")\n    \n    if kernel == 'poly':\n        svm = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm = SVC(kernel=kernel, random_state=42)\n    \n    # Trenowanie\n    svm.fit(X_train_scaled, y_train)\n    \n    # Predykcje\n    y_pred = svm.predict(X_test_scaled)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    svm_results[kernel] = {\n        'model': svm,\n        'accuracy': accuracy,\n        'predictions': y_pred\n    }\n    \n    print(f\"Dokładność: {accuracy:.1%}\")\n    print(f\"Liczba support vectors: {svm.n_support_}\")\n\n# Znajdź najlepszy kernel\nbest_kernel = max(svm_results.keys(), key=lambda k: svm_results[k]['accuracy'])\nbest_model = svm_results[best_kernel]['model']\n\nprint(f\"\\n🏆 Najlepszy kernel: {best_kernel}\")\nprint(f\"Najlepsza dokładność: {svm_results[best_kernel]['accuracy']:.1%}\")\n\n\n\n\n\n\n\nPokaż wyniki różnych kerneli\n\n\n\n\n\n\n\n\nKernel linear:\nDokładność: 100.0%\nLiczba support vectors: [2 3]\n\nKernel rbf:\nDokładność: 100.0%\nLiczba support vectors: [11 26]\n\nKernel poly:\nDokładność: 100.0%\nLiczba support vectors: [16 16]\n\n🏆 Najlepszy kernel: linear\nNajlepsza dokładność: 100.0%\n\n\n\n\n\n\n\n3) Ewaluacja najlepszego modelu\n\n# Szczegółowa analiza najlepszego modelu\nbest_predictions = svm_results[best_kernel]['predictions']\n\nprint(f\"SZCZEGÓŁOWA ANALIZA - SVM {best_kernel.upper()}\")\nprint(\"=\" * 50)\n\n# Classification report\nprint(\"\\nRaport klasyfikacji:\")\nprint(classification_report(y_test, best_predictions, target_names=['Ham', 'Spam']))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, best_predictions)\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:     Ham    Spam\")\nprint(f\"Rzeczywiste Ham:  {cm[0,0]:3d}    {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Spam: {cm[1,0]:3d}    {cm[1,1]:3d}\")\n\n# Analiza błędów\nfalse_positives = np.where((y_test == 0) & (best_predictions == 1))[0]\nfalse_negatives = np.where((y_test == 1) & (best_predictions == 0))[0]\n\nprint(f\"\\nAnaliza błędów:\")\nprint(f\"False Positives (Ham → Spam): {len(false_positives)}\")\nprint(f\"False Negatives (Spam → Ham): {len(false_negatives)}\")\n\nif len(false_positives) &gt; 0:\n    print(f\"\\nPrzykład błędnie sklasyfikowanego Ham jako Spam:\")\n    fp_idx = false_positives[0]\n    print(f\"Email: {X_test.iloc[fp_idx].to_dict()}\")\n\n\n\n\n\n\n\nPokaż szczegółową analizę\n\n\n\n\n\n\n\nSZCZEGÓŁOWA ANALIZA - SVM LINEAR\n==================================================\n\nRaport klasyfikacji:\n              precision    recall  f1-score   support\n\n         Ham       1.00      1.00      1.00       280\n        Spam       1.00      1.00      1.00       120\n\n    accuracy                           1.00       400\n   macro avg       1.00      1.00      1.00       400\nweighted avg       1.00      1.00      1.00       400\n\n\nConfusion Matrix:\nPrzewidywane:     Ham    Spam\nRzeczywiste Ham:  280      0\nRzeczywiste Spam:   0    120\n\nAnaliza błędów:\nFalse Positives (Ham → Spam): 0\nFalse Negatives (Spam → Ham): 0"
  },
  {
    "objectID": "cheatsheets/06-svm.html#wizualizacja-granic-decyzyjnych",
    "href": "cheatsheets/06-svm.html#wizualizacja-granic-decyzyjnych",
    "title": "Support Vector Machines — optymalne granice decyzyjne",
    "section": "📊 Wizualizacja granic decyzyjnych",
    "text": "📊 Wizualizacja granic decyzyjnych\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Przygotuj dane (powtarzamy dla kompletności)\nnp.random.seed(42)\nn_emails = 2000\n\nemail_types = np.random.choice(['spam', 'ham'], n_emails, p=[0.3, 0.7])\n\ndata = pd.DataFrame({\n    'dlugosc_tematu': np.random.randint(5, 100, n_emails),\n    'liczba_wielkich_liter': np.random.randint(0, 50, n_emails),\n    'liczba_wykrzyknikow': np.random.randint(0, 10, n_emails),\n    'liczba_linkow': np.random.randint(0, 20, n_emails),\n    'slowa_promocyjne': np.random.randint(0, 15, n_emails),\n    'dlugosc_wiadomosci': np.random.randint(50, 5000, n_emails),\n    'godzina_wyslania': np.random.randint(0, 24, n_emails),\n    'typ': email_types\n})\n\n# Popraw charakterystyki\nfor i, typ in enumerate(data['typ']):\n    if typ == 'spam':\n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(20, 80)\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(10, 50)\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(3, 10)\n        data.loc[i, 'liczba_linkow'] = np.random.randint(5, 20)\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(5, 15)\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(200, 1000)\n        data.loc[i, 'godzina_wyslania'] = np.random.choice([2, 3, 22, 23, 1])\n    else:\n        data.loc[i, 'dlugosc_tematu'] = np.random.randint(5, 40)\n        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(0, 8)\n        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(0, 3)\n        data.loc[i, 'liczba_linkow'] = np.random.randint(0, 3)\n        data.loc[i, 'slowa_promocyjne'] = np.random.randint(0, 2)\n        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(50, 3000)\n        data.loc[i, 'godzina_wyslania'] = np.random.randint(8, 22)\n\ndata['is_spam'] = (data['typ'] == 'spam').astype(int)\n\n# Wybierz tylko 2 features dla wizualizacji 2D\nfeatures_2d = ['liczba_wielkich_liter', 'liczba_wykrzyknikow']\nX_2d = data[features_2d]\ny = data['is_spam']\n\nX_train_2d, X_test_2d, y_train, y_test = train_test_split(X_2d, y, test_size=0.2, random_state=42)\n\nscaler_2d = StandardScaler()\nX_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)\nX_test_2d_scaled = scaler_2d.transform(X_test_2d)\n\n# Trenuj modele z różnymi kernelami (na 2D)\nkernels = ['linear', 'rbf', 'poly']\nplt.figure(figsize=(15, 5))\n\nfor i, kernel in enumerate(kernels):\n    plt.subplot(1, 3, i+1)\n    \n    # Trenuj model\n    if kernel == 'poly':\n        svm = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm = SVC(kernel=kernel, random_state=42)\n    \n    svm.fit(X_train_2d_scaled, y_train)\n    \n    # Stwórz mesh do wizualizacji granic\n    h = 0.02\n    x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1\n    y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    # Predykcje na mesh\n    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    # Narysuj granice i punkty\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    scatter = plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], \n                         c=y_train, cmap=plt.cm.RdYlBu, edgecolors='black')\n    \n    # Podświetl support vectors\n    if hasattr(svm, 'support_'):\n        plt.scatter(X_train_2d_scaled[svm.support_, 0], \n                   X_train_2d_scaled[svm.support_, 1],\n                   s=100, facecolors='none', edgecolors='black', linewidth=2)\n    \n    plt.xlabel('Liczba wielkich liter (scaled)')\n    plt.ylabel('Liczba wykrzykników (scaled)')\n    plt.title(f'SVM - {kernel.upper()} kernel')\n    \n    # Dokładność\n    accuracy = accuracy_score(y_test, svm.predict(X_test_2d_scaled))\n    plt.text(0.02, 0.98, f'Accuracy: {accuracy:.1%}', \n             transform=plt.gca().transAxes, \n             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n             verticalalignment='top')\n\nplt.tight_layout()\nplt.close()\n\n# Przygotuj dane dla pełnego modelu\nfeatures = ['dlugosc_tematu', 'liczba_wielkich_liter', 'liczba_wykrzyknikow', \n           'liczba_linkow', 'slowa_promocyjne', 'dlugosc_wiadomosci', 'godzina_wyslania']\nX_full = data[features]\nX_train_full, X_test_full, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42)\n\nscaler_full = StandardScaler()\nX_train_full_scaled = scaler_full.fit_transform(X_train_full)\nX_test_full_scaled = scaler_full.transform(X_test_full)\n\n# Porównaj wyniki\nprint(\"PORÓWNANIE KERNELI (2D vs 7D features):\")\nprint(\"=\" * 50)\n\nfor kernel in kernels:\n    # 2D model\n    if kernel == 'poly':\n        svm_2d = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm_2d = SVC(kernel=kernel, random_state=42)\n    svm_2d.fit(X_train_2d_scaled, y_train)\n    acc_2d = accuracy_score(y_test, svm_2d.predict(X_test_2d_scaled))\n    \n    # 7D model\n    if kernel == 'poly':\n        svm_7d = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm_7d = SVC(kernel=kernel, random_state=42)\n    svm_7d.fit(X_train_full_scaled, y_train)\n    acc_7d = accuracy_score(y_test, svm_7d.predict(X_test_full_scaled))\n    \n    print(f\"{kernel.upper():6} - 2D: {acc_2d:.1%}, 7D: {acc_7d:.1%}, Poprawa: +{(acc_7d-acc_2d)*100:.1f}pp\")\n\n# Odtwórz wizualizację\nplt.figure(figsize=(15, 5))\n\nfor i, kernel in enumerate(kernels):\n    plt.subplot(1, 3, i+1)\n    \n    if kernel == 'poly':\n        svm = SVC(kernel=kernel, degree=3, random_state=42)\n    else:\n        svm = SVC(kernel=kernel, random_state=42)\n    \n    svm.fit(X_train_2d_scaled, y_train)\n    \n    h = 0.02\n    x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1\n    y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    \n    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    scatter = plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], \n                         c=y_train, cmap=plt.cm.RdYlBu, edgecolors='black')\n    \n    if hasattr(svm, 'support_'):\n        plt.scatter(X_train_2d_scaled[svm.support_, 0], \n                   X_train_2d_scaled[svm.support_, 1],\n                   s=100, facecolors='none', edgecolors='black', linewidth=2)\n    \n    plt.xlabel('Liczba wielkich liter (scaled)')\n    plt.ylabel('Liczba wykrzykników (scaled)')\n    plt.title(f'SVM - {kernel.upper()} kernel')\n    \n    accuracy = accuracy_score(y_test, svm.predict(X_test_2d_scaled))\n    plt.text(0.02, 0.98, f'Accuracy: {accuracy:.1%}', \n             transform=plt.gca().transAxes, \n             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n             verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nPokaż wizualizację granic i porównanie\n\n\n\n\n\n\n\nPORÓWNANIE KERNELI (2D vs 7D features):\n==================================================\nLINEAR - 2D: 100.0%, 7D: 100.0%, Poprawa: +0.0pp\nRBF    - 2D: 100.0%, 7D: 100.0%, Poprawa: +0.0pp\nPOLY   - 2D: 98.2%, 7D: 100.0%, Poprawa: +1.7pp\n\n\n\n\n\nWizualizacja granic decyzyjnych SVM dla różnych kerneli"
  },
  {
    "objectID": "cheatsheets/06-svm.html#rodzaje-kerneli-i-ich-zastosowania",
    "href": "cheatsheets/06-svm.html#rodzaje-kerneli-i-ich-zastosowania",
    "title": "Support Vector Machines — optymalne granice decyzyjne",
    "section": "🎯 Rodzaje kerneli i ich zastosowania",
    "text": "🎯 Rodzaje kerneli i ich zastosowania\n\n1) Linear Kernel - proste granice\n\n# Użyj gdy:\n# - Dane są liniowo separowalne\n# - Masz dużo features (high-dimensional)\n# - Potrzebujesz szybkiego modelu\n# - Chcesz interpretowalność\n\nlinear_use_cases = [\n    \"Text classification (high-dimensional sparse data)\",\n    \"Gene expression analysis\", \n    \"Document categorization\",\n    \"Sentiment analysis z bag-of-words\"\n]\n\nprint(\"LINEAR KERNEL - najlepsze zastosowania:\")\nfor i, use_case in enumerate(linear_use_cases, 1):\n    print(f\"{i}. {use_case}\")\n\n# Demo: Linear SVM na prostych danych\nX_simple = np.random.randn(100, 2)\ny_simple = (X_simple[:, 0] + X_simple[:, 1] &gt; 0).astype(int)\n\nlinear_svm = SVC(kernel='linear')\nlinear_svm.fit(X_simple, y_simple)\nprint(f\"\\nLinear SVM na prostych danych: {linear_svm.score(X_simple, y_simple):.1%}\")\n\n\n\n\n\n\n\nPokaż zastosowania Linear Kernel\n\n\n\n\n\n\n\nLINEAR KERNEL - najlepsze zastosowania:\n1. Text classification (high-dimensional sparse data)\n2. Gene expression analysis\n3. Document categorization\n4. Sentiment analysis z bag-of-words\n\nLinear SVM na prostych danych: 98.0%\n\n\n\n\n\n\n\n2) RBF Kernel - uniwersalny wybór\n\n# RBF (Radial Basis Function) - najczęściej używany\n# Może aproksymować dowolną granicę decyzyjną!\n\nrbf_use_cases = [\n    \"Image classification\",\n    \"Medical diagnosis\", \n    \"Fraud detection\",\n    \"Customer segmentation\",\n    \"General classification tasks\"\n]\n\nprint(\"RBF KERNEL - najlepsze zastosowania:\")\nfor i, use_case in enumerate(rbf_use_cases, 1):\n    print(f\"{i}. {use_case}\")\n\n# Demo: wpływ parametru gamma\ngammas = [0.1, 1.0, 10.0]\nprint(f\"\\nWpływ parametru gamma w RBF:\")\n\nfor gamma in gammas:\n    rbf_svm = SVC(kernel='rbf', gamma=gamma, random_state=42)\n    rbf_svm.fit(X_train_full_scaled, y_train)\n    accuracy = rbf_svm.score(X_test_full_scaled, y_test)\n    print(f\"Gamma {gamma:4.1f}: Accuracy = {accuracy:.1%}, Support Vectors = {rbf_svm.n_support_.sum()}\")\n\nprint(\"\\n💡 Gamma wysoka = bardziej skomplikowane granice\")\nprint(\"💡 Gamma niska = gładsze granice\")\n\n\n\n\n\n\n\nPokaż zastosowania RBF Kernel\n\n\n\n\n\n\n\nRBF KERNEL - najlepsze zastosowania:\n1. Image classification\n2. Medical diagnosis\n3. Fraud detection\n4. Customer segmentation\n5. General classification tasks\n\nWpływ parametru gamma w RBF:\nGamma  0.1: Accuracy = 100.0%, Support Vectors = 26\nGamma  1.0: Accuracy = 100.0%, Support Vectors = 374\nGamma 10.0: Accuracy = 100.0%, Support Vectors = 1546\n\n💡 Gamma wysoka = bardziej skomplikowane granice\n💡 Gamma niska = gładsze granice\n\n\n\n\n\n\n\n3) Polynomial Kernel - dla złożonych wzorców\n\npoly_use_cases = [\n    \"Computer vision (shape recognition)\",\n    \"Bioinformatics (protein classification)\",\n    \"Natural language processing\",\n    \"Pattern recognition\"\n]\n\nprint(\"POLYNOMIAL KERNEL - najlepsze zastosowania:\")\nfor i, use_case in enumerate(poly_use_cases, 1):\n    print(f\"{i}. {use_case}\")\n\n# Demo: wpływ stopnia wielomianu\ndegrees = [2, 3, 4, 5]\nprint(f\"\\nWpływ stopnia wielomianu:\")\n\nfor degree in degrees:\n    poly_svm = SVC(kernel='poly', degree=degree, random_state=42)\n    poly_svm.fit(X_train_full_scaled, y_train)\n    accuracy = poly_svm.score(X_test_full_scaled, y_test)\n    print(f\"Degree {degree}: Accuracy = {accuracy:.1%}\")\n\nprint(\"\\n⚠️ Uwaga: wyższie stopnie = ryzyko overfittingu!\")\n\n\n\n\n\n\n\nPokaż zastosowania Polynomial Kernel\n\n\n\n\n\n\n\nPOLYNOMIAL KERNEL - najlepsze zastosowania:\n1. Computer vision (shape recognition)\n2. Bioinformatics (protein classification)\n3. Natural language processing\n4. Pattern recognition\n\nWpływ stopnia wielomianu:\nDegree 2: Accuracy = 99.8%\nDegree 3: Accuracy = 100.0%\nDegree 4: Accuracy = 99.5%\nDegree 5: Accuracy = 100.0%\n\n⚠️ Uwaga: wyższie stopnie = ryzyko overfittingu!"
  },
  {
    "objectID": "cheatsheets/06-svm.html#tuning-parametrów-svm",
    "href": "cheatsheets/06-svm.html#tuning-parametrów-svm",
    "title": "Support Vector Machines — optymalne granice decyzyjne",
    "section": "⚙️ Tuning parametrów SVM",
    "text": "⚙️ Tuning parametrów SVM\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Grid Search dla RBF kernel\nparam_grid_rbf = {\n    'C': [0.1, 1, 10, 100],           # regularization strength\n    'gamma': [0.001, 0.01, 0.1, 1]   # kernel coefficient\n}\n\nprint(\"Rozpoczynam Grid Search dla RBF SVM...\")\ngrid_search = GridSearchCV(\n    SVC(kernel='rbf', random_state=42),\n    param_grid_rbf,\n    cv=3,  # 3-fold CV dla szybkości\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=0\n)\n\ngrid_search.fit(X_train_full_scaled, y_train)\n\nprint(\"Najlepsze parametry RBF:\")\nprint(grid_search.best_params_)\nprint(f\"Najlepsza dokładność CV: {grid_search.best_score_:.1%}\")\n\n# Test na zbiorze testowym\nbest_svm = grid_search.best_estimator_\ntest_accuracy = best_svm.score(X_test_full_scaled, y_test)\nprint(f\"Dokładność na test set: {test_accuracy:.1%}\")\n\n# Porównanie przed/po tuningu\nbaseline_svm = SVC(kernel='rbf', random_state=42)\nbaseline_svm.fit(X_train_full_scaled, y_train)\nbaseline_accuracy = baseline_svm.score(X_test_full_scaled, y_test)\n\nprint(f\"\\nPorównanie:\")\nprint(f\"Baseline RBF:      {baseline_accuracy:.1%}\")\nprint(f\"Tuned RBF:         {test_accuracy:.1%}\")\nprint(f\"Poprawa:           +{(test_accuracy - baseline_accuracy)*100:.1f}pp\")\n\n\n\n\n\n\n\nPokaż wyniki tuningu\n\n\n\n\n\n\n\nNajlepsze parametry RBF:\n{'C': 0.1, 'gamma': 0.01}\nNajlepsza dokładność CV: 100.0%\nDokładność na test set: 100.0%\n\nPorównanie:\nBaseline RBF:      100.0%\nTuned RBF:         100.0%\nPoprawa:           +0.0pp"
  },
  {
    "objectID": "cheatsheets/06-svm.html#pułapki-i-rozwiązania",
    "href": "cheatsheets/06-svm.html#pułapki-i-rozwiązania",
    "title": "Support Vector Machines — optymalne granice decyzyjne",
    "section": "⚠️ Pułapki i rozwiązania",
    "text": "⚠️ Pułapki i rozwiązania\n\n1) Brak standaryzacji\n\n# Problem: SVM jest bardzo wrażliwy na skale danych\nprint(\"DEMONSTRACJA: wpływ standaryzacji\")\n\n# Bez standaryzacji\nsvm_unscaled = SVC(kernel='rbf', random_state=42)\nsvm_unscaled.fit(X_train_full, y_train)  # raw data!\nacc_unscaled = svm_unscaled.score(X_test_full, y_test)\n\n# Ze standaryzacją\nsvm_scaled = SVC(kernel='rbf', random_state=42)\nsvm_scaled.fit(X_train_full_scaled, y_train)  # scaled data!\nacc_scaled = svm_scaled.score(X_test_full_scaled, y_test)\n\nprint(f\"Bez standaryzacji: {acc_unscaled:.1%}\")\nprint(f\"Ze standaryzacją:  {acc_scaled:.1%}\")\nprint(f\"Poprawa:           +{(acc_scaled - acc_unscaled)*100:.1f}pp\")\nprint(\"\\n✅ ZAWSZE standaryzuj dane przed SVM!\")\n\n\n\n\n\n\n\nPokaż wpływ standaryzacji\n\n\n\n\n\n\n\nBez standaryzacji: 88.0%\nZe standaryzacją:  100.0%\nPoprawa:           +12.0pp\n\n✅ ZAWSZE standaryzuj dane przed SVM!\n\n\n\n\n\n\n\n2) Niezbalansowane klasy\n\n# Problem: gdy jedna klasa jest bardzo rzadka\nprint(\"PROBLEM: niezbalansowane klasy\")\n\n# Sprawdź balans w naszych danych\nprint(f\"Balans klas: Spam {y_train.mean():.1%}, Ham {(1-y_train.mean()):.1%}\")\n\n# Rozwiązanie 1: class_weight='balanced'\nsvm_balanced = SVC(kernel='rbf', class_weight='balanced', random_state=42)\nsvm_balanced.fit(X_train_full_scaled, y_train)\n\n# Rozwiązanie 2: manual class weights\nspam_weight = len(y_train) / (2 * y_train.sum())  # 1/frequency\nham_weight = len(y_train) / (2 * (len(y_train) - y_train.sum()))\nclass_weights = {0: ham_weight, 1: spam_weight}\n\nsvm_manual = SVC(kernel='rbf', class_weight=class_weights, random_state=42)\nsvm_manual.fit(X_train_full_scaled, y_train)\n\n# Porównanie\nsvm_normal = SVC(kernel='rbf', random_state=42)\nsvm_normal.fit(X_train_full_scaled, y_train)\n\nmodels = {\n    'Normal': svm_normal,\n    'Balanced': svm_balanced, \n    'Manual weights': svm_manual\n}\n\nprint(\"\\nPorównanie podejść:\")\nfor name, model in models.items():\n    pred = model.predict(X_test_full_scaled)\n    accuracy = accuracy_score(y_test, pred)\n    \n    # Sprawdź recall dla spam (ważne!)\n    spam_indices = y_test == 1\n    spam_recall = np.mean(pred[spam_indices] == 1) if spam_indices.sum() &gt; 0 else 0\n    \n    print(f\"{name:12}: Accuracy={accuracy:.1%}, Spam Recall={spam_recall:.1%}\")\n\n\n\n\n\n\n\nPokaż rozwiązania dla niezbalansowanych klas\n\n\n\n\n\n\n\nBalans klas: Spam 30.9%, Ham 69.1%\n\nPorównanie podejść:\nNormal      : Accuracy=100.0%, Spam Recall=100.0%\nBalanced    : Accuracy=100.0%, Spam Recall=100.0%\nManual weights: Accuracy=100.0%, Spam Recall=100.0%\n\n\n\n\n\n\n\n3) Overfitting z wysokim C\n\n# Problem: zbyt wysoka wartość C prowadzi do overfittingu\nC_values = [0.01, 0.1, 1, 10, 100, 1000]\nresults = []\n\nprint(\"Wpływ parametru C (regularization):\")\nprint(\"C      | Train Acc | Test Acc | Difference | Support Vectors\")\nprint(\"-\" * 60)\n\nfor C in C_values:\n    svm = SVC(kernel='rbf', C=C, random_state=42)\n    svm.fit(X_train_full_scaled, y_train)\n    \n    train_acc = svm.score(X_train_full_scaled, y_train)\n    test_acc = svm.score(X_test_full_scaled, y_test)\n    diff = train_acc - test_acc\n    n_sv = svm.n_support_.sum()\n    \n    print(f\"{C:6.2f} | {train_acc:8.1%} | {test_acc:7.1%} | {diff:9.1%} | {n_sv:14d}\")\n    results.append((C, train_acc, test_acc, diff, n_sv))\n\nprint(\"\\n💡 Optimalne C: wysokie test accuracy, mała różnica train-test\")\nprint(\"💡 Zbyt wysokie C: perfect train accuracy, słabe test accuracy\")\n\n\n\n\n\n\n\nPokaż wpływ parametru C\n\n\n\n\n\n\n\nWpływ parametru C (regularization):\nC      | Train Acc | Test Acc | Difference | Support Vectors\n------------------------------------------------------------\n  0.01 |   100.0% |  100.0% |      0.0% |            582\n  0.10 |   100.0% |  100.0% |      0.0% |             99\n  1.00 |   100.0% |  100.0% |      0.0% |             37\n 10.00 |   100.0% |  100.0% |      0.0% |             35\n100.00 |   100.0% |  100.0% |      0.0% |             35\n1000.00 |   100.0% |  100.0% |      0.0% |             35\n\n💡 Optimalne C: wysokie test accuracy, mała różnica train-test\n💡 Zbyt wysokie C: perfect train accuracy, słabe test accuracy"
  },
  {
    "objectID": "cheatsheets/06-svm.html#real-world-przypadki-użycia",
    "href": "cheatsheets/06-svm.html#real-world-przypadki-użycia",
    "title": "Support Vector Machines — optymalne granice decyzyjne",
    "section": "🌍 Real-world przypadki użycia",
    "text": "🌍 Real-world przypadki użycia\n\nText Classification: Spam detection, sentiment analysis, document categorization\nComputer Vision: Image classification, face recognition, medical image analysis\nFinance: Credit scoring, algorithmic trading, fraud detection\nHealthcare: Disease diagnosis, drug discovery, medical image analysis\nCybersecurity: Intrusion detection, malware classification, network security\n\n\n\n\n\n\n\n💡 Kiedy używać SVM?\n\n\n\n✅ UŻYJ GDY:\n\nPotrzebujesz wysokiej dokładności klasyfikacji\nMasz średniej wielkości dataset (1K-100K samples)\nDane mają clear margins między klasami\nNie potrzebujesz probabilistic outputs\nMemory efficiency jest ważna\n\n❌ NIE UŻYWAJ GDY:\n\nMasz bardzo duże datasety (&gt;100K samples - użyj Neural Networks)\nPotrzebujesz probabilistic predictions (użyj Logistic Regression)\nDane mają dużo noise’u (użyj Random Forest)\nTarget jest continuous (użyj SVR - Support Vector Regression)\nPotrzebujesz bardzo szybkiego inference (użyj Decision Tree)\n\n\n\nPodsumowanie: SVM to potężny algoritm do klasyfikacji z optymalną separacją klas. Wybierz kernel w zależności od złożoności danych: Linear dla prostych, RBF dla uniwersalnych, Polynomial dla specjalnych przypadków. 🎯"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html",
    "href": "cheatsheets/04-random-forest.html",
    "title": "Random Forest — las drzew decyzyjnych",
    "section": "",
    "text": "Random Forest to zespół wielu drzew decyzyjnych, które głosują razem nad końcową decyzją. Jeden Decision Tree może się mylić, ale gdy masz 100 drzew i większość mówi “TAK” - to prawdopodobnie dobra odpowiedź!\n\n\n\n\n\n\n💡 Intuicja\n\n\n\nWyobraź sobie konsylium lekarskim: jeden lekarz może się pomylić w diagnozie, ale gdy 7 z 10 ekspertów zgadza się - diagnoza jest znacznie bardziej pewna. Random Forest działa dokładnie tak samo!"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#czym-jest-random-forest",
    "href": "cheatsheets/04-random-forest.html#czym-jest-random-forest",
    "title": "Random Forest — las drzew decyzyjnych",
    "section": "",
    "text": "Random Forest to zespół wielu drzew decyzyjnych, które głosują razem nad końcową decyzją. Jeden Decision Tree może się mylić, ale gdy masz 100 drzew i większość mówi “TAK” - to prawdopodobnie dobra odpowiedź!\n\n\n\n\n\n\n💡 Intuicja\n\n\n\nWyobraź sobie konsylium lekarskim: jeden lekarz może się pomylić w diagnozie, ale gdy 7 z 10 ekspertów zgadza się - diagnoza jest znacznie bardziej pewna. Random Forest działa dokładnie tak samo!"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#praktyczny-przykład-przewidywanie-sukcesu-produktu",
    "href": "cheatsheets/04-random-forest.html#praktyczny-przykład-przewidywanie-sukcesu-produktu",
    "title": "Random Forest — las drzew decyzyjnych",
    "section": "🎯 Praktyczny przykład: przewidywanie sukcesu produktu",
    "text": "🎯 Praktyczny przykład: przewidywanie sukcesu produktu\nCzy nowy produkt odniesie sukces na rynku?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Tworzenie realistycznych danych produktów\nnp.random.seed(42)\nn_products = 2000\n\ndata = pd.DataFrame({\n    'cena': np.random.lognormal(6, 1, n_products),  # log-normal dla cen\n    'jakosc': np.random.randint(1, 11, n_products),  # 1-10 skala jakości\n    'marketing_budget': np.random.exponential(50000, n_products),\n    'konkurencja': np.random.randint(1, 21, n_products),  # liczba konkurentów\n    'sezon': np.random.choice(['wiosna', 'lato', 'jesien', 'zima'], n_products),\n    'kategoria': np.random.choice(['elektronika', 'moda', 'dom', 'sport', 'ksiazki'], n_products),\n    'ocena_testowa': np.random.normal(7, 2, n_products),  # oceny focus group\n    'innowacyjnosc': np.random.randint(1, 11, n_products),\n    'marka_znana': np.random.choice([0, 1], n_products, p=[0.3, 0.7])  # czy znana marka\n})\n\n# Realistyczna logika sukcesu produktu\ndef czy_sukces(row):\n    score = 0\n    \n    # Cena (sweet spot 50-500)\n    if 50 &lt;= row['cena'] &lt;= 500:\n        score += 2\n    elif row['cena'] &gt; 1000:\n        score -= 2\n    \n    # Jakość (najważniejsze!)\n    score += row['jakosc'] * 0.8\n    \n    # Marketing\n    if row['marketing_budget'] &gt; 100000:\n        score += 3\n    elif row['marketing_budget'] &gt; 50000:\n        score += 1\n    \n    # Konkurencja (mniej = lepiej)\n    score -= row['konkurencja'] * 0.2\n    \n    # Sezon (lato i zima lepsze)\n    if row['sezon'] in ['lato', 'zima']:\n        score += 1\n    \n    # Kategoria\n    if row['kategoria'] == 'elektronika':\n        score += 1\n    elif row['kategoria'] == 'ksiazki':\n        score -= 1\n    \n    # Ocena testowa\n    score += (row['ocena_testowa'] - 5) * 0.5\n    \n    # Innowacyjność\n    score += row['innowacyjnosc'] * 0.3\n    \n    # Znana marka\n    if row['marka_znana']:\n        score += 2\n    \n    # Końcowa decyzja z odrobiną losowości\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 1)))\n    return probability &gt; 0.5\n\ndata['sukces'] = data.apply(czy_sukces, axis=1)\n\nprint(\"Statystyki produktów:\")\nprint(data.describe())\nprint(f\"\\nOdsetek udanych produktów: {data['sukces'].mean():.1%}\")\n\n\n\n\n\n\n\nPokaż statystyki produktów\n\n\n\n\n\n\n\nStatystyki produktów:\n               cena       jakosc  marketing_budget  konkurencja  \\\ncount   2000.000000  2000.000000       2000.000000   2000.00000   \nmean     692.530569     5.534500      49376.934612     10.53950   \nstd      929.701850     2.871922      49876.166534      5.76594   \nmin       15.779832     1.000000         11.353200      1.00000   \n25%      216.445352     3.000000      13957.783468      6.00000   \n50%      421.867820     6.000000      33526.684882     11.00000   \n75%      798.695019     8.000000      67692.822351     16.00000   \nmax    19010.210160    10.000000     376260.171802     20.00000   \n\n       ocena_testowa  innowacyjnosc  marka_znana  \ncount    2000.000000    2000.000000  2000.000000  \nmean        7.011461       5.500000     0.711000  \nstd         1.957420       2.909679     0.453411  \nmin        -0.844801       1.000000     0.000000  \n25%         5.700781       3.000000     0.000000  \n50%         7.041344       6.000000     1.000000  \n75%         8.307413       8.000000     1.000000  \nmax        13.754766      10.000000     1.000000  \n\nOdsetek udanych produktów: 99.2%"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#budowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/04-random-forest.html#budowanie-modelu-krok-po-kroku",
    "title": "Random Forest — las drzew decyzyjnych",
    "section": "🔧 Budowanie modelu krok po kroku",
    "text": "🔧 Budowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Encoding kategorycznych zmiennych\nfrom sklearn.preprocessing import LabelEncoder\n\ndata_encoded = data.copy()\nle_sezon = LabelEncoder()\nle_kategoria = LabelEncoder()\n\ndata_encoded['sezon_num'] = le_sezon.fit_transform(data['sezon'])\ndata_encoded['kategoria_num'] = le_kategoria.fit_transform(data['kategoria'])\n\n# Features do modelu\nfeature_columns = ['cena', 'jakosc', 'marketing_budget', 'konkurencja', \n                  'sezon_num', 'kategoria_num', 'ocena_testowa', \n                  'innowacyjnosc', 'marka_znana']\nX = data_encoded[feature_columns]\ny = data_encoded['sukces']\n\n# Podział train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Dane treningowe: {len(X_train)} produktów\")\nprint(f\"Dane testowe: {len(X_test)} produktów\")\n\n\n\n\n\n\n\nPokaż podział danych\n\n\n\n\n\n\n\nDane treningowe: 1600 produktów\nDane testowe: 400 produktów\n\n\n\n\n\n\n\n2) Trenowanie Random Forest\n\n# Tworzenie modelu Random Forest\nrf_model = RandomForestClassifier(\n    n_estimators=100,       # liczba drzew w lesie\n    max_depth=10,          # maksymalna głębokość każdego drzewa\n    min_samples_split=20,   # min. próbek do podziału\n    min_samples_leaf=10,    # min. próbek w liściu\n    random_state=42,\n    n_jobs=-1              # użyj wszystkie rdzenie CPU\n)\n\n# Trenowanie\nrf_model.fit(X_train, y_train)\n\nprint(\"Random Forest wytrenowany!\")\nprint(f\"Liczba drzew: {rf_model.n_estimators}\")\nprint(f\"Średnia głębokość drzew: {np.mean([tree.tree_.max_depth for tree in rf_model.estimators_]):.1f}\")\n\n\n\n\n\n\n\nPokaż parametry wytrenowanego modelu\n\n\n\n\n\n\n\nRandom Forest wytrenowany!\nLiczba drzew: 100\nŚrednia głębokość drzew: 5.2\n\n\n\n\n\n\n\n3) Ewaluacja modelu\n\n# Predykcje\ny_pred = rf_model.predict(X_test)\ny_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n\n# Metryki\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nDokładność Random Forest: {accuracy:.1%}\")\n\n# Szczegółowy raport\nprint(\"\\nRaport klasyfikacji:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:     Porażka  Sukces\")\nprint(f\"Rzeczywiste Porażka: {cm[0,0]:3d}     {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Sukces:  {cm[1,0]:3d}     {cm[1,1]:3d}\")\n\n\n\n\n\n\n\nPokaż wyniki Random Forest\n\n\n\n\n\n\n\n\nDokładność Random Forest: 99.8%\n\nConfusion Matrix:\nPrzewidywane:     Porażka  Sukces\nRzeczywiste Porażka:   0       1\nRzeczywiste Sukces:    0     399"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#analiza-ważności-cech-feature-importance",
    "href": "cheatsheets/04-random-forest.html#analiza-ważności-cech-feature-importance",
    "title": "Random Forest — las drzew decyzyjnych",
    "section": "📊 Analiza ważności cech (Feature Importance)",
    "text": "📊 Analiza ważności cech (Feature Importance)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Przygotuj dane (powtarzamy dla kompletności)\nnp.random.seed(42)\nn_products = 2000\n\ndata = pd.DataFrame({\n    'cena': np.random.lognormal(6, 1, n_products),\n    'jakosc': np.random.randint(1, 11, n_products),\n    'marketing_budget': np.random.exponential(50000, n_products),\n    'konkurencja': np.random.randint(1, 21, n_products),\n    'sezon': np.random.choice(['wiosna', 'lato', 'jesien', 'zima'], n_products),\n    'kategoria': np.random.choice(['elektronika', 'moda', 'dom', 'sport', 'ksiazki'], n_products),\n    'ocena_testowa': np.random.normal(7, 2, n_products),\n    'innowacyjnosc': np.random.randint(1, 11, n_products),\n    'marka_znana': np.random.choice([0, 1], n_products, p=[0.3, 0.7])\n})\n\ndef czy_sukces(row):\n    score = 0\n    if 50 &lt;= row['cena'] &lt;= 500:\n        score += 2\n    elif row['cena'] &gt; 1000:\n        score -= 2\n    score += row['jakosc'] * 0.8\n    if row['marketing_budget'] &gt; 100000:\n        score += 3\n    elif row['marketing_budget'] &gt; 50000:\n        score += 1\n    score -= row['konkurencja'] * 0.2\n    if row['sezon'] in ['lato', 'zima']:\n        score += 1\n    if row['kategoria'] == 'elektronika':\n        score += 1\n    elif row['kategoria'] == 'ksiazki':\n        score -= 1\n    score += (row['ocena_testowa'] - 5) * 0.5\n    score += row['innowacyjnosc'] * 0.3\n    if row['marka_znana']:\n        score += 2\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 1)))\n    return probability &gt; 0.5\n\ndata['sukces'] = data.apply(czy_sukces, axis=1)\n\n# Encoding i model\ndata_encoded = data.copy()\nle_sezon = LabelEncoder()\nle_kategoria = LabelEncoder()\ndata_encoded['sezon_num'] = le_sezon.fit_transform(data['sezon'])\ndata_encoded['kategoria_num'] = le_kategoria.fit_transform(data['kategoria'])\n\nfeature_columns = ['cena', 'jakosc', 'marketing_budget', 'konkurencja', \n                  'sezon_num', 'kategoria_num', 'ocena_testowa', \n                  'innowacyjnosc', 'marka_znana']\nX = data_encoded[feature_columns]\ny = data_encoded['sukces']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenuj model\nrf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\nrf_model.fit(X_train, y_train)\n\n# Ewaluacja\ny_pred = rf_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\n# Ważność cech\nfeature_names = ['cena', 'jakość', 'marketing_budget', 'konkurencja', \n                'sezon', 'kategoria', 'ocena_testowa', 'innowacyjność', 'marka_znana']\nimportance = rf_model.feature_importances_\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importance\n}).sort_values('importance', ascending=False)\n\n# Wykres ważności cech\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance['feature'], feature_importance['importance'])\nplt.xlabel('Ważność cechy')\nplt.title('Ważność cech w przewidywaniu sukcesu produktu')\nplt.gca().invert_yaxis()\nplt.close()\n\nprint(f\"Wyniki Random Forest:\")\nprint(f\"Dokładność modelu: {accuracy:.1%}\")\nprint(f\"Liczba drzew: {rf_model.n_estimators}\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:     Porażka  Sukces\")\nprint(f\"Rzeczywiste Porażka: {cm[0,0]:3d}     {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Sukces:  {cm[1,0]:3d}     {cm[1,1]:3d}\")\n\nprint(\"\\nWażność cech:\")\nfor _, row in feature_importance.iterrows():\n    print(f\"{row['feature']}: {row['importance']:.3f}\")\n\n# Odtwórz wykres\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance['feature'], feature_importance['importance'])\nplt.xlabel('Ważność cechy')\nplt.title('Ważność cech w przewidywaniu sukcesu produktu')\nplt.gca().invert_yaxis()\nplt.show()\n\n\n\n\n\n\n\nPokaż wyniki i wykres ważności cech\n\n\n\n\n\n\n\nWyniki Random Forest:\nDokładność modelu: 99.8%\nLiczba drzew: 100\n\nConfusion Matrix:\nPrzewidywane:     Porażka  Sukces\nRzeczywiste Porażka:   0       1\nRzeczywiste Sukces:    0     399\n\nWażność cech:\ncena: 0.294\nmarketing_budget: 0.168\nocena_testowa: 0.143\njakość: 0.117\nkonkurencja: 0.105\ninnowacyjność: 0.069\nsezon: 0.042\nkategoria: 0.033\nmarka_znana: 0.028\n\n\n\n\n\nWażność cech w Random Forest"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#random-forest-vs-decision-tree---porównanie",
    "href": "cheatsheets/04-random-forest.html#random-forest-vs-decision-tree---porównanie",
    "title": "Random Forest — las drzew decyzyjnych",
    "section": "🔍 Random Forest vs Decision Tree - porównanie",
    "text": "🔍 Random Forest vs Decision Tree - porównanie\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Porównanie z pojedynczym Decision Tree\ndt_model = DecisionTreeClassifier(max_depth=10, random_state=42)\ndt_model.fit(X_train, y_train)\n\ndt_pred = dt_model.predict(X_test)\ndt_accuracy = accuracy_score(y_test, dt_pred)\n\nprint(\"PORÓWNANIE MODELI:\")\nprint(f\"Decision Tree:    {dt_accuracy:.1%}\")\nprint(f\"Random Forest:    {accuracy:.1%}\")\nprint(f\"Poprawa:          +{(accuracy - dt_accuracy)*100:.1f} punktów procentowych\")\n\n# Test stabilności - różne random_state\ndt_results = []\nrf_results = []\n\nfor rs in range(10):\n    # Decision Tree\n    dt_temp = DecisionTreeClassifier(max_depth=10, random_state=rs)\n    dt_temp.fit(X_train, y_train)\n    dt_results.append(accuracy_score(y_test, dt_temp.predict(X_test)))\n    \n    # Random Forest  \n    rf_temp = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=rs)\n    rf_temp.fit(X_train, y_train)\n    rf_results.append(accuracy_score(y_test, rf_temp.predict(X_test)))\n\nprint(f\"\\nSTABILNOŚĆ (10 różnych random_state):\")\nprint(f\"Decision Tree rozrzut: {max(dt_results)-min(dt_results):.1%}\")\nprint(f\"Random Forest rozrzut: {max(rf_results)-min(rf_results):.1%}\")\nprint(\"Random Forest jest znacznie bardziej stabilny!\")\n\n\n\n\n\n\n\nPokaż porównanie modeli\n\n\n\n\n\n\n\nPORÓWNANIE MODELI:\nDecision Tree:    98.5%\nRandom Forest:    99.8%\nPoprawa:          +1.3 punktów procentowych\n\nSTABILNOŚĆ (10 różnych random_state):\nDecision Tree rozrzut: 1.0%\nRandom Forest rozrzut: 0.0%\nRandom Forest jest znacznie bardziej stabilny!"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#zastosowania-random-forest",
    "href": "cheatsheets/04-random-forest.html#zastosowania-random-forest",
    "title": "Random Forest — las drzew decyzyjnych",
    "section": "🎯 Zastosowania Random Forest",
    "text": "🎯 Zastosowania Random Forest\n\n1) E-commerce - rekomendacje produktów\n\n# Przewidywanie czy użytkownik kupi produkt\necommerce_features = ['historia_zakupów', 'czas_na_stronie', 'kategoria_preferowana', \n                     'cena_produktu', 'oceny_produktu', 'sezon']\n# Target: 'kupi_produkt' (tak/nie)\n\necommerce_rf = RandomForestClassifier(n_estimators=200)\n# Wynik: system rekomendacji uwzględniający kompleksowe zachowania użytkowników\n\n\n\n2) Finanse - wykrywanie fraudów\n\n# Identyfikacja podejrzanych transakcji\nfraud_features = ['kwota', 'godzina', 'lokalizacja', 'typ_karty', \n                 'historia_klienta', 'częstotliwość_transakcji']\n# Target: 'fraud' (tak/nie)\n\nfraud_rf = RandomForestClassifier(n_estimators=500, class_weight='balanced')\n# Wynik: system antyfaudowy z wysoką dokładnością\n\n\n\n3) HR - przewidywanie odejść pracowników\n\n# Employee churn prediction\nhr_features = ['satisfaction', 'last_evaluation', 'projects', 'salary', \n              'time_company', 'work_accident', 'promotion']\n# Target: 'left_company' (tak/nie)\n\nhr_rf = RandomForestClassifier(n_estimators=300)\n# Wynik: wczesne ostrzeżenia przed odejściami kluczowych pracowników"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#tuning-parametrów-random-forest",
    "href": "cheatsheets/04-random-forest.html#tuning-parametrów-random-forest",
    "title": "Random Forest — las drzew decyzyjnych",
    "section": "⚙️ Tuning parametrów Random Forest",
    "text": "⚙️ Tuning parametrów Random Forest\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Najważniejsze parametry do tuningu\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [5, 10, 15, None],\n    'min_samples_split': [10, 20, 50],\n    'min_samples_leaf': [5, 10, 20],\n    'max_features': ['sqrt', 'log2', None]\n}\n\n# Grid search z cross-validation (uwaga: może potrwać!)\nprint(\"Rozpoczynam Grid Search - może potrwać kilka minut...\")\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_grid,\n    cv=3,  # redukcja CV dla szybkości\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(\"Najlepsze parametry:\")\nprint(grid_search.best_params_)\nprint(f\"Najlepsza dokładność CV: {grid_search.best_score_:.1%}\")\n\n# Model z najlepszymi parametrami\nbest_rf = grid_search.best_estimator_\nbest_accuracy = accuracy_score(y_test, best_rf.predict(X_test))\nprint(f\"Dokładność na test set: {best_accuracy:.1%}\")\n\n\n\n\n\n\n\nPokaż najlepsze parametry\n\n\n\n\n\n\n\nNajlepsze parametry:\n{'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 50}\nNajlepsza dokładność CV: 99.0%\nDokładność na test set: 99.8%"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#pułapki-i-rozwiązania",
    "href": "cheatsheets/04-random-forest.html#pułapki-i-rozwiązania",
    "title": "Random Forest — las drzew decyzyjnych",
    "section": "⚠️ Pułapki i rozwiązania",
    "text": "⚠️ Pułapki i rozwiązania\n\n1) Overfitting mimo ensemble\n\n# Problem: za głębokie drzewa mogą nadal prowadzić do overfittingu\noverfitted_rf = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=None,    # bez limitu głębokości!\n    min_samples_leaf=1  # pozwól na liście z 1 próbką!\n)\noverfitted_rf.fit(X_train, y_train)\n\ntrain_acc_over = accuracy_score(y_train, overfitted_rf.predict(X_train))\ntest_acc_over = accuracy_score(y_test, overfitted_rf.predict(X_test))\n\nprint(f\"Overfitted Random Forest:\")\nprint(f\"Train accuracy: {train_acc_over:.1%}\")\nprint(f\"Test accuracy: {test_acc_over:.1%}\")\nprint(f\"Różnica: {train_acc_over - test_acc_over:.1%}\")\n\n# Rozwiązanie: odpowiednie parametry\nbalanced_rf = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,      # ogranicz głębokość\n    min_samples_leaf=10  # wymuś więcej próbek w liściach\n)\nbalanced_rf.fit(X_train, y_train)\n\ntrain_acc_bal = accuracy_score(y_train, balanced_rf.predict(X_train))\ntest_acc_bal = accuracy_score(y_test, balanced_rf.predict(X_test))\n\nprint(f\"\\nBalanced Random Forest:\")\nprint(f\"Train accuracy: {train_acc_bal:.1%}\")\nprint(f\"Test accuracy: {test_acc_bal:.1%}\")\nprint(f\"Różnica: {train_acc_bal - test_acc_bal:.1%} - znacznie lepiej!\")\n\n\n\n\n\n\n\nPokaż wyniki overfittingu\n\n\n\n\n\n\n\nOverfitted Random Forest:\nTrain accuracy: 100.0%\nTest accuracy: 99.8%\nRóżnica: 0.2%\n\nBalanced Random Forest:\nTrain accuracy: 99.0%\nTest accuracy: 99.8%\nRóżnica: -0.8% - znacznie lepiej!\n\n\n\n\n\n\n\n2) Niezbalansowane klasy\n\n# Problem: gdy jedna klasa jest rzadka (np. 5% fraudów, 95% normalnych transakcji)\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Sprawdź balans klas w naszych danych\nprint(f\"Rozkład klas:\")\nprint(f\"Porażka: {(~y_train).sum()} ({(~y_train).mean():.1%})\")\nprint(f\"Sukces: {y_train.sum()} ({y_train.mean():.1%})\")\n\n# Rozwiązanie 1: class_weight='balanced'\nbalanced_class_rf = RandomForestClassifier(\n    n_estimators=100,\n    class_weight='balanced',  # automatyczne ważenie klas\n    random_state=42\n)\nbalanced_class_rf.fit(X_train, y_train)\n\n# Rozwiązanie 2: bootstrap sampling\nbalanced_rf = RandomForestClassifier(\n    n_estimators=100,\n    max_samples=0.8,  # użyj tylko 80% próbek do każdego drzewa\n    random_state=42\n)\nbalanced_rf.fit(X_train, y_train)\n\nprint(\"Rozwiązania zostały zaimplementowane!\")\n\n\n\n\n\n\n\nPokaż rozkład klas\n\n\n\n\n\n\n\nRozkład klas:\nPorażka: 16 (1.0%)\nSukces: 1584 (99.0%)\nRozwiązania zostały zaimplementowane!"
  },
  {
    "objectID": "cheatsheets/04-random-forest.html#real-world-przypadki-użycia",
    "href": "cheatsheets/04-random-forest.html#real-world-przypadki-użycia",
    "title": "Random Forest — las drzew decyzyjnych",
    "section": "🌍 Real-world przypadki użycia",
    "text": "🌍 Real-world przypadki użycia\n\nFinanse: Credit scoring, wykrywanie prania pieniędzy, trading algorytmiczny\nE-commerce: Systemy rekomendacji, dynamic pricing, churn prediction\nHealthcare: Diagnoza medyczna, drug discovery, analiza obrazów medycznych\nMarketing: Customer segmentation, campaign optimization, A/B testing\nCybersecurity: Intrusion detection, malware classification, anomaly detection\n\n\n\n\n\n\n\n💡 Kiedy używać Random Forest?\n\n\n\n✅ UŻYJ GDY:\n\nPotrzebujesz wysokiej dokładności z interpretowalnym modelem\nMasz mixed features (liczbowe + kategoryczne)\nDane zawierają missing values (RF radzi sobie z nimi dobrze)\nChcesz feature importance bez complex feature engineering\nPotrzebujesz stabilnego modelu (mała wariancja)\n\n❌ NIE UŻYWAJ GDY:\n\nMasz bardzo duże datasety (użyj XGBoost/LightGBM)\nPotrzebujesz bardzo prostego modelu (użyj Decision Tree)\nTarget jest continuous i potrzebujesz liniowej interpretacji (użyj Linear Regression)\nMasz bardzo wysokowymiarowe dane (użyj SVM lub Neural Networks)\n\n\n\nNastępna ściągawka: K-Means Clustering - grupowanie bez etykiet! 🎯"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html",
    "href": "cheatsheets/05-kmeans.html",
    "title": "K-Means — grupowanie klientów bez etykiet",
    "section": "",
    "text": "K-Means to algorytm uczenia nienadzorowanego, który automatycznie grupuje podobne dane w klastry. Nie potrzebujesz etykiet - algorytm sam znajduje wzorce i dzieli dane na K grup.\n\n\n\n\n\n\n💡 Intuicja\n\n\n\nWyobraź sobie imprezę, gdzie ludzie sami grupują się w kręgi na podstawie wspólnych zainteresowań. K-Means działa podobnie - znajduje “centra grup” i przydziela każdy punkt do najbliższego centrum."
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#czym-jest-k-means",
    "href": "cheatsheets/05-kmeans.html#czym-jest-k-means",
    "title": "K-Means — grupowanie klientów bez etykiet",
    "section": "",
    "text": "K-Means to algorytm uczenia nienadzorowanego, który automatycznie grupuje podobne dane w klastry. Nie potrzebujesz etykiet - algorytm sam znajduje wzorce i dzieli dane na K grup.\n\n\n\n\n\n\n💡 Intuicja\n\n\n\nWyobraź sobie imprezę, gdzie ludzie sami grupują się w kręgi na podstawie wspólnych zainteresowań. K-Means działa podobnie - znajduje “centra grup” i przydziela każdy punkt do najbliższego centrum."
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#praktyczny-przykład-segmentacja-klientów-sklepu",
    "href": "cheatsheets/05-kmeans.html#praktyczny-przykład-segmentacja-klientów-sklepu",
    "title": "K-Means — grupowanie klientów bez etykiet",
    "section": "🛍️ Praktyczny przykład: segmentacja klientów sklepu",
    "text": "🛍️ Praktyczny przykład: segmentacja klientów sklepu\nJak pogrupować klientów do targetowanych kampanii marketingowych?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Tworzenie realistycznych danych klientów e-commerce\nnp.random.seed(42)\nn_customers = 1000\n\n# Generujemy różne typy klientów z wyraźnymi wzorcami\ncustomer_types = np.random.choice(['premium', 'average', 'bargain', 'sporadic'], n_customers, \n                                 p=[0.15, 0.35, 0.35, 0.15])\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_customers),\n    'roczny_dochod': np.random.normal(50000, 20000, n_customers),\n    'wydatki_roczne': np.random.normal(25000, 15000, n_customers),\n    'liczba_zakupow': np.random.randint(1, 50, n_customers),\n    'sredni_koszt_zakupu': np.random.normal(200, 100, n_customers),\n    'lata_jako_klient': np.random.randint(0, 10, n_customers),\n    'typ_klienta': customer_types\n})\n\n# Realistyczne korelacje między zmiennymi\nfor i, typ in enumerate(data['typ_klienta']):\n    if typ == 'premium':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(80000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(45000, 10000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(20, 50)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(400, 100)\n    elif typ == 'bargain':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(30000, 10000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(8000, 3000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(15, 40)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(80, 30)\n    elif typ == 'sporadic':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(45000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(5000, 2000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(1, 8)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(300, 150)\n\n# Czyść dane - usuń wartości ujemne\ndata['roczny_dochod'] = np.maximum(data['roczny_dochod'], 15000)\ndata['wydatki_roczne'] = np.maximum(data['wydatki_roczne'], 1000)\ndata['sredni_koszt_zakupu'] = np.maximum(data['sredni_koszt_zakupu'], 20)\n\nprint(\"Statystyki klientów:\")\nprint(data[['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', 'sredni_koszt_zakupu']].describe())\nprint(f\"\\nRozkład typów klientów:\")\nprint(data['typ_klienta'].value_counts())\n\n\n\n\n\n\n\nPokaż statystyki klientów\n\n\n\n\n\n\n\nStatystyki klientów:\n              wiek  roczny_dochod  wydatki_roczne  liczba_zakupow  \\\ncount  1000.000000    1000.000000     1000.000000      1000.00000   \nmean     43.359000   46988.933946    19816.768355        24.00700   \nstd      14.817276   22175.621844    17169.303009        13.41898   \nmin      18.000000   15000.000000     1000.000000         1.00000   \n25%      30.000000   29854.699281     6462.783773        15.00000   \n50%      43.000000   42533.564242    11041.836565        25.00000   \n75%      56.000000   62388.416902    33188.043332        35.00000   \nmax      69.000000  122502.434178    74817.934938        49.00000   \n\n       sredni_koszt_zakupu  \ncount          1000.000000  \nmean            203.883750  \nstd             143.706490  \nmin              20.000000  \n25%              83.839117  \n50%             162.389656  \n75%             303.607754  \nmax             763.483441  \n\nRozkład typów klientów:\ntyp_klienta\nbargain     344\naverage     337\npremium     166\nsporadic    153\nName: count, dtype: int64"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#budowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/05-kmeans.html#budowanie-modelu-krok-po-kroku",
    "title": "K-Means — grupowanie klientów bez etykiet",
    "section": "🔧 Budowanie modelu krok po kroku",
    "text": "🔧 Budowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Wybierz features do clusteringu (bez typ_klienta - to chcemy odkryć!)\nfeatures = ['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', \n           'sredni_koszt_zakupu', 'lata_jako_klient']\nX = data[features]\n\n# Standaryzacja - BARDZO WAŻNE w K-Means!\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(\"Przed standaryzacją:\")\nprint(X.describe())\n\nprint(\"\\nPo standaryzacji (pierwsze 5 wierszy):\")\nX_scaled_df = pd.DataFrame(X_scaled, columns=features)\nprint(X_scaled_df.head())\nprint(\"\\nŚrednie po standaryzacji:\", X_scaled_df.mean().round(3))\nprint(\"Odchylenia po standaryzacji:\", X_scaled_df.std().round(3))\n\n\n\n\n\n\n\nPokaż wyniki standaryzacji\n\n\n\n\n\n\n\nPrzed standaryzacją:\n              wiek  roczny_dochod  wydatki_roczne  liczba_zakupow  \\\ncount  1000.000000    1000.000000     1000.000000      1000.00000   \nmean     43.359000   46988.933946    19816.768355        24.00700   \nstd      14.817276   22175.621844    17169.303009        13.41898   \nmin      18.000000   15000.000000     1000.000000         1.00000   \n25%      30.000000   29854.699281     6462.783773        15.00000   \n50%      43.000000   42533.564242    11041.836565        25.00000   \n75%      56.000000   62388.416902    33188.043332        35.00000   \nmax      69.000000  122502.434178    74817.934938        49.00000   \n\n       sredni_koszt_zakupu  lata_jako_klient  \ncount          1000.000000       1000.000000  \nmean            203.883750          4.550000  \nstd             143.706490          2.914789  \nmin              20.000000          0.000000  \n25%              83.839117          2.000000  \n50%             162.389656          5.000000  \n75%             303.607754          7.000000  \nmax             763.483441          9.000000  \n\nPo standaryzacji (pierwsze 5 wierszy):\n       wiek  roczny_dochod  wydatki_roczne  liczba_zakupow  \\\n0  1.393733       0.346806        0.136732        0.894181   \n1 -0.969556       0.499661       -0.675331       -1.342577   \n2 -0.699466      -1.432630       -0.608789       -0.224198   \n3 -0.159286      -1.443249       -0.634140        0.223154   \n4 -0.496898      -1.443249       -0.966417        0.894181   \n\n   sredni_koszt_zakupu  lata_jako_klient  \n0             0.053442          1.184211  \n1             1.809673         -1.561786  \n2            -0.914825          0.154462  \n3            -1.101092         -1.561786  \n4            -0.652771         -0.875287  \n\nŚrednie po standaryzacji: wiek                  -0.0\nroczny_dochod         -0.0\nwydatki_roczne         0.0\nliczba_zakupow        -0.0\nsredni_koszt_zakupu    0.0\nlata_jako_klient       0.0\ndtype: float64\nOdchylenia po standaryzacji: wiek                   1.001\nroczny_dochod          1.001\nwydatki_roczne         1.001\nliczba_zakupow         1.001\nsredni_koszt_zakupu    1.001\nlata_jako_klient       1.001\ndtype: float64\n\n\n\n\n\n\n\n2) Znajdowanie optymalnej liczby klastrów\n\n# Metoda łokcia (Elbow Method)\ninertias = []\nsilhouette_scores = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n\n# Znajdź optymalne K\nbest_k = K_range[np.argmax(silhouette_scores)]\nprint(f\"Optymalna liczba klastrów (Silhouette): {best_k}\")\n\nprint(\"\\nWyniki dla różnych K:\")\nfor k, inertia, sil_score in zip(K_range, inertias, silhouette_scores):\n    print(f\"K={k}: Inertia={inertia:.0f}, Silhouette={sil_score:.3f}\")\n\n\n\n\n\n\n\nPokaż optymalne K\n\n\n\n\n\n\n\nOptymalna liczba klastrów (Silhouette): 2\n\nWyniki dla różnych K:\nK=2: Inertia=4416, Silhouette=0.281\nK=3: Inertia=3691, Silhouette=0.232\nK=4: Inertia=3296, Silhouette=0.200\nK=5: Inertia=3018, Silhouette=0.196\nK=6: Inertia=2828, Silhouette=0.195\nK=7: Inertia=2665, Silhouette=0.176\nK=8: Inertia=2526, Silhouette=0.177\nK=9: Inertia=2403, Silhouette=0.181\nK=10: Inertia=2286, Silhouette=0.176\n\n\n\n\n\n\n\n3) Trenowanie finalnego modelu\n\n# Trenuj model z optymalnym K\nfinal_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\ncluster_labels = final_kmeans.fit_predict(X_scaled)\n\n# Dodaj etykiety klastrów do danych\ndata['klaster'] = cluster_labels\n\nprint(f\"Model K-Means wytrenowany z K={best_k}\")\nprint(f\"Silhouette Score: {silhouette_score(X_scaled, cluster_labels):.3f}\")\n\n# Analiza klastrów\nprint(\"\\nRozmiary klastrów:\")\ncluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\nfor i, count in enumerate(cluster_counts):\n    print(f\"Klaster {i}: {count} klientów ({count/len(data)*100:.1f}%)\")\n\n\n\n\n\n\n\nPokaż wyniki finalnego modelu\n\n\n\n\n\n\n\nModel K-Means wytrenowany z K=2\nSilhouette Score: 0.281\n\nRozmiary klastrów:\nKlaster 0: 275 klientów (27.5%)\nKlaster 1: 725 klientów (72.5%)"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#analiza-i-interpretacja-klastrów",
    "href": "cheatsheets/05-kmeans.html#analiza-i-interpretacja-klastrów",
    "title": "K-Means — grupowanie klientów bez etykiet",
    "section": "📊 Analiza i interpretacja klastrów",
    "text": "📊 Analiza i interpretacja klastrów\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Przygotuj dane (powtarzamy dla kompletności)\nnp.random.seed(42)\nn_customers = 1000\n\ncustomer_types = np.random.choice(['premium', 'average', 'bargain', 'sporadic'], n_customers, \n                                 p=[0.15, 0.35, 0.35, 0.15])\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_customers),\n    'roczny_dochod': np.random.normal(50000, 20000, n_customers),\n    'wydatki_roczne': np.random.normal(25000, 15000, n_customers),\n    'liczba_zakupow': np.random.randint(1, 50, n_customers),\n    'sredni_koszt_zakupu': np.random.normal(200, 100, n_customers),\n    'lata_jako_klient': np.random.randint(0, 10, n_customers),\n    'typ_klienta': customer_types\n})\n\n# Popraw dane według typów\nfor i, typ in enumerate(data['typ_klienta']):\n    if typ == 'premium':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(80000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(45000, 10000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(20, 50)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(400, 100)\n    elif typ == 'bargain':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(30000, 10000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(8000, 3000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(15, 40)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(80, 30)\n    elif typ == 'sporadic':\n        data.loc[i, 'roczny_dochod'] = np.random.normal(45000, 15000)\n        data.loc[i, 'wydatki_roczne'] = np.random.normal(5000, 2000)\n        data.loc[i, 'liczba_zakupow'] = np.random.randint(1, 8)\n        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(300, 150)\n\ndata['roczny_dochod'] = np.maximum(data['roczny_dochod'], 15000)\ndata['wydatki_roczne'] = np.maximum(data['wydatki_roczne'], 1000)\ndata['sredni_koszt_zakupu'] = np.maximum(data['sredni_koszt_zakupu'], 20)\n\n# Przygotuj model\nfeatures = ['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', \n           'sredni_koszt_zakupu', 'lata_jako_klient']\nX = data[features]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Znajdź optymalne K\nsilhouette_scores = []\nK_range = range(2, 11)\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n\nbest_k = K_range[np.argmax(silhouette_scores)]\n\n# Trenuj finalny model\nfinal_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\ncluster_labels = final_kmeans.fit_predict(X_scaled)\ndata['klaster'] = cluster_labels\n\n# Analiza charakterystyk klastrów\ncluster_analysis = data.groupby('klaster')[features].mean()\n\nprint(\"CHARAKTERYSTYKI KLASTRÓW:\")\nprint(\"=\" * 50)\n\nfor cluster_id in range(best_k):\n    print(f\"\\nKLASTER {cluster_id}:\")\n    cluster_data = data[data['klaster'] == cluster_id]\n    \n    print(f\"Liczba klientów: {len(cluster_data)} ({len(cluster_data)/len(data)*100:.1f}%)\")\n    print(f\"Średni wiek: {cluster_data['wiek'].mean():.0f} lat\")\n    print(f\"Średni dochód: {cluster_data['roczny_dochod'].mean():.0f} zł\")\n    print(f\"Średnie wydatki: {cluster_data['wydatki_roczne'].mean():.0f} zł\")\n    print(f\"Średnia liczba zakupów: {cluster_data['liczba_zakupow'].mean():.0f}\")\n    print(f\"Średni koszt zakupu: {cluster_data['sredni_koszt_zakupu'].mean():.0f} zł\")\n    \n    # Nadaj nazwę klastrowi na podstawie charakterystyk\n    avg_income = cluster_data['roczny_dochod'].mean()\n    avg_spending = cluster_data['wydatki_roczne'].mean()\n    avg_frequency = cluster_data['liczba_zakupow'].mean()\n    \n    if avg_income &gt; 60000 and avg_spending &gt; 30000:\n        cluster_name = \"🌟 PREMIUM CUSTOMERS\"\n    elif avg_frequency &lt; 10 and avg_spending &lt; 10000:\n        cluster_name = \"😴 SPORADYCZNI KLIENCI\"\n    elif avg_spending / avg_income &lt; 0.3 and avg_frequency &gt; 20:\n        cluster_name = \"💰 BARGAIN HUNTERS\"\n    else:\n        cluster_name = \"🔄 ŚREDNI KLIENCI\"\n    \n    print(f\"Typ: {cluster_name}\")\n\n# Przygotuj wizualizację\nplt.figure(figsize=(12, 8))\n\n# Wykres 1: Dochód vs Wydatki\nplt.subplot(2, 2, 1)\nscatter = plt.scatter(data['roczny_dochod'], data['wydatki_roczne'], \n                     c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Roczny dochód')\nplt.ylabel('Wydatki roczne')\nplt.title('Dochód vs Wydatki')\nplt.colorbar(scatter)\n\n# Wykres 2: Wiek vs Liczba zakupów\nplt.subplot(2, 2, 2)\nscatter2 = plt.scatter(data['wiek'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Wiek')\nplt.ylabel('Liczba zakupów')\nplt.title('Wiek vs Liczba zakupów')\nplt.colorbar(scatter2)\n\n# Wykres 3: Średni koszt vs Liczba zakupów\nplt.subplot(2, 2, 3)\nscatter3 = plt.scatter(data['sredni_koszt_zakupu'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Średni koszt zakupu')\nplt.ylabel('Liczba zakupów')\nplt.title('Koszt vs Częstotliwość')\nplt.colorbar(scatter3)\n\n# Wykres 4: Rozkład klastrów\nplt.subplot(2, 2, 4)\ncluster_counts = data['klaster'].value_counts().sort_index()\nplt.bar(range(len(cluster_counts)), cluster_counts.values, \n        color=plt.cm.viridis(np.linspace(0, 1, len(cluster_counts))))\nplt.xlabel('Klaster')\nplt.ylabel('Liczba klientów')\nplt.title('Rozmiary klastrów')\nplt.xticks(range(len(cluster_counts)))\n\nplt.tight_layout()\nplt.close()\n\n# Porównanie z rzeczywistymi typami klientów\nif 'typ_klienta' in data.columns:\n    print(\"\\n\" + \"=\"*50)\n    print(\"PORÓWNANIE Z RZECZYWISTYMI TYPAMI:\")\n    comparison = pd.crosstab(data['klaster'], data['typ_klienta'])\n    print(comparison)\n\n# Odtwórz wykresy\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nscatter = plt.scatter(data['roczny_dochod'], data['wydatki_roczne'], \n                     c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Roczny dochód')\nplt.ylabel('Wydatki roczne')\nplt.title('Dochód vs Wydatki')\nplt.colorbar(scatter)\n\nplt.subplot(2, 2, 2)\nscatter2 = plt.scatter(data['wiek'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Wiek')\nplt.ylabel('Liczba zakupów')\nplt.title('Wiek vs Liczba zakupów')\nplt.colorbar(scatter2)\n\nplt.subplot(2, 2, 3)\nscatter3 = plt.scatter(data['sredni_koszt_zakupu'], data['liczba_zakupow'], \n                      c=data['klaster'], cmap='viridis', alpha=0.6)\nplt.xlabel('Średni koszt zakupu')\nplt.ylabel('Liczba zakupów')\nplt.title('Koszt vs Częstotliwość')\nplt.colorbar(scatter3)\n\nplt.subplot(2, 2, 4)\ncluster_counts = data['klaster'].value_counts().sort_index()\nplt.bar(range(len(cluster_counts)), cluster_counts.values, \n        color=plt.cm.viridis(np.linspace(0, 1, len(cluster_counts))))\nplt.xlabel('Klaster')\nplt.ylabel('Liczba klientów')\nplt.title('Rozmiary klastrów')\nplt.xticks(range(len(cluster_counts)))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nPokaż analizę klastrów i wizualizacje\n\n\n\n\n\n\n\nCHARAKTERYSTYKI KLASTRÓW:\n==================================================\n\nKLASTER 0:\nLiczba klientów: 275 (27.5%)\nŚredni wiek: 43 lat\nŚredni dochód: 71194 zł\nŚrednie wydatki: 41779 zł\nŚrednia liczba zakupów: 34\nŚredni koszt zakupu: 332 zł\nTyp: 🌟 PREMIUM CUSTOMERS\n\nKLASTER 1:\nLiczba klientów: 725 (72.5%)\nŚredni wiek: 44 lat\nŚredni dochód: 37808 zł\nŚrednie wydatki: 11486 zł\nŚrednia liczba zakupów: 20\nŚredni koszt zakupu: 155 zł\nTyp: 🔄 ŚREDNI KLIENCI\n\n==================================================\nPORÓWNANIE Z RZECZYWISTYMI TYPAMI:\ntyp_klienta  average  bargain  premium  sporadic\nklaster                                         \n0                109        0      166         0\n1                228      344        0       153\n\n\n\n\n\nAnaliza klastrów klientów"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#praktyczne-zastosowania-k-means",
    "href": "cheatsheets/05-kmeans.html#praktyczne-zastosowania-k-means",
    "title": "K-Means — grupowanie klientów bez etykiet",
    "section": "🎯 Praktyczne zastosowania K-Means",
    "text": "🎯 Praktyczne zastosowania K-Means\n\n1) Marketing - personalizacja kampanii\n\n# Strategia marketingowa na podstawie klastrów\ndef marketing_strategy(cluster_id, cluster_data):\n    avg_income = cluster_data['roczny_dochod'].mean()\n    avg_spending = cluster_data['wydatki_roczne'].mean()\n    avg_frequency = cluster_data['liczba_zakupow'].mean()\n    \n    if avg_income &gt; 60000 and avg_spending &gt; 30000:\n        return {\n            'segment': 'Premium Customers',\n            'strategy': 'Produkty luksusowe, VIP program, personal shopping',\n            'budget_allocation': '40%',\n            'channels': 'Email premium, personal calls, exclusive events'\n        }\n    elif avg_frequency &lt; 10 and avg_spending &lt; 10000:\n        return {\n            'segment': 'Sporadic Customers', \n            'strategy': 'Reaktywacja, oferty specjalne, przypomnienia',\n            'budget_allocation': '15%',\n            'channels': 'SMS, push notifications, retargeting ads'\n        }\n    elif avg_spending / avg_income &lt; 0.3 and avg_frequency &gt; 20:\n        return {\n            'segment': 'Bargain Hunters',\n            'strategy': 'Promocje, wyprzedaże, programy lojalnościowe',\n            'budget_allocation': '25%', \n            'channels': 'Newsletter z promocjami, social media deals'\n        }\n    else:\n        return {\n            'segment': 'Average Customers',\n            'strategy': 'Standardowe produkty, cross-selling, up-selling',\n            'budget_allocation': '20%',\n            'channels': 'Email marketing, social media, display ads'\n        }\n\nprint(\"STRATEGIA MARKETINGOWA DLA KAŻDEGO KLASTRA:\")\nprint(\"=\" * 60)\n\nfor cluster_id in range(best_k):\n    cluster_data = data[data['klaster'] == cluster_id]\n    strategy = marketing_strategy(cluster_id, cluster_data)\n    \n    print(f\"\\nKLASTER {cluster_id}: {strategy['segment']}\")\n    print(f\"Strategia: {strategy['strategy']}\")\n    print(f\"Budżet: {strategy['budget_allocation']}\")\n    print(f\"Kanały: {strategy['channels']}\")\n\n\n\n\n\n\n\nPokaż strategię marketingową\n\n\n\n\n\n\n\nSTRATEGIA MARKETINGOWA DLA KAŻDEGO KLASTRA:\n============================================================\n\nKLASTER 0: Premium Customers\nStrategia: Produkty luksusowe, VIP program, personal shopping\nBudżet: 40%\nKanały: Email premium, personal calls, exclusive events\n\nKLASTER 1: Average Customers\nStrategia: Standardowe produkty, cross-selling, up-selling\nBudżet: 20%\nKanały: Email marketing, social media, display ads\n\n\n\n\n\n\n\n2) Inne branże\n\n# Healthcare - grupowanie pacjentów\nhealthcare_features = ['wiek', 'BMI', 'ciśnienie', 'cholesterol', 'aktywność_fizyczna']\n# Wynik: programy profilaktyczne dostosowane do grup ryzyka\n\n# Finanse - portfolio management  \nfinance_features = ['dochód', 'tolerancja_ryzyka', 'horyzont_inwestycji', 'doświadczenie']\n# Wynik: personalizowane porady inwestycyjne\n\n# Retail - optymalizacja sklepów\nretail_features = ['lokalizacja', 'demografia', 'konkurencja', 'ruch_pieszy']  \n# Wynik: optymalne rozmieszczenie produktów w różnych lokalizacjach"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#tuning-parametrów-k-means",
    "href": "cheatsheets/05-kmeans.html#tuning-parametrów-k-means",
    "title": "K-Means — grupowanie klientów bez etykiet",
    "section": "⚙️ Tuning parametrów K-Means",
    "text": "⚙️ Tuning parametrów K-Means\n\nfrom sklearn.cluster import KMeans\n\n# 1) Różne metody inicjalizacji\ninit_methods = ['k-means++', 'random']\nresults = {}\n\nfor init_method in init_methods:\n    kmeans = KMeans(n_clusters=best_k, init=init_method, n_init=10, random_state=42)\n    labels = kmeans.fit_predict(X_scaled)\n    sil_score = silhouette_score(X_scaled, labels)\n    results[init_method] = sil_score\n\nprint(\"Porównanie metod inicjalizacji:\")\nfor method, score in results.items():\n    print(f\"{method}: Silhouette = {score:.3f}\")\n\n# 2) Wpływ liczby inicjalizacji\nn_init_values = [1, 5, 10, 20]\nfor n_init in n_init_values:\n    kmeans = KMeans(n_clusters=best_k, n_init=n_init, random_state=42)\n    start_time = pd.Timestamp.now()\n    kmeans.fit(X_scaled)\n    end_time = pd.Timestamp.now()\n    duration = (end_time - start_time).total_seconds()\n    sil_score = silhouette_score(X_scaled, kmeans.labels_)\n    print(f\"n_init={n_init}: Silhouette={sil_score:.3f}, Czas={duration:.2f}s\")\n\n# 3) Różne algorytmy\nalgorithms = ['lloyd', 'elkan']  # 'auto' wybiera automatycznie\nfor algorithm in algorithms:\n    try:\n        kmeans = KMeans(n_clusters=best_k, algorithm=algorithm, random_state=42)\n        start_time = pd.Timestamp.now()\n        kmeans.fit(X_scaled)\n        end_time = pd.Timestamp.now()\n        duration = (end_time - start_time).total_seconds()\n        print(f\"Algorytm {algorithm}: Czas={duration:.2f}s\")\n    except:\n        print(f\"Algorytm {algorithm}: Niedostępny w tej wersji\")\n\n\n\n\n\n\n\nPokaż wyniki tuningu\n\n\n\n\n\n\n\nPorównanie metod inicjalizacji:\nk-means++: Silhouette = 0.281\nrandom: Silhouette = 0.281\nn_init=1: Silhouette=0.280, Czas=0.00s\nn_init=5: Silhouette=0.281, Czas=0.00s\nn_init=10: Silhouette=0.281, Czas=0.01s\nn_init=20: Silhouette=0.281, Czas=0.03s"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#pułapki-i-rozwiązania",
    "href": "cheatsheets/05-kmeans.html#pułapki-i-rozwiązania",
    "title": "K-Means — grupowanie klientów bez etykiet",
    "section": "⚠️ Pułapki i rozwiązania",
    "text": "⚠️ Pułapki i rozwiązania\n\n1) Brak standaryzacji danych\n\n# Problem: różne skale zmiennych\nprint(\"PROBLEM: Bez standaryzacji\")\nprint(\"Dochód (tysiące): 20-80\")  \nprint(\"Wiek (lata): 18-70\")\nprint(\"→ K-Means będzie skupiać się głównie na dochodzie!\")\n\n# Demonstracja\nkmeans_unscaled = KMeans(n_clusters=3, random_state=42)\nlabels_unscaled = kmeans_unscaled.fit_predict(X[['roczny_dochod', 'wiek']])\n\nkmeans_scaled = KMeans(n_clusters=3, random_state=42)  \nX_subset_scaled = scaler.fit_transform(X[['roczny_dochod', 'wiek']])\nlabels_scaled = kmeans_scaled.fit_predict(X_subset_scaled)\n\nsil_unscaled = silhouette_score(X[['roczny_dochod', 'wiek']], labels_unscaled)\nsil_scaled = silhouette_score(X_subset_scaled, labels_scaled)\n\nprint(f\"\\nWyniki:\")\nprint(f\"Bez standaryzacji: Silhouette = {sil_unscaled:.3f}\")\nprint(f\"Ze standaryzacją: Silhouette = {sil_scaled:.3f}\")\nprint(\"✅ Standaryzacja ZAWSZE poprawia wyniki!\")\n\n\n\n\n\n\n\nPokaż problem standaryzacji\n\n\n\n\n\n\n\n\nWyniki:\nBez standaryzacji: Silhouette = 0.562\nZe standaryzacją: Silhouette = 0.406\n✅ Standaryzacja ZAWSZE poprawia wyniki!\n\n\n\n\n\n\n\n2) Outliers - wartości odstające\n\n# Problem: outliers zakłócają centroidy klastrów\nfrom sklearn.preprocessing import RobustScaler\n\n# Dodaj kilka outlierów\ndata_with_outliers = data.copy()\ndata_with_outliers.loc[0, 'roczny_dochod'] = 500000  # milioner!\ndata_with_outliers.loc[1, 'wydatki_roczne'] = 200000  # mega wydatki\n\nX_outliers = data_with_outliers[features]\n\n# Porównaj różne skalery\nscalers = {\n    'StandardScaler': StandardScaler(),\n    'RobustScaler': RobustScaler()  # odporny na outliers\n}\n\nprint(\"Wpływ outliers:\")\nfor scaler_name, scaler_obj in scalers.items():\n    X_scaled_comp = scaler_obj.fit_transform(X_outliers)\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    labels = kmeans.fit_predict(X_scaled_comp)\n    sil_score = silhouette_score(X_scaled_comp, labels)\n    print(f\"{scaler_name}: Silhouette = {sil_score:.3f}\")\n\nprint(\"\\n✅ RobustScaler lepiej radzi sobie z outliers!\")\n\n\n\n\n\n\n\nPokaż wpływ outliers\n\n\n\n\n\n\n\nWpływ outliers:\nStandardScaler: Silhouette = 0.193\nRobustScaler: Silhouette = 0.246\n\n✅ RobustScaler lepiej radzi sobie z outliers!\n\n\n\n\n\n\n\n3) Curse of dimensionality\n\n# Problem: za dużo wymiarów\nfrom sklearn.decomposition import PCA\n\nprint(\"Problem wielu wymiarów:\")\ndimensions = [2, 5, 10, 20, 50]\n\nfor n_dims in dimensions:\n    if n_dims &lt;= X_scaled.shape[1]:\n        X_subset = X_scaled[:, :n_dims]\n    else:\n        # Dodaj sztuczne wymiary\n        extra_dims = np.random.randn(X_scaled.shape[0], n_dims - X_scaled.shape[1])\n        X_subset = np.hstack([X_scaled, extra_dims])\n    \n    kmeans = KMeans(n_clusters=3, random_state=42)\n    labels = kmeans.fit_predict(X_subset)\n    \n    if len(np.unique(labels)) &gt; 1:  # sprawdź czy są różne klastry\n        sil_score = silhouette_score(X_subset, labels)\n        print(f\"{n_dims} wymiarów: Silhouette = {sil_score:.3f}\")\n    else:\n        print(f\"{n_dims} wymiarów: Wszystkie punkty w jednym klastrze!\")\n\nprint(\"\\nRozwiązanie: PCA dimensionality reduction\")\npca = PCA(n_components=0.95)  # zachowaj 95% wariancji\nX_pca = pca.fit_transform(X_scaled)\nprint(f\"Redukcja z {X_scaled.shape[1]} do {X_pca.shape[1]} wymiarów\")\n\nkmeans_pca = KMeans(n_clusters=3, random_state=42)\nlabels_pca = kmeans_pca.fit_predict(X_pca)\nsil_pca = silhouette_score(X_pca, labels_pca)\nprint(f\"Po PCA: Silhouette = {sil_pca:.3f}\")\n\n\n\n\n\n\n\nPokaż problem wymiarowości\n\n\n\n\n\n\n\nProblem wielu wymiarów:\n2 wymiarów: Silhouette = 0.406\n5 wymiarów: Silhouette = 0.285\n10 wymiarów: Silhouette = 0.133\n20 wymiarów: Silhouette = 0.055\n50 wymiarów: Silhouette = 0.024\n\nRozwiązanie: PCA dimensionality reduction\nRedukcja z 6 do 6 wymiarów\nPo PCA: Silhouette = 0.233"
  },
  {
    "objectID": "cheatsheets/05-kmeans.html#real-world-przypadki-użycia",
    "href": "cheatsheets/05-kmeans.html#real-world-przypadki-użycia",
    "title": "K-Means — grupowanie klientów bez etykiet",
    "section": "🌍 Real-world przypadki użycia",
    "text": "🌍 Real-world przypadki użycia\n\nE-commerce: Segmentacja klientów, personalizacja, dynamic pricing\nMarketing: Customer personas, campaign optimization, market research\n\nFinance: Portfolio optimization, risk assessment, fraud detection\nHealthcare: Patient stratification, treatment personalization, drug discovery\nOperations: Supply chain optimization, demand forecasting, quality control\n\n\n\n\n\n\n\n💡 Kiedy używać K-Means?\n\n\n\n✅ UŻYJ GDY:\n\nChcesz odkryć ukryte grupy w danych bez etykiet\nDane mają podobne gęstości i są “okrągłe” (sferyczne klastry)\nPotrzebujesz szybkiego i skalowalnego algorytmu\nWiesz w przybliżeniu ile może być grup (K)\nChcesz segmentować klientów, produkty, rynki\n\n❌ NIE UŻYWAJ GDY:\n\nKlastry mają różne rozmiary lub gęstości (użyj DBSCAN)\nKlastry mają nieregularne kształty (użyj Hierarchical Clustering)\nNie wiesz wcale ile może być grup (użyj DBSCAN/HDBSCAN)\nMasz kategoryczne zmienne (użyj K-Modes)\nDane mają dużo outliers (użyj DBSCAN)\n\n\n\nNastępna ściągawka: Support Vector Machines - znajdowanie optymalnych granic! 🎯"
  }
]