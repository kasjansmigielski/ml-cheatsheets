[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Witaj w świecie Machine Learning!",
    "section": "",
    "text": "Witaj w praktycznej kolekcji ściąg z Machine Learning! Znajdziesz tutaj algorytmy, teorię i real-world przykłady przedstawione w przystępny sposób dla kursantów Data Science. Każda ściągawka to kompaktowa porcja wiedzy gotowa do zastosowania.\n\n\n\nWyjaśnienie algorytmu - jak działa i dlaczego\nPraktyczne przykłady kodu gotowe do skopiowania i uruchomienia\nReal-world zastosowania - kiedy i gdzie użyć\nKluczowe parametry - co można dostroić i jak\nTipsy od praktyków - na co uważać w rzeczywistych projektach\n\n\n\n\nNajlepsza metoda nauki:\n\nPrzeczytaj teorię - zrozum podstawy algorytmu\nPrzeanalizuj przykład - zobacz jak teoria przekłada się na kod\n\nSkopiuj snippety - użyj przyciska 📋 Copy w prawym górnym rogu\nTestuj w swoim środowisku - Jupyter, Colab, VS Code\nEksperymentuj z parametrami - zmień wartości i obserwuj efekty!\n\n\n\n\n\n\n\n💡 Pro tip dla kursantów\n\n\n\nNie tylko kopiuj kod - zrozum co robi każda linia. Spróbuj zmienić dane wejściowe, parametry algorytmu lub dodać własne modyfikacje. To najlepszy sposób na naukę ML!\n\n\n\n\n\n\n\n\nWprowadzenie do ML - Czym jest ML i jakie są główne rodzaje algorytmów\n\n\n\n\n\nLinear Regression - Predykcja wartości liczbowych\n\n\n\n\n\nDecision Trees - Proste drzewa decyzyjne\n\n\n\n\n\n\n\n🔥 Kolejne ściągi w przygotowaniu!\n\n\n\n\nK-Means Clustering (segmentacja klientów)\nRandom Forest (ensembles)\nLogistic Regression (klasyfikacja binarna)\nFeature Engineering (przygotowanie danych)\nModel Evaluation (metryki i walidacja)\n\nŚledź aktualizacje!\n\n\n\n\n\n\n\nZacznij od podstaw - Wprowadzenie do ML\nWybierz algorytm który Cię interesuje z menu ML Cheatsheets\n\nPrzeczytaj teorię - zrozum jak działa algorytm\nSkopiuj kod (ikona 📋 Copy w prawym górnym rogu bloku)\nTestuj w praktyce - uruchom w Jupyter/Colab i eksperymentuj!\n\n\nPowodzenia w nauce Machine Learning! 🚀🤖"
  },
  {
    "objectID": "index.html#co-znajdziesz-w-każdej-ściągawce",
    "href": "index.html#co-znajdziesz-w-każdej-ściągawce",
    "title": "Witaj w świecie Machine Learning!",
    "section": "",
    "text": "Wyjaśnienie algorytmu - jak działa i dlaczego\nPraktyczne przykłady kodu gotowe do skopiowania i uruchomienia\nReal-world zastosowania - kiedy i gdzie użyć\nKluczowe parametry - co można dostroić i jak\nTipsy od praktyków - na co uważać w rzeczywistych projektach"
  },
  {
    "objectID": "index.html#jak-korzystać-z-ml-cheatsheets",
    "href": "index.html#jak-korzystać-z-ml-cheatsheets",
    "title": "Witaj w świecie Machine Learning!",
    "section": "",
    "text": "Najlepsza metoda nauki:\n\nPrzeczytaj teorię - zrozum podstawy algorytmu\nPrzeanalizuj przykład - zobacz jak teoria przekłada się na kod\n\nSkopiuj snippety - użyj przyciska 📋 Copy w prawym górnym rogu\nTestuj w swoim środowisku - Jupyter, Colab, VS Code\nEksperymentuj z parametrami - zmień wartości i obserwuj efekty!\n\n\n\n\n\n\n\n💡 Pro tip dla kursantów\n\n\n\nNie tylko kopiuj kod - zrozum co robi każda linia. Spróbuj zmienić dane wejściowe, parametry algorytmu lub dodać własne modyfikacje. To najlepszy sposób na naukę ML!"
  },
  {
    "objectID": "index.html#dostępne-ściągi-ml",
    "href": "index.html#dostępne-ściągi-ml",
    "title": "Witaj w świecie Machine Learning!",
    "section": "",
    "text": "Wprowadzenie do ML - Czym jest ML i jakie są główne rodzaje algorytmów\n\n\n\n\n\nLinear Regression - Predykcja wartości liczbowych\n\n\n\n\n\nDecision Trees - Proste drzewa decyzyjne\n\n\n\n\n\n\n\n🔥 Kolejne ściągi w przygotowaniu!\n\n\n\n\nK-Means Clustering (segmentacja klientów)\nRandom Forest (ensembles)\nLogistic Regression (klasyfikacja binarna)\nFeature Engineering (przygotowanie danych)\nModel Evaluation (metryki i walidacja)\n\nŚledź aktualizacje!"
  },
  {
    "objectID": "index.html#szybki-start",
    "href": "index.html#szybki-start",
    "title": "Witaj w świecie Machine Learning!",
    "section": "",
    "text": "Zacznij od podstaw - Wprowadzenie do ML\nWybierz algorytm który Cię interesuje z menu ML Cheatsheets\n\nPrzeczytaj teorię - zrozum jak działa algorytm\nSkopiuj kod (ikona 📋 Copy w prawym górnym rogu bloku)\nTestuj w praktyce - uruchom w Jupyter/Colab i eksperymentuj!\n\n\nPowodzenia w nauce Machine Learning! 🚀🤖"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html",
    "href": "cheatsheets/02-linear-regression.html",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "",
    "text": "Linear Regression to jeden z najprostszych i najczęściej używanych algorytmów ML do przewidywania wartości liczbowych (np. ceny, temperatury, sprzedaży). Szuka najlepszej linii prostej, która opisuje zależność między zmiennymi.\n\n\n\n\n\n\n💡 Intuicja matematyczna\n\n\n\nSzukamy takiej linii y = ax + b, która najlepiej “przechodzi” przez nasze punkty danych. Algorytm automatycznie dobiera optymalne wartości a (slope) i b (intercept)."
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#czym-jest-linear-regression",
    "href": "cheatsheets/02-linear-regression.html#czym-jest-linear-regression",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "",
    "text": "Linear Regression to jeden z najprostszych i najczęściej używanych algorytmów ML do przewidywania wartości liczbowych (np. ceny, temperatury, sprzedaży). Szuka najlepszej linii prostej, która opisuje zależność między zmiennymi.\n\n\n\n\n\n\n💡 Intuicja matematyczna\n\n\n\nSzukamy takiej linii y = ax + b, która najlepiej “przechodzi” przez nasze punkty danych. Algorytm automatycznie dobiera optymalne wartości a (slope) i b (intercept)."
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#praktyczny-przykład-predykcja-cen-mieszkań",
    "href": "cheatsheets/02-linear-regression.html#praktyczny-przykład-predykcja-cen-mieszkań",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "🏠 Praktyczny przykład: predykcja cen mieszkań",
    "text": "🏠 Praktyczny przykład: predykcja cen mieszkań\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Tworzenie przykładowych danych mieszkań\nnp.random.seed(42)\nn_mieszkan = 1000\n\ndata = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_mieszkan),\n    'liczba_pokoi': np.random.randint(1, 5, n_mieszkan),\n    'wiek_budynku': np.random.randint(0, 50, n_mieszkan),\n    'odleglosc_centrum': np.random.exponential(5, n_mieszkan)\n})\n\n# Realistyczna formuła ceny (z szumem)\ndata['cena'] = (\n    data['powierzchnia'] * 8000 +           # 8k za m²\n    data['liczba_pokoi'] * 15000 +          # 15k za pokój\n    (50 - data['wiek_budynku']) * 2000 +    # starsze = tańsze\n    data['odleglosc_centrum'] * (-3000) +   # dalej = tańsze\n    np.random.normal(0, 50000, n_mieszkan)  # szum losowy\n)\n\nprint(\"Podstawowe statystyki:\")\nprint(data.describe())\n\n\n\n\n\n\n\nPokaż statystyki\n\n\n\n\n\n\n\nPodstawowe statystyki:\n       powierzchnia  liczba_pokoi  wiek_budynku  odleglosc_centrum  \\\ncount   1000.000000   1000.000000   1000.000000        1000.000000   \nmean      70.483301      2.473000     25.049000           4.971457   \nstd       24.480398      1.128958     14.384001           4.883716   \nmin      -11.031684      1.000000      0.000000           0.000058   \n25%       53.810242      1.000000     13.000000           1.459988   \n50%       70.632515      2.000000     25.000000           3.505541   \n75%       86.198597      4.000000     38.000000           6.954361   \nmax      166.318287      4.000000     49.000000          34.028756   \n\n               cena  \ncount  1.000000e+03  \nmean   6.363187e+05  \nstd    2.004394e+05  \nmin    1.686504e+04  \n25%    5.049745e+05  \n50%    6.356690e+05  \n75%    7.647808e+05  \nmax    1.440259e+06"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#trenowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/02-linear-regression.html#trenowanie-modelu-krok-po-kroku",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "🔧 Trenowanie modelu krok po kroku",
    "text": "🔧 Trenowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Sprawdzenie korelacji - które zmienne są najważniejsze?\ncorrelation = data.corr()['cena'].sort_values(ascending=False)\n\n# Podział na features (X) i target (y)\nX = data[['powierzchnia', 'liczba_pokoi', 'wiek_budynku', 'odleglosc_centrum']]\ny = data['cena']\n\n# Podział train/test (80/20)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"Korelacja z ceną:\")\nprint(correlation)\n\nprint(f\"Dane treningowe: {X_train.shape[0]} mieszkań\")\nprint(f\"Dane testowe: {X_test.shape[0]} mieszkań\")\n\n\n\n\n\n\n\nPokaż korelacje oraz dane treningowe\n\n\n\n\n\n\n\nKorelacja z ceną:\ncena                 1.000000\npowierzchnia         0.949080\nliczba_pokoi         0.025094\nodleglosc_centrum   -0.042040\nwiek_budynku        -0.115566\nName: cena, dtype: float64\nDane treningowe: 800 mieszkań\nDane testowe: 200 mieszkań\n\n\n\n\n\n\n\n2) Trenowanie modelu\n\n# Utworzenie i trenowanie modelu\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Sprawdzenie współczynników\nprint(\"Współczynniki modelu:\")\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"{feature}: {coef:.0f} zł\")\nprint(f\"Intercept (stała): {model.intercept_:.0f} zł\")\n\n\n\n\n\n\n\nPokaż współczynniki\n\n\n\n\n\n\n\nWspółczynniki modelu:\npowierzchnia: 7924 zł\nliczba_pokoi: 15811 zł\nwiek_budynku: -2108 zł\nodleglosc_centrum: -2698 zł\nIntercept (stała): 104042 zł\n\n\n\n\n\n\n\n3) Ewaluacja wyników\n\n# Predykcje na zbiorze testowym\ny_pred = model.predict(X_test)\n\n# Metryki\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"\\nWyniki modelu:\")\nprint(f\"Mean Absolute Error: {mae:.0f} zł\")\nprint(f\"R² Score: {r2:.3f}\")\nprint(f\"Średni błąd to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania\")\n\n# Przykładowe predykcje\nfor i in range(5):\n    rzeczywista = y_test.iloc[i]\n    przewidywana = y_pred[i]\n    błąd = abs(rzeczywista - przewidywana)\n    print(f\"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}zł, \"\n          f\"przewidywana={przewidywana:.0f}zł, błąd={błąd:.0f}zł\")\n\n\n\n\n\n\n\nPokaż współczynniki\n\n\n\n\n\n\n\n\nWyniki modelu:\nMean Absolute Error: 38912 zł\nR² Score: 0.934\nŚredni błąd to 6.1% ceny mieszkania\nMieszkanie 1: rzeczywista=702313zł, przewidywana=771489zł, błąd=69177zł\nMieszkanie 2: rzeczywista=791174zł, przewidywana=816756zł, błąd=25582zł\nMieszkanie 3: rzeczywista=326702zł, przewidywana=274297zł, błąd=52405zł\nMieszkanie 4: rzeczywista=441380zł, przewidywana=456054zł, błąd=14674zł\nMieszkanie 5: rzeczywista=445427zł, przewidywana=383199zł, błąd=62228zł"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#wizualizacja-wyników",
    "href": "cheatsheets/02-linear-regression.html#wizualizacja-wyników",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "📊 Wizualizacja wyników",
    "text": "📊 Wizualizacja wyników\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Przykładowe dane mieszkań (powtarzamy dla demonstracji)\nnp.random.seed(42)\nn_mieszkan = 1000\n\ndata = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_mieszkan),\n    'liczba_pokoi': np.random.randint(1, 5, n_mieszkan),\n    'wiek_budynku': np.random.randint(0, 50, n_mieszkan),\n    'odleglosc_centrum': np.random.exponential(5, n_mieszkan)\n})\n\ndata['cena'] = (\n    data['powierzchnia'] * 8000 +\n    data['liczba_pokoi'] * 15000 +\n    (50 - data['wiek_budynku']) * 2000 +\n    data['odleglosc_centrum'] * (-3000) +\n    np.random.normal(0, 50000, n_mieszkan)\n)\n\n# Przygotowanie i trenowanie modelu\nX = data[['powierzchnia', 'liczba_pokoi', 'wiek_budynku', 'odleglosc_centrum']]\ny = data['cena']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Oblicz metryki\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Przygotuj wykresy\nplt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Rzeczywiste ceny')\nplt.ylabel('Przewidywane ceny')\nplt.title('Rzeczywiste vs Przewidywane')\n\nplt.subplot(1, 2, 2)\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Przewidywane ceny')\nplt.ylabel('Residuals (błędy)')\nplt.title('Analiza residuals')\n\nplt.tight_layout()\n\n# Zapisz wykres do zmiennej\nimport io\nimport base64\nbuf = io.BytesIO()\nplt.savefig(buf, format='png', dpi=100, bbox_inches='tight')\nbuf.seek(0)\nplt.close()\n\n# Przygotuj przykładowe predykcje\nprzykladowe_predykcje = []\nfor i in range(3):\n    rzeczywista = y_test.iloc[i]\n    przewidywana = y_pred[i]\n    blad = abs(rzeczywista - przewidywana)\n    przykladowe_predykcje.append(f\"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}zł, przewidywana={przewidywana:.0f}zł, błąd={blad:.0f}zł\")\n\nprint(f\"Wyniki modelu Linear Regression:\")\nprint(f\"Mean Absolute Error: {mae:.0f} zł\")\nprint(f\"R² Score: {r2:.3f}\")\nprint(f\"Średni błąd to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania\")\nprint(f\"\\nPrzykładowe predykcje:\")\nfor pred in przykladowe_predykcje:\n    print(pred)\n\n# Odtwórz wykres\nplt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Rzeczywiste ceny')\nplt.ylabel('Przewidywane ceny')\nplt.title('Rzeczywiste vs Przewidywane')\n\nplt.subplot(1, 2, 2)\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Przewidywane ceny')\nplt.ylabel('Residuals (błędy)')\nplt.title('Analiza residuals')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nPokaż wyniki i wykresy analizy\n\n\n\n\n\n\n\nWyniki modelu Linear Regression:\nMean Absolute Error: 38912 zł\nR² Score: 0.934\nŚredni błąd to 6.1% ceny mieszkania\n\nPrzykładowe predykcje:\nMieszkanie 1: rzeczywista=702313zł, przewidywana=771489zł, błąd=69177zł\nMieszkanie 2: rzeczywista=791174zł, przewidywana=816756zł, błąd=25582zł\nMieszkanie 3: rzeczywista=326702zł, przewidywana=274297zł, błąd=52405zł\n\n\n\n\n\nAnaliza wyników modelu Linear Regression"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#praktyczne-zastosowania-linear-regression",
    "href": "cheatsheets/02-linear-regression.html#praktyczne-zastosowania-linear-regression",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "🎯 Praktyczne zastosowania Linear Regression",
    "text": "🎯 Praktyczne zastosowania Linear Regression\n\n1) Biznesowe\n\n# Przewidywanie sprzedaży na podstawie budżetu marketingowego\nsales_data = pd.DataFrame({\n    'marketing_budget': [10000, 15000, 20000, 25000, 30000],\n    'sales': [100000, 140000, 180000, 220000, 260000]\n})\n\nmodel_sales = LinearRegression()\nmodel_sales.fit(sales_data[['marketing_budget']], sales_data['sales'])\n\n# \"Jeśli zwiększę budżet do 35k, sprzedaż wzrośnie do:\"\nnew_sales = model_sales.predict([[35000]])\nprint(f\"Przewidywana sprzedaż przy budżecie 35k: {new_sales[0]:.0f}\")\n\n\n\n\n\n\n\nPokaż przewidywaną sprzedaż\n\n\n\n\n\n\n\nPrzewidywana sprzedaż przy budżecie 35k: 300000\n\n\n\n\n\n\n\n2) Analizy finansowe\n\n# Relacja między PKB a konsumpcją\neconomics_data = pd.DataFrame({\n    'pkb_per_capita': [25000, 30000, 35000, 40000, 45000],\n    'konsumpcja': [18000, 21000, 24000, 27000, 30000]\n})\n\nmodel_econ = LinearRegression()\nmodel_econ.fit(economics_data[['pkb_per_capita']], economics_data['konsumpcja'])\n\n# Współczynnik skłonności do konsumpcji\nprint(f\"Na każde dodatkowe 1000zł PKB, konsumpcja rośnie o: {model_econ.coef_[0]:.0f}zł\")\n\n\n\n\n\n\n\nPokaż współczynnik skłonności do konsumpcji\n\n\n\n\n\n\n\nNa każde dodatkowe 1000zł PKB, konsumpcja rośnie o: 1zł"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#najczęstsze-pułapki-i-jak-ich-unikać",
    "href": "cheatsheets/02-linear-regression.html#najczęstsze-pułapki-i-jak-ich-unikać",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "⚠️ Najczęstsze pułapki i jak ich unikać",
    "text": "⚠️ Najczęstsze pułapki i jak ich unikać\n\n1) Overfitting z wieloma zmiennymi\n\n# ZŁO: za dużo features względem danych\n# Jeśli masz 100 mieszkań, nie używaj 50 features!\n\n# DOBRZE: zasada kciuka\ndef check_features_ratio(X, y):\n    ratio = len(y) / X.shape[1]\n    if ratio &lt; 10:\n        print(f\"⚠️ Uwaga: masz tylko {ratio:.1f} obserwacji na feature!\")\n        print(\"Rozważ: więcej danych lub mniej features\")\n    else:\n        print(f\"✅ OK: {ratio:.1f} obserwacji na feature\")\n\ncheck_features_ratio(X_train, y_train)\n\n\n\n\n\n\n\nPokaż wyniki\n\n\n\n\n\n\n\n✅ OK: 200.0 obserwacji na feature\n\n\n\n\n\n\n\n2) Sprawdzanie założeń linearności\n\n# Sprawdź czy zależności są rzeczywiście liniowe\nfrom scipy import stats\n\nfor col in X.columns:\n    correlation, p_value = stats.pearsonr(data[col], data['cena'])\n    print(f\"{col}: korelacja={correlation:.3f}, p-value={p_value:.3f}\")\n    \n    if abs(correlation) &lt; 0.1:\n        print(f\"⚠️ {col} ma słabą korelację - może nie być przydatna\")\n\n\n\n\n\n\n\nPokaż wyniki korelacji\n\n\n\n\n\n\n\npowierzchnia: korelacja=0.949, p-value=0.000\nliczba_pokoi: korelacja=0.025, p-value=0.428\n⚠️ liczba_pokoi ma słabą korelację - może nie być przydatna\nwiek_budynku: korelacja=-0.116, p-value=0.000\nodleglosc_centrum: korelacja=-0.042, p-value=0.184\n⚠️ odleglosc_centrum ma słabą korelację - może nie być przydatna\n\n\n\n\n\n\n\n3) Wielokoliniowość (features korelują między sobą)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Przykładowe dane features (demonstracja)\nnp.random.seed(42)\nn_samples = 1000\n\nX = pd.DataFrame({\n    'powierzchnia': np.random.normal(70, 25, n_samples),\n    'liczba_pokoi': np.random.randint(1, 5, n_samples),\n    'wiek_budynku': np.random.randint(0, 50, n_samples),\n    'odleglosc_centrum': np.random.exponential(5, n_samples)\n})\n\n# Oblicz korelacje między features\ncorrelation_matrix = X.corr()\n\n# Przygotuj wykres\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Korelacje między features')\nplt.close()\n\nprint(\"Macierz korelacji między zmiennymi:\")\nprint(correlation_matrix)\nprint(\"\\nInterpretacja:\")\nprint(\"• Wartości blisko 1.0 = silna korelacja pozytywna\")\nprint(\"• Wartości blisko -1.0 = silna korelacja negatywna\") \nprint(\"• Wartości blisko 0.0 = brak korelacji\")\nprint(\"\\nJeśli korelacja między features &gt; 0.8, usuń jedną z nich (multicollinearity)!\")\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Korelacje między features')\nplt.show()\n\n\n\n\n\n\n\nPokaż mapę korelacji\n\n\n\n\n\n\n\nMacierz korelacji między zmiennymi:\n                   powierzchnia  liczba_pokoi  wiek_budynku  odleglosc_centrum\npowierzchnia           1.000000     -0.064763      0.033920           0.029856\nliczba_pokoi          -0.064763      1.000000     -0.003894          -0.057555\nwiek_budynku           0.033920     -0.003894      1.000000          -0.037242\nodleglosc_centrum      0.029856     -0.057555     -0.037242           1.000000\n\nInterpretacja:\n• Wartości blisko 1.0 = silna korelacja pozytywna\n• Wartości blisko -1.0 = silna korelacja negatywna\n• Wartości blisko 0.0 = brak korelacji\n\nJeśli korelacja między features &gt; 0.8, usuń jedną z nich (multicollinearity)!\n\n\n\n\n\nMapa korelacji między zmiennymi"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#parametry-do-tuningu",
    "href": "cheatsheets/02-linear-regression.html#parametry-do-tuningu",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "🔧 Parametry do tuningu",
    "text": "🔧 Parametry do tuningu\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n# 1) Standardization - gdy features mają różne skale\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train)\n\n# 2) Polynomial Features - dla nieliniowych zależności\npoly_model = Pipeline([\n    ('poly', PolynomialFeatures(degree=2)),\n    ('linear', LinearRegression())\n])\npoly_model.fit(X_train, y_train)\n\npoly_pred = poly_model.predict(X_test)\npoly_r2 = r2_score(y_test, poly_pred)\nprint(f\"Polynomial Regression R²: {poly_r2:.3f}\")\n\n# 3) Regularization - Ridge i Lasso\nfrom sklearn.linear_model import Ridge, Lasso\n\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\nridge_r2 = r2_score(y_test, ridge.predict(X_test))\n\nlasso = Lasso(alpha=1.0)\nlasso.fit(X_train, y_train)\nlasso_r2 = r2_score(y_test, lasso.predict(X_test))\n\nprint(f\"Ridge R²: {ridge_r2:.3f}\")\nprint(f\"Lasso R²: {lasso_r2:.3f}\")\n\n\n\n\n\n\n\nPokaż wyniki predykcji cen\n\n\n\n\n\n\n\nPolynomial Regression R²: 0.933\nRidge R²: 0.934\nLasso R²: 0.934"
  },
  {
    "objectID": "cheatsheets/02-linear-regression.html#real-world-przykłady-zastosowań",
    "href": "cheatsheets/02-linear-regression.html#real-world-przykłady-zastosowań",
    "title": "Linear Regression — przewiduj wartości liczbowe",
    "section": "🌍 Real-world przykłady zastosowań",
    "text": "🌍 Real-world przykłady zastosowań\n\nE-commerce: Przewidywanie wartości życiowej klienta (LTV)\nNieruchomości: Automatyczna wycena domów (Zillow)\nFinanse: Scoring kredytowy, przewidywanie cen akcji\nMarketing: ROI kampanii reklamowych\nSupply Chain: Prognozowanie popytu na produkty\n\n\n\n\n\n\n\n💡 Kiedy używać Linear Regression?\n\n\n\n✅ UŻYJ GDY:\n\nPrzewidujesz wartości liczbowe (continuous target)\nZależności wydają się liniowe\nChcesz interpretowalny model\nMasz stosunkowo mało features\n\n❌ NIE UŻYWAJ GDY:\n\nTarget jest kategoryczny (użyj klasyfikacji)\nZależności są bardzo nieliniowe (użyj Random Forest, XGBoost)\nMasz tysiące features (użyj regularization)\n\n\n\nNastępna ściągawka: Decision Trees - klasyfikacja 🌳"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html",
    "href": "cheatsheets/01-intro-ml.html",
    "title": "Wprowadzenie do Machine Learning — fundament",
    "section": "",
    "text": "Machine Learning (ML) to dziedzina informatyki, która pozwala komputerom uczyć się i podejmować decyzje na podstawie danych, bez konieczności programowania każdej reguły z góry.\n\n\n\n\n\n\n💡 Intuicja\n\n\n\nZamiast pisać kod “jeśli temperatura &gt; 25°C, to będzie słonecznie”, ML pozwala algorytmowi samemu odkryć te zależności z historycznych danych pogodowych."
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#czym-jest-machine-learning",
    "href": "cheatsheets/01-intro-ml.html#czym-jest-machine-learning",
    "title": "Wprowadzenie do Machine Learning — fundament",
    "section": "",
    "text": "Machine Learning (ML) to dziedzina informatyki, która pozwala komputerom uczyć się i podejmować decyzje na podstawie danych, bez konieczności programowania każdej reguły z góry.\n\n\n\n\n\n\n💡 Intuicja\n\n\n\nZamiast pisać kod “jeśli temperatura &gt; 25°C, to będzie słonecznie”, ML pozwala algorytmowi samemu odkryć te zależności z historycznych danych pogodowych."
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#główne-rodzaje-ml",
    "href": "cheatsheets/01-intro-ml.html#główne-rodzaje-ml",
    "title": "Wprowadzenie do Machine Learning — fundament",
    "section": "📊 Główne rodzaje ML",
    "text": "📊 Główne rodzaje ML\n\n1) Supervised Learning (Uczenie nadzorowane)\nMamy dane + znamy prawidłowe odpowiedzi\n\n# Przykład: predykcja ceny domu\n# Dane wejściowe: powierzchnia, lokalizacja, rok budowy\n# Cel: przewidzieć cenę\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Przykładowe dane\ndata = pd.DataFrame({\n    'powierzchnia': [50, 75, 100, 120, 150],\n    'rok_budowy': [1990, 2000, 2010, 2015, 2020],\n    'cena': [300000, 400000, 550000, 650000, 800000]  # znamy prawdziwe ceny!\n})\n\n# Trenowanie modelu\nmodel = LinearRegression()\nX = data[['powierzchnia', 'rok_budowy']]\ny = data['cena']\nmodel.fit(X, y)\n\n# Predykcja dla nowego domu\nnowy_dom = [[90, 2005]]\nprzewidywana_cena = model.predict(nowy_dom)\n\n# Zapisz wyniki do zmiennych dla expandera\ndane_treningowe = data.to_string()\nwynik_predykcji = f\"Przewidywana cena: {przewidywana_cena[0]:.0f} zł\"\n\nprint(\"Dane treningowe:\")\nprint(dane_treningowe)\nprint(f\"\\nPredykcja dla domu 90m², rok 2005:\")\nprint(wynik_predykcji)\n\n\n\n\n\n\n\nPokaż wyniki predykcji cen\n\n\n\n\n\n\n\nDane treningowe:\n   powierzchnia  rok_budowy    cena\n0            50        1990  300000\n1            75        2000  400000\n2           100        2010  550000\n3           120        2015  650000\n4           150        2020  800000\n\nPredykcja dla domu 90m², rok 2005:\nPrzewidywana cena: 493819 zł\n\n\n\n\n\nReal-world zastosowania:\n\nPredykcja cen akcji/nieruchomości\nDiagnoza medyczna (klasyfikacja chorób)\nFiltrowanie spamu w emailach\nRozpoznawanie mowy/obrazów\n\n\n\n\n2) Unsupervised Learning (Uczenie nienadzorowane)\nMamy tylko dane, szukamy ukrytych wzorców\n\n# Przykład: segmentacja klientów sklepu\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Dane klientów: wiek i wydatki miesięczne\nklienci = pd.DataFrame({\n    'wiek': [25, 30, 35, 22, 28, 45, 50, 55, 60, 65],\n    'wydatki': [2000, 2500, 3000, 1800, 2200, 4000, 4500, 3500, 3000, 2800]\n})\n\n# Grupowanie klientów w 3 segmenty\nkmeans = KMeans(n_clusters=3, random_state=42)\nklienci['segment'] = kmeans.fit_predict(klienci[['wiek', 'wydatki']])\n\n# Przygotuj wyniki do wyświetlenia\ndane_klientow = klienci.head().to_string()\nsegmenty_info = []\nfor i in range(3):\n    segment = klienci[klienci['segment'] == i]\n    segmenty_info.append(f\"Segment {i}: średni wiek {segment['wiek'].mean():.0f}, średnie wydatki {segment['wydatki'].mean():.0f}\")\n\nprzyklad_predykcji = kmeans.predict([[30, 2500]])[0]\n\nprint(\"Dane klientów:\")\nprint(dane_klientow)\nprint(\"\\nSegmenty klientów:\")\nfor info in segmenty_info:\n    print(info)\nprint(f\"\\nPrzykład: klient 30 lat, wydaje 2500zł -&gt; segment {przyklad_predykcji}\")\n\n\n\n\n\n\n\nPokaż wyniki segmentacji klientów\n\n\n\n\n\n\n\nDane klientów:\n   wiek  wydatki  segment\n0    25     2000        0\n1    30     2500        2\n2    35     3000        2\n3    22     1800        0\n4    28     2200        0\n\nSegmenty klientów:\nSegment 0: średni wiek 25, średnie wydatki 2000\nSegment 1: średni wiek 50, średnie wydatki 4000\nSegment 2: średni wiek 48, średnie wydatki 2825\n\nPrzykład: klient 30 lat, wydaje 2500zł -&gt; segment 2\n\n\n\n\n\nReal-world zastosowania:\n\nSegmentacja klientów (marketing)\nWykrywanie anomalii (cyberbezpieczeństwo)\nAnaliza koszykowa (co kupują razem)\nKompresja danych\n\n\n\n\n3) Reinforcement Learning (Uczenie ze wzmocnieniem)\nAgent uczy się przez interakcję i nagrody/kary\n\n# Przykład koncepcyjny: optymalizacja reklam\nclass SimpleAgent:\n    def __init__(self):\n        # Jakie reklamy pokazywać: [\"sportowe\", \"technologiczne\", \"modowe\"]\n        self.ad_types = [\"sportowe\", \"technologiczne\", \"modowe\"]\n        self.rewards = [0, 0, 0]  # nagrody za każdy typ\n        self.counts = [0, 0, 0]   # ile razy pokazane\n    \n    def choose_ad(self):\n        # Wybierz reklamę z najwyższą średnią nagrodą\n        avg_rewards = [r/max(c,1) for r, c in zip(self.rewards, self.counts)]\n        return avg_rewards.index(max(avg_rewards))\n    \n    def update_reward(self, ad_type, clicked):\n        # Aktualizuj nagrody na podstawie kliknięć\n        self.counts[ad_type] += 1\n        if clicked:\n            self.rewards[ad_type] += 1\n    \n    def get_stats(self):\n        stats_list = []\n        for i, ad_type in enumerate(self.ad_types):\n            rate = self.rewards[i] / max(self.counts[i], 1) * 100\n            stats_list.append(f\"{ad_type}: {self.counts[i]} pokazań, {self.rewards[i]} kliknięć ({rate:.1f}%)\")\n        return stats_list\n\n# Symulacja\nimport numpy as np\nnp.random.seed(42)\n\nagent = SimpleAgent()\nsymulacja_wyniki = []\nsymulacja_wyniki.append(\"🎯 Symulacja optymalizacji reklam:\")\nsymulacja_wyniki.append(\"Agent uczy się, które reklamy działają najlepiej...\")\n\nfor day in range(10):\n    ad = agent.choose_ad()\n    # Różne prawdopodobieństwa kliknięć dla różnych typów reklam\n    click_probs = [0.1, 0.3, 0.2]  # technologiczne najlepsze\n    clicked = np.random.random() &lt; click_probs[ad]\n    agent.update_reward(ad, clicked)\n    symulacja_wyniki.append(f\"Dzień {day+1}: pokazano {agent.ad_types[ad]}, kliknięta: {clicked}\")\n\nkoncowe_statystyki = agent.get_stats()\n\nfor wynik in symulacja_wyniki:\n    print(wynik)\nprint(\"\\n📊 Końcowe statystyki:\")\nfor stat in koncowe_statystyki:\n    print(stat)\n\n\n\n\n\n\n\nPokaż symulację agenta reklamowego\n\n\n\n\n\n\n\n🎯 Symulacja optymalizacji reklam:\nAgent uczy się, które reklamy działają najlepiej...\nDzień 1: pokazano sportowe, kliknięta: False\nDzień 2: pokazano sportowe, kliknięta: False\nDzień 3: pokazano sportowe, kliknięta: False\nDzień 4: pokazano sportowe, kliknięta: False\nDzień 5: pokazano sportowe, kliknięta: False\nDzień 6: pokazano sportowe, kliknięta: False\nDzień 7: pokazano sportowe, kliknięta: True\nDzień 8: pokazano sportowe, kliknięta: False\nDzień 9: pokazano sportowe, kliknięta: False\nDzień 10: pokazano sportowe, kliknięta: False\n\n📊 Końcowe statystyki:\nsportowe: 10 pokazań, 1 kliknięć (10.0%)\ntechnologiczne: 0 pokazań, 0 kliknięć (0.0%)\nmodowe: 0 pokazań, 0 kliknięć (0.0%)\n\n\n\n\n\nReal-world zastosowania:\n\nGry komputerowe (AI graczy)\nAutonomiczne pojazdy\nOptymalizacja reklam online\nRoboty przemysłowe"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#jak-wybrać-odpowiedni-typ-ml",
    "href": "cheatsheets/01-intro-ml.html#jak-wybrać-odpowiedni-typ-ml",
    "title": "Wprowadzenie do Machine Learning — fundament",
    "section": "🎯 Jak wybrać odpowiedni typ ML?",
    "text": "🎯 Jak wybrać odpowiedni typ ML?\n\n\n\n\n\n\n\n\nSytuacja\nTyp ML\nPrzykład\n\n\n\n\nMasz dane z prawidłowymi odpowiedziami\nSupervised\nSpam/nie-spam w emailach\n\n\nChcesz znaleźć ukryte grupы\nUnsupervised\nSegmentacja klientów\n\n\nSystem ma się uczyć przez trial & error\nReinforcement\nBot do gier"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#podstawowe-kroki-projektu-ml",
    "href": "cheatsheets/01-intro-ml.html#podstawowe-kroki-projektu-ml",
    "title": "Wprowadzenie do Machine Learning — fundament",
    "section": "🔧 Podstawowe kroki projektu ML",
    "text": "🔧 Podstawowe kroki projektu ML\n\n# Kompletny workflow ML na przykładzie klasyfikacji iris\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# 1. Załaduj i poznaj dane\niris = load_iris()\ndata = pd.DataFrame(iris.data, columns=iris.feature_names)\ndata['target'] = iris.target\ntarget_names = iris.target_names\n\n# 2. Przygotuj dane (czyszczenie, encoding)\nX = data.drop('target', axis=1)\ny = data['target']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 3. Podziel na train/test\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# 4. Wybierz i wytrenuj model\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# 5. Oceń wyniki\npredictions = model.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\n\n# Przykład predykcji\nsample_flower = X_test[0:1]\npredicted_class = model.predict(sample_flower)[0]\npredicted_name = target_names[predicted_class]\nactual_name = target_names[y_test.iloc[0]]\n\n# Przygotuj wyniki do wyświetlenia\nwyniki_workflow = []\nwyniki_workflow.append(\"🔬 Kompletny workflow projektu ML\")\nwyniki_workflow.append(\"=\" * 40)\nwyniki_workflow.append(\"1️⃣ Dane załadowane:\")\nwyniki_workflow.append(f\"Kształt: {data.shape}\")\nwyniki_workflow.append(f\"Klasy: {target_names}\")\nwyniki_workflow.append(f\"Pierwsze 3 wiersze:\\n{data.head(3).to_string()}\")\nwyniki_workflow.append(f\"\\n2️⃣ Dane przeskalowane (pierwsze 3 cechy pierwszej próbki):\")\nwyniki_workflow.append(f\"Przed: {X.iloc[0, :3].values}\")\nwyniki_workflow.append(f\"Po: {X_scaled[0, :3]}\")\nwyniki_workflow.append(f\"\\n3️⃣ Podział danych:\")\nwyniki_workflow.append(f\"Train: {len(X_train)} próbek\")\nwyniki_workflow.append(f\"Test: {len(X_test)} próbek\")\nwyniki_workflow.append(f\"\\n4️⃣ Model wytrenowany: RandomForestClassifier\")\nwyniki_workflow.append(f\"\\n5️⃣ Wyniki:\")\nwyniki_workflow.append(f\"Dokładność: {accuracy:.2%}\")\nwyniki_workflow.append(f\"\\nPrzykład predykcji:\")\nwyniki_workflow.append(f\"Przewidywana klasa: {predicted_name}\")\nwyniki_workflow.append(f\"Rzeczywista klasa: {actual_name}\")\nwyniki_workflow.append(\"✅ Poprawnie!\" if predicted_name == actual_name else \"❌ Błąd\")\n\nfor wynik in wyniki_workflow:\n    print(wynik)\n\n\n\n\n\n\n\nPokaż kompletny workflow ML\n\n\n\n\n\n\n\n🔬 Kompletny workflow projektu ML\n========================================\n1️⃣ Dane załadowane:\nKształt: (150, 5)\nKlasy: ['setosa' 'versicolor' 'virginica']\nPierwsze 3 wiersze:\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n0                5.1               3.5                1.4               0.2       0\n1                4.9               3.0                1.4               0.2       0\n2                4.7               3.2                1.3               0.2       0\n\n2️⃣ Dane przeskalowane (pierwsze 3 cechy pierwszej próbki):\nPrzed: [5.1 3.5 1.4]\nPo: [-0.90068117  1.01900435 -1.34022653]\n\n3️⃣ Podział danych:\nTrain: 120 próbek\nTest: 30 próbek\n\n4️⃣ Model wytrenowany: RandomForestClassifier\n\n5️⃣ Wyniki:\nDokładność: 100.00%\n\nPrzykład predykcji:\nPrzewidywana klasa: versicolor\nRzeczywista klasa: versicolor\n✅ Poprawnie!"
  },
  {
    "objectID": "cheatsheets/01-intro-ml.html#najważniejsze-biblioteki-python-dla-ml",
    "href": "cheatsheets/01-intro-ml.html#najważniejsze-biblioteki-python-dla-ml",
    "title": "Wprowadzenie do Machine Learning — fundament",
    "section": "💡 Najważniejsze biblioteki Python dla ML",
    "text": "💡 Najważniejsze biblioteki Python dla ML\n# Podstawowe przetwarzanie danych\nimport pandas as pd      # DataFrames, CSV, analiza\nimport numpy as np       # obliczenia numeryczne\n\n# Machine Learning\nfrom sklearn import *    # algorytmy ML, preprocessing, metryki\nimport xgboost as xgb   # zaawansowane drzewa decyzyjne\n\n# Wizualizacja\nimport matplotlib.pyplot as plt  # wykresy\nimport seaborn as sns           # piękne wykresy statystyczne\n\n# Deep Learning\nimport tensorflow as tf  # sieci neuronowe (Google)\nimport torch            # sieci neuronowe (Facebook)\n\n\n\n\n\n\n\n🎯 Pro tips dla początkujących\n\n\n\n\nZacznij od prostych algorytmów - Linear Regression, Decision Trees\n80% czasu to przygotowanie danych - czyszczenie, eksploracja, feature engineering\nZawsze sprawdź czy model nie jest overfitted - użyj validation set\nRozumiej swoje dane przed wyborem algorytmu\nPraktyka &gt; teoria - rób dużo projektów na różnych danych!\n\n\n\nNastępna ściągawka: Linear Regression w praktyce 🚀"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html",
    "href": "cheatsheets/03-decision-trees.html",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "",
    "text": "Decision Trees to jeden z najbardziej intuicyjnych algorytmów ML, który podejmuje decyzje jak człowiek - zadając szereg pytań typu “tak/nie” i na podstawie odpowiedzi klasyfikuje lub przewiduje wartości.\n\n\n\n\n\n\n💡 Intuicja\n\n\n\nWyobraź sobie lekarza, który diagnozuje chorobę: “Czy ma gorączkę? TAK → Czy boli gardło? TAK → Czy ma katar? NIE → Prawdopodobnie angina”. Decision Tree działa dokładnie tak samo!"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#czym-są-decision-trees",
    "href": "cheatsheets/03-decision-trees.html#czym-są-decision-trees",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "",
    "text": "Decision Trees to jeden z najbardziej intuicyjnych algorytmów ML, który podejmuje decyzje jak człowiek - zadając szereg pytań typu “tak/nie” i na podstawie odpowiedzi klasyfikuje lub przewiduje wartości.\n\n\n\n\n\n\n💡 Intuicja\n\n\n\nWyobraź sobie lekarza, który diagnozuje chorobę: “Czy ma gorączkę? TAK → Czy boli gardło? TAK → Czy ma katar? NIE → Prawdopodobnie angina”. Decision Tree działa dokładnie tak samo!"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#praktyczny-przykład-klasyfikacja-klientów-banku",
    "href": "cheatsheets/03-decision-trees.html#praktyczny-przykład-klasyfikacja-klientów-banku",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "🎯 Praktyczny przykład: klasyfikacja klientów banku",
    "text": "🎯 Praktyczny przykład: klasyfikacja klientów banku\nCzy klient weźmie kredyt na podstawie jego profilu?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Tworzenie realistycznych danych klientów banku\nnp.random.seed(42)\nn_clients = 2000\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_clients),\n    'dochod': np.random.lognormal(10, 0.5, n_clients),  # log-normal dla dochodów\n    'historia_kredytowa': np.random.choice(['dobra', 'średnia', 'słaba'], n_clients, p=[0.6, 0.3, 0.1]),\n    'zatrudnienie': np.random.choice(['stałe', 'tymczasowe', 'bezrobotny'], n_clients, p=[0.7, 0.25, 0.05]),\n    'oszczednosci': np.random.exponential(20000, n_clients),\n    'liczba_dzieci': np.random.randint(0, 4, n_clients)\n})\n\n# Realistyczna logika decyzyjna banku\ndef czy_kredyt(row):\n    score = 0\n    \n    # Wiek (30-50 lat = najlepsi klienci)\n    if 30 &lt;= row['wiek'] &lt;= 50:\n        score += 2\n    elif row['wiek'] &lt; 25 or row['wiek'] &gt; 60:\n        score -= 1\n    \n    # Dochód\n    if row['dochod'] &gt; 50000:\n        score += 3\n    elif row['dochod'] &gt; 30000:\n        score += 1\n    else:\n        score -= 2\n    \n    # Historia kredytowa\n    if row['historia_kredytowa'] == 'dobra':\n        score += 2\n    elif row['historia_kredytowa'] == 'słaba':\n        score -= 3\n    \n    # Zatrudnienie\n    if row['zatrudnienie'] == 'stałe':\n        score += 2\n    elif row['zatrudnienie'] == 'bezrobotny':\n        score -= 4\n    \n    # Oszczędności\n    if row['oszczednosci'] &gt; 50000:\n        score += 1\n    \n    # Dzieci (więcej dzieci = większe ryzyko)\n    score -= row['liczba_dzieci'] * 0.5\n    \n    # Końcowa decyzja z odrobiną losowości\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 0.5)))\n    return probability &gt; 0.5\n\ndata['kredyt_przyznany'] = data.apply(czy_kredyt, axis=1)\n\nprint(\"Statystyki klientów:\")\nprint(data.describe())\nprint(f\"\\nOdsetek przyznanych kredytów: {data['kredyt_przyznany'].mean():.1%}\")\n\n\n\n\n\n\n\nPokaż statystyki i odsetki przyznanych kredytów\n\n\n\n\n\n\n\nStatystyki klientów:\n              wiek         dochod   oszczednosci  liczba_dzieci\ncount  2000.000000    2000.000000    2000.000000    2000.000000\nmean     43.805500   25726.361013   19950.270464       1.523000\nstd      14.929203   14090.406882   20299.940268       1.130535\nmin      18.000000    4047.221838      27.128881       0.000000\n25%      31.000000   15922.761710    5923.833818       1.000000\n50%      44.000000   22782.391426   13844.909016       2.000000\n75%      56.000000   31922.003675   27566.091135       3.000000\nmax      69.000000  112447.929023  165194.684774       3.000000\n\nOdsetek przyznanych kredytów: 63.8%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#budowanie-modelu-krok-po-kroku",
    "href": "cheatsheets/03-decision-trees.html#budowanie-modelu-krok-po-kroku",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "🔧 Budowanie modelu krok po kroku",
    "text": "🔧 Budowanie modelu krok po kroku\n\n1) Przygotowanie danych\n\n# Encoding kategorycznych zmiennych\nfrom sklearn.preprocessing import LabelEncoder\n\ndata_encoded = data.copy()\nle_historia = LabelEncoder()\nle_zatrudnienie = LabelEncoder()\n\ndata_encoded['historia_kredytowa_num'] = le_historia.fit_transform(data['historia_kredytowa'])\ndata_encoded['zatrudnienie_num'] = le_zatrudnienie.fit_transform(data['zatrudnienie'])\n\n# Features do modelu\nfeature_columns = ['wiek', 'dochod', 'historia_kredytowa_num', 'zatrudnienie_num', 'oszczednosci', 'liczba_dzieci']\nX = data_encoded[feature_columns]\ny = data_encoded['kredyt_przyznany']\n\n# Podział train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Dane treningowe: {len(X_train)} klientów\")\nprint(f\"Dane testowe: {len(X_test)} klientów\")\n\n\n\n\n\n\n\nPokaż wyniki dane treningowe i testowe\n\n\n\n\n\n\n\nDane treningowe: 1600 klientów\nDane testowe: 400 klientów\n\n\n\n\n\n\n\n2) Trenowanie Decision Tree\n\n# Tworzenie modelu z ograniczeniami (żeby nie był za głęboki)\ndt_model = DecisionTreeClassifier(\n    max_depth=5,        # maksymalna głębokość drzewa\n    min_samples_split=50,  # min. próbek do podziału węzła\n    min_samples_leaf=20,   # min. próbek w liściu\n    random_state=42\n)\n\n# Trenowanie\ndt_model.fit(X_train, y_train)\n\nprint(\"Model wytrenowany!\")\nprint(f\"Głębokość drzewa: {dt_model.tree_.max_depth}\")\nprint(f\"Liczba liści: {dt_model.tree_.n_leaves}\")\n\n\n\n\n\n\n\nPokaż parametry wytrenowanego modelu\n\n\n\n\n\n\n\nModel wytrenowany!\nGłębokość drzewa: 5\nLiczba liści: 20\n\n\n\n\n\n\n\n3) Ewaluacja modelu\n\n# Predykcje\ny_pred = dt_model.predict(X_test)\ny_pred_proba = dt_model.predict_proba(X_test)[:, 1]\n\n# Metryki\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nDokładność modelu: {accuracy:.1%}\")\n\n# Szczegółowy raport\nprint(\"\\nRaport klasyfikacji:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:    Nie    Tak\")\nprint(f\"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}\")\n\n\n\n\n\n\n\nPokaż wyniki\n\n\n\n\n\n\n\n\nConfusion Matrix:\nPrzewidywane:    Nie    Tak\nRzeczywiste Nie: 122    34\nRzeczywiste Tak:  27   217"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#wizualizacja-drzewa-decyzyjnego",
    "href": "cheatsheets/03-decision-trees.html#wizualizacja-drzewa-decyzyjnego",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "🎨 Wizualizacja drzewa decyzyjnego",
    "text": "🎨 Wizualizacja drzewa decyzyjnego\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Tworzenie przykładowych danych (kompletny przykład)\nnp.random.seed(42)\nn_clients = 2000\n\ndata = pd.DataFrame({\n    'wiek': np.random.randint(18, 70, n_clients),\n    'dochod': np.random.lognormal(10, 0.5, n_clients),\n    'historia_kredytowa': np.random.choice(['dobra', 'średnia', 'słaba'], n_clients, p=[0.6, 0.3, 0.1]),\n    'zatrudnienie': np.random.choice(['stałe', 'tymczasowe', 'bezrobotny'], n_clients, p=[0.7, 0.25, 0.05]),\n    'oszczednosci': np.random.exponential(20000, n_clients),\n    'liczba_dzieci': np.random.randint(0, 4, n_clients)\n})\n\n# Prosta logika przyznawania kredytu\ndef czy_kredyt(row):\n    score = 0\n    if 30 &lt;= row['wiek'] &lt;= 50:\n        score += 2\n    if row['dochod'] &gt; 50000:\n        score += 3\n    if row['historia_kredytowa'] == 'dobra':\n        score += 2\n    if row['zatrudnienie'] == 'stałe':\n        score += 2\n    if row['oszczednosci'] &gt; 50000:\n        score += 1\n    score -= row['liczba_dzieci'] * 0.5\n    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 0.5)))\n    return probability &gt; 0.5\n\ndata['kredyt_przyznany'] = data.apply(czy_kredyt, axis=1)\n\n# Encoding i model\ndata_encoded = data.copy()\nle_historia = LabelEncoder()\nle_zatrudnienie = LabelEncoder()\ndata_encoded['historia_kredytowa_num'] = le_historia.fit_transform(data['historia_kredytowa'])\ndata_encoded['zatrudnienie_num'] = le_zatrudnienie.fit_transform(data['zatrudnienie'])\n\nfeature_columns = ['wiek', 'dochod', 'historia_kredytowa_num', 'zatrudnienie_num', 'oszczednosci', 'liczba_dzieci']\nX = data_encoded[feature_columns]\ny = data_encoded['kredyt_przyznany']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Trenowanie drzewa\ndt_model = DecisionTreeClassifier(max_depth=5, min_samples_split=50, min_samples_leaf=20, random_state=42)\ndt_model.fit(X_train, y_train)\n\n# Ewaluacja\ny_pred = dt_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\n# Ważność features\nfeature_names = ['wiek', 'dochód', 'historia_kred.', 'zatrudnienie', 'oszczędności', 'liczba_dzieci']\nimportance = dt_model.feature_importances_\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importance\n}).sort_values('importance', ascending=False)\n\n# Przygotuj wykres drzewa\nplt.figure(figsize=(20, 10))\ntree.plot_tree(dt_model, \n               feature_names=feature_names,\n               class_names=['Nie', 'Tak'],\n               filled=True,\n               rounded=True,\n               fontsize=10)\nplt.title(\"Drzewo decyzyjne - Przyznanie kredytu\")\nplt.close()\n\nprint(f\"Wyniki modelu Decision Tree:\")\nprint(f\"Głębokość drzewa: {dt_model.tree_.max_depth}\")\nprint(f\"Liczba liści: {dt_model.tree_.n_leaves}\")\nprint(f\"Dokładność modelu: {accuracy:.1%}\")\n\nprint(f\"\\nStatystyki danych:\")\nprint(f\"Dane treningowe: {len(X_train)} klientów\")\nprint(f\"Dane testowe: {len(X_test)} klientów\")\nprint(f\"Odsetek przyznanych kredytów: {data['kredyt_przyznany'].mean():.1%}\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(\"Przewidywane:    Nie    Tak\")\nprint(f\"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}\")\nprint(f\"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}\")\n\nprint(\"\\nWażność cech:\")\nfor _, row in feature_importance.iterrows():\n    print(f\"{row['feature']}: {row['importance']:.3f}\")\n\n# Odtwórz wykres drzewa\nplt.figure(figsize=(20, 10))\ntree.plot_tree(dt_model, \n               feature_names=feature_names,\n               class_names=['Nie', 'Tak'],\n               filled=True,\n               rounded=True,\n               fontsize=10)\nplt.title(\"Drzewo decyzyjne - Przyznanie kredytu\")\nplt.show()\n\n\n\n\n\n\n\nPokaż wyniki i drzewo decyzyjne\n\n\n\n\n\n\n\nWyniki modelu Decision Tree:\nGłębokość drzewa: 5\nLiczba liści: 13\nDokładność modelu: 95.0%\n\nStatystyki danych:\nDane treningowe: 1600 klientów\nDane testowe: 400 klientów\nOdsetek przyznanych kredytów: 93.2%\n\nConfusion Matrix:\nPrzewidywane:    Nie    Tak\nRzeczywiste Nie:  19    11\nRzeczywiste Tak:   9   361\n\nWażność cech:\nzatrudnienie: 0.441\nwiek: 0.317\nhistoria_kred.: 0.178\nliczba_dzieci: 0.055\noszczędności: 0.008\ndochód: 0.001\n\n\n\n\n\nWizualizacja drzewa decyzyjnego dla kredytów"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#interpretacja-decyzji-dla-konkretnego-klienta",
    "href": "cheatsheets/03-decision-trees.html#interpretacja-decyzji-dla-konkretnego-klienta",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "🔍 Interpretacja decyzji dla konkretnego klienta",
    "text": "🔍 Interpretacja decyzji dla konkretnego klienta\n\n# Funkcja do interpretacji ścieżki decyzyjnej\ndef explain_decision(model, X_sample, feature_names):\n    # Pobierz ścieżkę w drzewie\n    leaf_id = model.decision_path(X_sample.reshape(1, -1)).toarray()[0]\n    feature = model.tree_.feature\n    threshold = model.tree_.threshold\n    \n    print(\"Ścieżka decyzyjna:\")\n    for node_id in range(len(leaf_id)):\n        if leaf_id[node_id] == 1:  # jeśli węzeł jest na ścieżce\n            if feature[node_id] != -2:  # jeśli nie jest liściem\n                feature_name = feature_names[feature[node_id]]\n                threshold_val = threshold[node_id]\n                feature_val = X_sample[feature[node_id]]\n                \n                if feature_val &lt;= threshold_val:\n                    condition = \"&lt;=\"\n                else:\n                    condition = \"&gt;\"\n                    \n                print(f\"  {feature_name} ({feature_val:.0f}) {condition} {threshold_val:.0f}\")\n\n# Przykład dla konkretnego klienta\nsample_client = X_test.iloc[0]\nprediction = dt_model.predict([sample_client])[0]\nprobability = dt_model.predict_proba([sample_client])[0]\n\nprint(f\"Klient testowy:\")\nprint(f\"Wiek: {sample_client['wiek']}\")\nprint(f\"Dochód: {sample_client['dochod']:.0f}\")\nprint(f\"Oszczędności: {sample_client['oszczednosci']:.0f}\")\nprint(f\"\\nDecyzja: {'KREDYT PRZYZNANY' if prediction else 'KREDYT ODRZUCONY'}\")\nprint(f\"Prawdopodobieństwo: {probability[1]:.1%}\")\n\nexplain_decision(dt_model, sample_client.values, feature_names)\n\n\n\n\n\n\n\nPokaż dane klienta testowego\n\n\n\n\n\n\n\nKlient testowy:\nWiek: 56.0\nDochód: 7927\nOszczędności: 32270\n\nDecyzja: KREDYT ODRZUCONY\nPrawdopodobieństwo: 3.0%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#różne-zastosowania-decision-trees",
    "href": "cheatsheets/03-decision-trees.html#różne-zastosowania-decision-trees",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "🎯 Różne zastosowania Decision Trees",
    "text": "🎯 Różne zastosowania Decision Trees\n\n1) Medyczna diagnoza\n\n# Przykład klasyfikacji ryzyka chorób serca\nmedical_features = ['wiek', 'cholesterol', 'ciśnienie', 'BMI', 'pali_papierosy']\n# Target: 'ryzyko_chorób_serca' (wysokie/niskie)\n\nmedical_tree = DecisionTreeClassifier(max_depth=4)\n# Model automatycznie znajdzie progi: \"Jeśli cholesterol &gt; 240 I BMI &gt; 30 ORAZ wiek &gt; 50...\"\n\n\n\n2) Marketing - segmentacja klientów\n\n# Przewidywanie czy klient kupi produkt premium\nmarketing_features = ['dochód_roczny', 'wiek', 'wykształcenie', 'poprzednie_zakupy']\n# Target: 'kupi_premium' (tak/nie)\n\nmarketing_tree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=100)\n# Wynik: jasne reguły marketingowe do targetowania reklam\n\n\n\n3) HR - decyzje o zatrudnieniu\n\n# Przewidywanie sukcesu kandydata w rekrutacji\nhr_features = ['doświadczenie_lat', 'wykształcenie', 'wynik_testów', 'referencje']\n# Target: 'zatrudniony' (tak/nie)\n\nhr_tree = DecisionTreeClassifier(max_depth=5)\n# Wynik: automatyczne zasady rekrutacyjne"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#tuning-parametrów",
    "href": "cheatsheets/03-decision-trees.html#tuning-parametrów",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "⚙️ Tuning parametrów",
    "text": "⚙️ Tuning parametrów\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Najważniejsze parametry do tuningu\nparam_grid = {\n    'max_depth': [3, 5, 7, 10, None],\n    'min_samples_split': [20, 50, 100],\n    'min_samples_leaf': [10, 20, 50],\n    'criterion': ['gini', 'entropy']\n}\n\n# Grid search z cross-validation\ngrid_search = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(\"Najlepsze parametry:\")\nprint(grid_search.best_params_)\nprint(f\"Najlepsza dokładność CV: {grid_search.best_score_:.1%}\")\n\n# Model z najlepszymi parametrami\nbest_model = grid_search.best_estimator_\nbest_accuracy = accuracy_score(y_test, best_model.predict(X_test))\nprint(f\"Dokładność na test set: {best_accuracy:.1%}\")\n\n\n\n\n\n\n\nPokaż najlepsze parametry\n\n\n\n\n\n\n\nNajlepsze parametry:\n{'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 20}\nNajlepsza dokładność CV: 96.5%\nDokładność na test set: 95.0%"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#problemy-i-rozwiązania",
    "href": "cheatsheets/03-decision-trees.html#problemy-i-rozwiązania",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "⚠️ Problemy i rozwiązania",
    "text": "⚠️ Problemy i rozwiązania\n\n1) Overfitting - drzewo za głębokie\n\n# Problem: drzewo \"pamięta\" dane treningowe\noverfitted_tree = DecisionTreeClassifier()  # bez ograniczeń!\noverfitted_tree.fit(X_train, y_train)\n\ntrain_acc = accuracy_score(y_train, overfitted_tree.predict(X_train))\ntest_acc = accuracy_score(y_test, overfitted_tree.predict(X_test))\n\nprint(f\"Overfitted model:\")\nprint(f\"Train accuracy: {train_acc:.1%}\")\nprint(f\"Test accuracy: {test_acc:.1%}\")\nprint(f\"Różnica: {train_acc - test_acc:.1%} - to overfitting!\")\n\n# Rozwiązanie: ograniczenia\npruned_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=20)\npruned_tree.fit(X_train, y_train)\n\ntrain_acc_pruned = accuracy_score(y_train, pruned_tree.predict(X_train))\ntest_acc_pruned = accuracy_score(y_test, pruned_tree.predict(X_test))\n\nprint(f\"\\nPruned model:\")\nprint(f\"Train accuracy: {train_acc_pruned:.1%}\")\nprint(f\"Test accuracy: {test_acc_pruned:.1%}\")\nprint(f\"Różnica: {train_acc_pruned - test_acc_pruned:.1%} - znacznie lepiej!\")\n\n\n\n\n\n\n\nPokaż wyniki\n\n\n\n\n\n\n\nOverfitted model:\nTrain accuracy: 100.0%\nTest accuracy: 94.8%\nRóżnica: 5.2% - to overfitting!\n\nPruned model:\nTrain accuracy: 96.7%\nTest accuracy: 95.0%\nRóżnica: 1.7% - znacznie lepiej!\n\n\n\n\n\n\n\n2) Brak stabilności - małe zmiany = różne drzewa\n\n# Problem demonstracji\nresults = []\nfor i in range(10):\n    # Różne random_state = różne drzewa\n    tree_test = DecisionTreeClassifier(random_state=i, max_depth=5)\n    tree_test.fit(X_train, y_train)\n    acc = accuracy_score(y_test, tree_test.predict(X_test))\n    results.append(acc)\n\nprint(f\"Dokładność różnych drzew: {min(results):.1%} - {max(results):.1%}\")\nprint(f\"Rozrzut: {max(results) - min(results):.1%}\")\nprint(\"Rozwiązanie: Random Forest (następna ściągawka!)\")\n\n\n\n\n\n\n\nPokaż wyniki\n\n\n\n\n\n\n\nDokładność różnych drzew: 96.2% - 96.2%\nRozrzut: 0.0%\nRozwiązanie: Random Forest (następna ściągawka!)"
  },
  {
    "objectID": "cheatsheets/03-decision-trees.html#real-world-przypadki-użycia",
    "href": "cheatsheets/03-decision-trees.html#real-world-przypadki-użycia",
    "title": "Decision Trees — proste drzewa decyzyjne",
    "section": "🌍 Real-world przypadki użycia",
    "text": "🌍 Real-world przypadki użycia\n\nBankowość: Ocena ryzyka kredytowego, wykrywanie fraudów\nMedycyna: Systemy wspomagania diagnostyki, triage pacjentów\n\nE-commerce: Rekomendacje produktów, ustalanie cen dynamicznych\nHR: Automatyzacja procesów rekrutacyjnych\nMarketing: Segmentacja klientów, personalizacja oferowania\n\n\n\n\n\n\n\n💡 Kiedy używać Decision Trees?\n\n\n\n✅ UŻYJ GDY:\n\nPotrzebujesz interpretowalnego modelu\nDane mają kategoryczne zmienne\n\nChcesz zrozumieć “dlaczego” model podjął decyzję\nMasz nieliniowe zależności w danych\nBraków w danych nie trzeba impute’ować\n\n❌ NIE UŻYWAJ GDY:\n\nPotrzebujesz najwyższej dokładności (użyj Random Forest/XGBoost)\nMasz bardzo głębokie wzorce w danych (użyj Neural Networks)\nDane są bardzo hałaśliwe\nPrzewidujesz wartości ciągłe (lepiej Linear Regression)\n\n\n\nNastępna ściągawka: Random Forest - zespół drzew (wkrótce)! 🌲🌲🌲"
  }
]