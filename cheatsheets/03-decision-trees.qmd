---
title: "Decision Trees ‚Äî proste drzewa decyzyjne"
format:
  html:
    code-tools: true
---

## üå≥ Czym sƒÖ Decision Trees?

**Decision Trees** to jeden z najbardziej intuicyjnych algorytm√≥w ML, kt√≥ry **podejmuje decyzje jak cz≈Çowiek** - zadajƒÖc szereg pyta≈Ñ typu "tak/nie" i na podstawie odpowiedzi klasyfikuje lub przewiduje warto≈õci.

::: {.callout-note}
## üí° Intuicja
Wyobra≈∫ sobie lekarza, kt√≥ry diagnozuje chorobƒô: "Czy ma gorƒÖczkƒô? TAK ‚Üí Czy boli gard≈Ço? TAK ‚Üí Czy ma katar? NIE ‚Üí Prawdopodobnie angina". Decision Tree dzia≈Ça dok≈Çadnie tak samo!
:::

---

## üéØ Praktyczny przyk≈Çad: klasyfikacja klient√≥w banku

Czy klient we≈∫mie kredyt na podstawie jego profilu?

```{python}
#| echo: true
#| output: false

import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn import tree
import matplotlib.pyplot as plt

# Tworzenie realistycznych danych klient√≥w banku
np.random.seed(42)
n_clients = 2000

data = pd.DataFrame({
    'wiek': np.random.randint(18, 70, n_clients),
    'dochod': np.random.lognormal(10, 0.5, n_clients),  # log-normal dla dochod√≥w
    'historia_kredytowa': np.random.choice(['dobra', '≈õrednia', 's≈Çaba'], n_clients, p=[0.6, 0.3, 0.1]),
    'zatrudnienie': np.random.choice(['sta≈Çe', 'tymczasowe', 'bezrobotny'], n_clients, p=[0.7, 0.25, 0.05]),
    'oszczednosci': np.random.exponential(20000, n_clients),
    'liczba_dzieci': np.random.randint(0, 4, n_clients)
})

# Realistyczna logika decyzyjna banku
def czy_kredyt(row):
    score = 0
    
    # Wiek (30-50 lat = najlepsi klienci)
    if 30 <= row['wiek'] <= 50:
        score += 2
    elif row['wiek'] < 25 or row['wiek'] > 60:
        score -= 1
    
    # Doch√≥d
    if row['dochod'] > 50000:
        score += 3
    elif row['dochod'] > 30000:
        score += 1
    else:
        score -= 2
    
    # Historia kredytowa
    if row['historia_kredytowa'] == 'dobra':
        score += 2
    elif row['historia_kredytowa'] == 's≈Çaba':
        score -= 3
    
    # Zatrudnienie
    if row['zatrudnienie'] == 'sta≈Çe':
        score += 2
    elif row['zatrudnienie'] == 'bezrobotny':
        score -= 4
    
    # Oszczƒôdno≈õci
    if row['oszczednosci'] > 50000:
        score += 1
    
    # Dzieci (wiƒôcej dzieci = wiƒôksze ryzyko)
    score -= row['liczba_dzieci'] * 0.5
    
    # Ko≈Ñcowa decyzja z odrobinƒÖ losowo≈õci
    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 0.5)))
    return probability > 0.5

data['kredyt_przyznany'] = data.apply(czy_kredyt, axis=1)

print("Statystyki klient√≥w:")
print(data.describe())
print(f"\nOdsetek przyznanych kredyt√≥w: {data['kredyt_przyznany'].mean():.1%}")
```

::: {.callout-note collapse="true"}
## Poka≈º statystyki i odsetki przyznanych kredyt√≥w

```{python}
#| echo: false

print("Statystyki klient√≥w:")
print(data.describe())
print(f"\nOdsetek przyznanych kredyt√≥w: {data['kredyt_przyznany'].mean():.1%}")
```
:::

---

## üîß Budowanie modelu krok po kroku

### 1) Przygotowanie danych

```{python}
#| echo: true
#| output: false

# Encoding kategorycznych zmiennych
from sklearn.preprocessing import LabelEncoder

data_encoded = data.copy()
le_historia = LabelEncoder()
le_zatrudnienie = LabelEncoder()

data_encoded['historia_kredytowa_num'] = le_historia.fit_transform(data['historia_kredytowa'])
data_encoded['zatrudnienie_num'] = le_zatrudnienie.fit_transform(data['zatrudnienie'])

# Features do modelu
feature_columns = ['wiek', 'dochod', 'historia_kredytowa_num', 'zatrudnienie_num', 'oszczednosci', 'liczba_dzieci']
X = data_encoded[feature_columns]
y = data_encoded['kredyt_przyznany']

# Podzia≈Ç train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Dane treningowe: {len(X_train)} klient√≥w")
print(f"Dane testowe: {len(X_test)} klient√≥w")
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki dane treningowe i testowe

```{python}
#| echo: false

print(f"Dane treningowe: {len(X_train)} klient√≥w")
print(f"Dane testowe: {len(X_test)} klient√≥w")
```
:::

### 2) Trenowanie Decision Tree

```{python}
#| echo: true
#| output: false

# Tworzenie modelu z ograniczeniami (≈ºeby nie by≈Ç za g≈Çƒôboki)
dt_model = DecisionTreeClassifier(
    max_depth=5,        # maksymalna g≈Çƒôboko≈õƒá drzewa
    min_samples_split=50,  # min. pr√≥bek do podzia≈Çu wƒôz≈Ça
    min_samples_leaf=20,   # min. pr√≥bek w li≈õciu
    random_state=42
)

# Trenowanie
dt_model.fit(X_train, y_train)

print("Model wytrenowany!")
print(f"G≈Çƒôboko≈õƒá drzewa: {dt_model.tree_.max_depth}")
print(f"Liczba li≈õci: {dt_model.tree_.n_leaves}")
```

::: {.callout-note collapse="true"}
## Poka≈º parametry wytrenowanego modelu

```{python}
#| echo: false

print("Model wytrenowany!")
print(f"G≈Çƒôboko≈õƒá drzewa: {dt_model.tree_.max_depth}")
print(f"Liczba li≈õci: {dt_model.tree_.n_leaves}")
```
:::

### 3) Ewaluacja modelu

```{python}
#| echo: true
#| output: false

# Predykcje
y_pred = dt_model.predict(X_test)
y_pred_proba = dt_model.predict_proba(X_test)[:, 1]

# Metryki
accuracy = accuracy_score(y_test, y_pred)
print(f"\nDok≈Çadno≈õƒá modelu: {accuracy:.1%}")

# Szczeg√≥≈Çowy raport
print("\nRaport klasyfikacji:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print("Przewidywane:    Nie    Tak")
print(f"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}")
print(f"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}")
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki

```{python}
#| echo: false

cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print("Przewidywane:    Nie    Tak")
print(f"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}")
print(f"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}")
```
:::

---

## üé® Wizualizacja drzewa decyzyjnego

```{python}
#| echo: true
#| output: false

import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn import tree
import matplotlib.pyplot as plt

# Tworzenie przyk≈Çadowych danych (kompletny przyk≈Çad)
np.random.seed(42)
n_clients = 2000

data = pd.DataFrame({
    'wiek': np.random.randint(18, 70, n_clients),
    'dochod': np.random.lognormal(10, 0.5, n_clients),
    'historia_kredytowa': np.random.choice(['dobra', '≈õrednia', 's≈Çaba'], n_clients, p=[0.6, 0.3, 0.1]),
    'zatrudnienie': np.random.choice(['sta≈Çe', 'tymczasowe', 'bezrobotny'], n_clients, p=[0.7, 0.25, 0.05]),
    'oszczednosci': np.random.exponential(20000, n_clients),
    'liczba_dzieci': np.random.randint(0, 4, n_clients)
})

# Prosta logika przyznawania kredytu
def czy_kredyt(row):
    score = 0
    if 30 <= row['wiek'] <= 50:
        score += 2
    if row['dochod'] > 50000:
        score += 3
    if row['historia_kredytowa'] == 'dobra':
        score += 2
    if row['zatrudnienie'] == 'sta≈Çe':
        score += 2
    if row['oszczednosci'] > 50000:
        score += 1
    score -= row['liczba_dzieci'] * 0.5
    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 0.5)))
    return probability > 0.5

data['kredyt_przyznany'] = data.apply(czy_kredyt, axis=1)

# Encoding i model
data_encoded = data.copy()
le_historia = LabelEncoder()
le_zatrudnienie = LabelEncoder()
data_encoded['historia_kredytowa_num'] = le_historia.fit_transform(data['historia_kredytowa'])
data_encoded['zatrudnienie_num'] = le_zatrudnienie.fit_transform(data['zatrudnienie'])

feature_columns = ['wiek', 'dochod', 'historia_kredytowa_num', 'zatrudnienie_num', 'oszczednosci', 'liczba_dzieci']
X = data_encoded[feature_columns]
y = data_encoded['kredyt_przyznany']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Trenowanie drzewa
dt_model = DecisionTreeClassifier(max_depth=5, min_samples_split=50, min_samples_leaf=20, random_state=42)
dt_model.fit(X_train, y_train)

# Ewaluacja
y_pred = dt_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

# Wa≈ºno≈õƒá features
feature_names = ['wiek', 'doch√≥d', 'historia_kred.', 'zatrudnienie', 'oszczƒôdno≈õci', 'liczba_dzieci']
importance = dt_model.feature_importances_
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'importance': importance
}).sort_values('importance', ascending=False)

# Przygotuj wykres drzewa
plt.figure(figsize=(20, 10))
tree.plot_tree(dt_model, 
               feature_names=feature_names,
               class_names=['Nie', 'Tak'],
               filled=True,
               rounded=True,
               fontsize=10)
plt.title("Drzewo decyzyjne - Przyznanie kredytu")
plt.close()

print(f"Wyniki modelu Decision Tree:")
print(f"G≈Çƒôboko≈õƒá drzewa: {dt_model.tree_.max_depth}")
print(f"Liczba li≈õci: {dt_model.tree_.n_leaves}")
print(f"Dok≈Çadno≈õƒá modelu: {accuracy:.1%}")

print(f"\nStatystyki danych:")
print(f"Dane treningowe: {len(X_train)} klient√≥w")
print(f"Dane testowe: {len(X_test)} klient√≥w")
print(f"Odsetek przyznanych kredyt√≥w: {data['kredyt_przyznany'].mean():.1%}")

print("\nConfusion Matrix:")
print("Przewidywane:    Nie    Tak")
print(f"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}")
print(f"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}")

print("\nWa≈ºno≈õƒá cech:")
for _, row in feature_importance.iterrows():
    print(f"{row['feature']}: {row['importance']:.3f}")

# Odtw√≥rz wykres drzewa
plt.figure(figsize=(20, 10))
tree.plot_tree(dt_model, 
               feature_names=feature_names,
               class_names=['Nie', 'Tak'],
               filled=True,
               rounded=True,
               fontsize=10)
plt.title("Drzewo decyzyjne - Przyznanie kredytu")
plt.show()
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki i drzewo decyzyjne

```{python}
#| echo: false
#| fig-cap: "Wizualizacja drzewa decyzyjnego dla kredyt√≥w"
#| fig-width: 20
#| fig-height: 10

print(f"Wyniki modelu Decision Tree:")
print(f"G≈Çƒôboko≈õƒá drzewa: {dt_model.tree_.max_depth}")
print(f"Liczba li≈õci: {dt_model.tree_.n_leaves}")
print(f"Dok≈Çadno≈õƒá modelu: {accuracy:.1%}")

print(f"\nStatystyki danych:")
print(f"Dane treningowe: {len(X_train)} klient√≥w")
print(f"Dane testowe: {len(X_test)} klient√≥w")
print(f"Odsetek przyznanych kredyt√≥w: {data['kredyt_przyznany'].mean():.1%}")

print("\nConfusion Matrix:")
print("Przewidywane:    Nie    Tak")
print(f"Rzeczywiste Nie: {cm[0,0]:3d}   {cm[0,1]:3d}")
print(f"Rzeczywiste Tak: {cm[1,0]:3d}   {cm[1,1]:3d}")

print("\nWa≈ºno≈õƒá cech:")
for _, row in feature_importance.iterrows():
    print(f"{row['feature']}: {row['importance']:.3f}")

# Odtw√≥rz wykres drzewa
plt.figure(figsize=(20, 10))
tree.plot_tree(dt_model, 
               feature_names=feature_names,
               class_names=['Nie', 'Tak'],
               filled=True,
               rounded=True,
               fontsize=10)
plt.title("Drzewo decyzyjne - Przyznanie kredytu")
plt.show()
```
:::

---

## üîç Interpretacja decyzji dla konkretnego klienta

```{python}
#| echo: true
#| output: false

# Funkcja do interpretacji ≈õcie≈ºki decyzyjnej
def explain_decision(model, X_sample, feature_names):
    # Pobierz ≈õcie≈ºkƒô w drzewie
    leaf_id = model.decision_path(X_sample.reshape(1, -1)).toarray()[0]
    feature = model.tree_.feature
    threshold = model.tree_.threshold
    
    print("≈öcie≈ºka decyzyjna:")
    for node_id in range(len(leaf_id)):
        if leaf_id[node_id] == 1:  # je≈õli wƒôze≈Ç jest na ≈õcie≈ºce
            if feature[node_id] != -2:  # je≈õli nie jest li≈õciem
                feature_name = feature_names[feature[node_id]]
                threshold_val = threshold[node_id]
                feature_val = X_sample[feature[node_id]]
                
                if feature_val <= threshold_val:
                    condition = "<="
                else:
                    condition = ">"
                    
                print(f"  {feature_name} ({feature_val:.0f}) {condition} {threshold_val:.0f}")

# Przyk≈Çad dla konkretnego klienta
sample_client = X_test.iloc[0]
prediction = dt_model.predict([sample_client])[0]
probability = dt_model.predict_proba([sample_client])[0]

print(f"Klient testowy:")
print(f"Wiek: {sample_client['wiek']}")
print(f"Doch√≥d: {sample_client['dochod']:.0f}")
print(f"Oszczƒôdno≈õci: {sample_client['oszczednosci']:.0f}")
print(f"\nDecyzja: {'KREDYT PRZYZNANY' if prediction else 'KREDYT ODRZUCONY'}")
print(f"Prawdopodobie≈Ñstwo: {probability[1]:.1%}")

explain_decision(dt_model, sample_client.values, feature_names)
```

::: {.callout-note collapse="true"}
## Poka≈º dane klienta testowego

```{python}
#| echo: false

print(f"Klient testowy:")
print(f"Wiek: {sample_client['wiek']}")
print(f"Doch√≥d: {sample_client['dochod']:.0f}")
print(f"Oszczƒôdno≈õci: {sample_client['oszczednosci']:.0f}")
print(f"\nDecyzja: {'KREDYT PRZYZNANY' if prediction else 'KREDYT ODRZUCONY'}")
print(f"Prawdopodobie≈Ñstwo: {probability[1]:.1%}")
```
:::

---

## üéØ R√≥≈ºne zastosowania Decision Trees

### 1) **Medyczna diagnoza**
```{python}
# Przyk≈Çad klasyfikacji ryzyka chor√≥b serca
medical_features = ['wiek', 'cholesterol', 'ci≈õnienie', 'BMI', 'pali_papierosy']
# Target: 'ryzyko_chor√≥b_serca' (wysokie/niskie)

medical_tree = DecisionTreeClassifier(max_depth=4)
# Model automatycznie znajdzie progi: "Je≈õli cholesterol > 240 I BMI > 30 ORAZ wiek > 50..."
```

### 2) **Marketing - segmentacja klient√≥w**
```{python}
# Przewidywanie czy klient kupi produkt premium
marketing_features = ['doch√≥d_roczny', 'wiek', 'wykszta≈Çcenie', 'poprzednie_zakupy']
# Target: 'kupi_premium' (tak/nie)

marketing_tree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=100)
# Wynik: jasne regu≈Çy marketingowe do targetowania reklam
```

### 3) **HR - decyzje o zatrudnieniu**
```{python}
# Przewidywanie sukcesu kandydata w rekrutacji
hr_features = ['do≈õwiadczenie_lat', 'wykszta≈Çcenie', 'wynik_test√≥w', 'referencje']
# Target: 'zatrudniony' (tak/nie)

hr_tree = DecisionTreeClassifier(max_depth=5)
# Wynik: automatyczne zasady rekrutacyjne
```

---

## ‚öôÔ∏è Tuning parametr√≥w

```{python}
#| echo: true
#| output: false

from sklearn.model_selection import GridSearchCV

# Najwa≈ºniejsze parametry do tuningu
param_grid = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [20, 50, 100],
    'min_samples_leaf': [10, 20, 50],
    'criterion': ['gini', 'entropy']
}

# Grid search z cross-validation
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print("Najlepsze parametry:")
print(grid_search.best_params_)
print(f"Najlepsza dok≈Çadno≈õƒá CV: {grid_search.best_score_:.1%}")

# Model z najlepszymi parametrami
best_model = grid_search.best_estimator_
best_accuracy = accuracy_score(y_test, best_model.predict(X_test))
print(f"Dok≈Çadno≈õƒá na test set: {best_accuracy:.1%}")
```

::: {.callout-note collapse="true"}
## Poka≈º najlepsze parametry

```{python}
#| echo: false

print("Najlepsze parametry:")
print(grid_search.best_params_)
print(f"Najlepsza dok≈Çadno≈õƒá CV: {grid_search.best_score_:.1%}")

# Model z najlepszymi parametrami
best_model = grid_search.best_estimator_
best_accuracy = accuracy_score(y_test, best_model.predict(X_test))
print(f"Dok≈Çadno≈õƒá na test set: {best_accuracy:.1%}")
```
:::

---

## ‚ö†Ô∏è Problemy i rozwiƒÖzania

### 1) **Overfitting - drzewo za g≈Çƒôbokie**
```{python}
#| echo: true
#| output: false

# Problem: drzewo "pamiƒôta" dane treningowe
overfitted_tree = DecisionTreeClassifier()  # bez ogranicze≈Ñ!
overfitted_tree.fit(X_train, y_train)

train_acc = accuracy_score(y_train, overfitted_tree.predict(X_train))
test_acc = accuracy_score(y_test, overfitted_tree.predict(X_test))

print(f"Overfitted model:")
print(f"Train accuracy: {train_acc:.1%}")
print(f"Test accuracy: {test_acc:.1%}")
print(f"R√≥≈ºnica: {train_acc - test_acc:.1%} - to overfitting!")

# RozwiƒÖzanie: ograniczenia
pruned_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=20)
pruned_tree.fit(X_train, y_train)

train_acc_pruned = accuracy_score(y_train, pruned_tree.predict(X_train))
test_acc_pruned = accuracy_score(y_test, pruned_tree.predict(X_test))

print(f"\nPruned model:")
print(f"Train accuracy: {train_acc_pruned:.1%}")
print(f"Test accuracy: {test_acc_pruned:.1%}")
print(f"R√≥≈ºnica: {train_acc_pruned - test_acc_pruned:.1%} - znacznie lepiej!")
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki

```{python}
#| echo: false

print(f"Overfitted model:")
print(f"Train accuracy: {train_acc:.1%}")
print(f"Test accuracy: {test_acc:.1%}")
print(f"R√≥≈ºnica: {train_acc - test_acc:.1%} - to overfitting!")

print(f"\nPruned model:")
print(f"Train accuracy: {train_acc_pruned:.1%}")
print(f"Test accuracy: {test_acc_pruned:.1%}")
print(f"R√≥≈ºnica: {train_acc_pruned - test_acc_pruned:.1%} - znacznie lepiej!")
```
:::

### 2) **Brak stabilno≈õci - ma≈Çe zmiany = r√≥≈ºne drzewa**
```{python}
#| echo: true
#| output: false

# Problem demonstracji
results = []
for i in range(10):
    # R√≥≈ºne random_state = r√≥≈ºne drzewa
    tree_test = DecisionTreeClassifier(random_state=i, max_depth=5)
    tree_test.fit(X_train, y_train)
    acc = accuracy_score(y_test, tree_test.predict(X_test))
    results.append(acc)

print(f"Dok≈Çadno≈õƒá r√≥≈ºnych drzew: {min(results):.1%} - {max(results):.1%}")
print(f"Rozrzut: {max(results) - min(results):.1%}")
print("RozwiƒÖzanie: Random Forest (nastƒôpna ≈õciƒÖgawka!)")
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki

```{python}
#| echo: false

print(f"Dok≈Çadno≈õƒá r√≥≈ºnych drzew: {min(results):.1%} - {max(results):.1%}")
print(f"Rozrzut: {max(results) - min(results):.1%}")
print("RozwiƒÖzanie: Random Forest (nastƒôpna ≈õciƒÖgawka!)")
```
:::

---

## üåç Real-world przypadki u≈ºycia

1. **Bankowo≈õƒá:** Ocena ryzyka kredytowego, wykrywanie fraud√≥w
2. **Medycyna:** Systemy wspomagania diagnostyki, triage pacjent√≥w  
3. **E-commerce:** Rekomendacje produkt√≥w, ustalanie cen dynamicznych
4. **HR:** Automatyzacja proces√≥w rekrutacyjnych
5. **Marketing:** Segmentacja klient√≥w, personalizacja oferowania

::: {.callout-tip}
## üí° Kiedy u≈ºywaƒá Decision Trees?

**‚úÖ U≈ªYJ GDY:**

- Potrzebujesz interpretowalnego modelu
- Dane majƒÖ kategoryczne zmienne  
- Chcesz zrozumieƒá "dlaczego" model podjƒÖ≈Ç decyzjƒô
- Masz nieliniowe zale≈ºno≈õci w danych
- Brak√≥w w danych nie trzeba impute'owaƒá

**‚ùå NIE U≈ªYWAJ GDY:**

- Potrzebujesz najwy≈ºszej dok≈Çadno≈õci (u≈ºyj Random Forest/XGBoost)
- Masz bardzo g≈Çƒôbokie wzorce w danych (u≈ºyj Neural Networks)
- Dane sƒÖ bardzo ha≈Ça≈õliwe
- Przewidujesz warto≈õci ciƒÖg≈Çe (lepiej Linear Regression)
:::

**Nastƒôpna ≈õciƒÖgawka:** Random Forest - zesp√≥≈Ç drzew (wkr√≥tce)! üå≤üå≤üå≤
