---
title: "K-Means ‚Äî grupowanie klient√≥w bez etykiet"
format:
  html:
    code-tools: true
---

## üéØ Czym jest K-Means?

**K-Means** to algorytm **uczenia nienadzorowanego**, kt√≥ry automatycznie **grupuje podobne dane** w klastry. Nie potrzebujesz etykiet - algorytm sam znajduje wzorce i dzieli dane na K grup.

::: {.callout-note}
## üí° Intuicja
Wyobra≈∫ sobie imprezƒô, gdzie ludzie sami grupujƒÖ siƒô w krƒôgi na podstawie wsp√≥lnych zainteresowa≈Ñ. K-Means dzia≈Ça podobnie - znajduje "centra grup" i przydziela ka≈ºdy punkt do najbli≈ºszego centrum.
:::

---

## üõçÔ∏è Praktyczny przyk≈Çad: segmentacja klient√≥w sklepu

Jak pogrupowaƒá klient√≥w do targetowanych kampanii marketingowych?

```{python}
#| echo: true
#| output: false

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

# Tworzenie realistycznych danych klient√≥w e-commerce
np.random.seed(42)
n_customers = 1000

# Generujemy r√≥≈ºne typy klient√≥w z wyra≈∫nymi wzorcami
customer_types = np.random.choice(['premium', 'average', 'bargain', 'sporadic'], n_customers, 
                                 p=[0.15, 0.35, 0.35, 0.15])

data = pd.DataFrame({
    'wiek': np.random.randint(18, 70, n_customers),
    'roczny_dochod': np.random.normal(50000, 20000, n_customers),
    'wydatki_roczne': np.random.normal(25000, 15000, n_customers),
    'liczba_zakupow': np.random.randint(1, 50, n_customers),
    'sredni_koszt_zakupu': np.random.normal(200, 100, n_customers),
    'lata_jako_klient': np.random.randint(0, 10, n_customers),
    'typ_klienta': customer_types
})

# Realistyczne korelacje miƒôdzy zmiennymi
for i, typ in enumerate(data['typ_klienta']):
    if typ == 'premium':
        data.loc[i, 'roczny_dochod'] = np.random.normal(80000, 15000)
        data.loc[i, 'wydatki_roczne'] = np.random.normal(45000, 10000)
        data.loc[i, 'liczba_zakupow'] = np.random.randint(20, 50)
        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(400, 100)
    elif typ == 'bargain':
        data.loc[i, 'roczny_dochod'] = np.random.normal(30000, 10000)
        data.loc[i, 'wydatki_roczne'] = np.random.normal(8000, 3000)
        data.loc[i, 'liczba_zakupow'] = np.random.randint(15, 40)
        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(80, 30)
    elif typ == 'sporadic':
        data.loc[i, 'roczny_dochod'] = np.random.normal(45000, 15000)
        data.loc[i, 'wydatki_roczne'] = np.random.normal(5000, 2000)
        data.loc[i, 'liczba_zakupow'] = np.random.randint(1, 8)
        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(300, 150)

# Czy≈õƒá dane - usu≈Ñ warto≈õci ujemne
data['roczny_dochod'] = np.maximum(data['roczny_dochod'], 15000)
data['wydatki_roczne'] = np.maximum(data['wydatki_roczne'], 1000)
data['sredni_koszt_zakupu'] = np.maximum(data['sredni_koszt_zakupu'], 20)

print("Statystyki klient√≥w:")
print(data[['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', 'sredni_koszt_zakupu']].describe())
print(f"\nRozk≈Çad typ√≥w klient√≥w:")
print(data['typ_klienta'].value_counts())
```

::: {.callout-note collapse="true"}
## Poka≈º statystyki klient√≥w

```{python}
#| echo: false

print("Statystyki klient√≥w:")
print(data[['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', 'sredni_koszt_zakupu']].describe())
print(f"\nRozk≈Çad typ√≥w klient√≥w:")
print(data['typ_klienta'].value_counts())
```
:::

---

## üîß Budowanie modelu krok po kroku

### 1) Przygotowanie danych

```{python}
#| echo: true
#| output: false

# Wybierz features do clusteringu (bez typ_klienta - to chcemy odkryƒá!)
features = ['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', 
           'sredni_koszt_zakupu', 'lata_jako_klient']
X = data[features]

# Standaryzacja - BARDZO WA≈ªNE w K-Means!
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Przed standaryzacjƒÖ:")
print(X.describe())

print("\nPo standaryzacji (pierwsze 5 wierszy):")
X_scaled_df = pd.DataFrame(X_scaled, columns=features)
print(X_scaled_df.head())
print("\n≈örednie po standaryzacji:", X_scaled_df.mean().round(3))
print("Odchylenia po standaryzacji:", X_scaled_df.std().round(3))
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki standaryzacji

```{python}
#| echo: false

print("Przed standaryzacjƒÖ:")
print(X.describe())

print("\nPo standaryzacji (pierwsze 5 wierszy):")
X_scaled_df = pd.DataFrame(X_scaled, columns=features)
print(X_scaled_df.head())
print("\n≈örednie po standaryzacji:", X_scaled_df.mean().round(3))
print("Odchylenia po standaryzacji:", X_scaled_df.std().round(3))
```
:::

### 2) Znajdowanie optymalnej liczby klastr√≥w

```{python}
#| echo: true
#| output: false

# Metoda ≈Çokcia (Elbow Method)
inertias = []
silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))

# Znajd≈∫ optymalne K
best_k = K_range[np.argmax(silhouette_scores)]
print(f"Optymalna liczba klastr√≥w (Silhouette): {best_k}")

print("\nWyniki dla r√≥≈ºnych K:")
for k, inertia, sil_score in zip(K_range, inertias, silhouette_scores):
    print(f"K={k}: Inertia={inertia:.0f}, Silhouette={sil_score:.3f}")
```

::: {.callout-note collapse="true"}
## Poka≈º optymalne K

```{python}
#| echo: false

print(f"Optymalna liczba klastr√≥w (Silhouette): {best_k}")

print("\nWyniki dla r√≥≈ºnych K:")
for k, inertia, sil_score in zip(K_range, inertias, silhouette_scores):
    print(f"K={k}: Inertia={inertia:.0f}, Silhouette={sil_score:.3f}")
```
:::

### 3) Trenowanie finalnego modelu

```{python}
#| echo: true
#| output: false

# Trenuj model z optymalnym K
final_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
cluster_labels = final_kmeans.fit_predict(X_scaled)

# Dodaj etykiety klastr√≥w do danych
data['klaster'] = cluster_labels

print(f"Model K-Means wytrenowany z K={best_k}")
print(f"Silhouette Score: {silhouette_score(X_scaled, cluster_labels):.3f}")

# Analiza klastr√≥w
print("\nRozmiary klastr√≥w:")
cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()
for i, count in enumerate(cluster_counts):
    print(f"Klaster {i}: {count} klient√≥w ({count/len(data)*100:.1f}%)")
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki finalnego modelu

```{python}
#| echo: false

print(f"Model K-Means wytrenowany z K={best_k}")
print(f"Silhouette Score: {silhouette_score(X_scaled, cluster_labels):.3f}")

print("\nRozmiary klastr√≥w:")
cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()
for i, count in enumerate(cluster_counts):
    print(f"Klaster {i}: {count} klient√≥w ({count/len(data)*100:.1f}%)")
```
:::

---

## üìä Analiza i interpretacja klastr√≥w

```{python}
#| echo: true
#| output: false

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

# Przygotuj dane (powtarzamy dla kompletno≈õci)
np.random.seed(42)
n_customers = 1000

customer_types = np.random.choice(['premium', 'average', 'bargain', 'sporadic'], n_customers, 
                                 p=[0.15, 0.35, 0.35, 0.15])

data = pd.DataFrame({
    'wiek': np.random.randint(18, 70, n_customers),
    'roczny_dochod': np.random.normal(50000, 20000, n_customers),
    'wydatki_roczne': np.random.normal(25000, 15000, n_customers),
    'liczba_zakupow': np.random.randint(1, 50, n_customers),
    'sredni_koszt_zakupu': np.random.normal(200, 100, n_customers),
    'lata_jako_klient': np.random.randint(0, 10, n_customers),
    'typ_klienta': customer_types
})

# Popraw dane wed≈Çug typ√≥w
for i, typ in enumerate(data['typ_klienta']):
    if typ == 'premium':
        data.loc[i, 'roczny_dochod'] = np.random.normal(80000, 15000)
        data.loc[i, 'wydatki_roczne'] = np.random.normal(45000, 10000)
        data.loc[i, 'liczba_zakupow'] = np.random.randint(20, 50)
        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(400, 100)
    elif typ == 'bargain':
        data.loc[i, 'roczny_dochod'] = np.random.normal(30000, 10000)
        data.loc[i, 'wydatki_roczne'] = np.random.normal(8000, 3000)
        data.loc[i, 'liczba_zakupow'] = np.random.randint(15, 40)
        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(80, 30)
    elif typ == 'sporadic':
        data.loc[i, 'roczny_dochod'] = np.random.normal(45000, 15000)
        data.loc[i, 'wydatki_roczne'] = np.random.normal(5000, 2000)
        data.loc[i, 'liczba_zakupow'] = np.random.randint(1, 8)
        data.loc[i, 'sredni_koszt_zakupu'] = np.random.normal(300, 150)

data['roczny_dochod'] = np.maximum(data['roczny_dochod'], 15000)
data['wydatki_roczne'] = np.maximum(data['wydatki_roczne'], 1000)
data['sredni_koszt_zakupu'] = np.maximum(data['sredni_koszt_zakupu'], 20)

# Przygotuj model
features = ['wiek', 'roczny_dochod', 'wydatki_roczne', 'liczba_zakupow', 
           'sredni_koszt_zakupu', 'lata_jako_klient']
X = data[features]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Znajd≈∫ optymalne K
silhouette_scores = []
K_range = range(2, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))

best_k = K_range[np.argmax(silhouette_scores)]

# Trenuj finalny model
final_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
cluster_labels = final_kmeans.fit_predict(X_scaled)
data['klaster'] = cluster_labels

# Analiza charakterystyk klastr√≥w
cluster_analysis = data.groupby('klaster')[features].mean()

print("CHARAKTERYSTYKI KLASTR√ìW:")
print("=" * 50)

for cluster_id in range(best_k):
    print(f"\nKLASTER {cluster_id}:")
    cluster_data = data[data['klaster'] == cluster_id]
    
    print(f"Liczba klient√≥w: {len(cluster_data)} ({len(cluster_data)/len(data)*100:.1f}%)")
    print(f"≈öredni wiek: {cluster_data['wiek'].mean():.0f} lat")
    print(f"≈öredni doch√≥d: {cluster_data['roczny_dochod'].mean():.0f} z≈Ç")
    print(f"≈örednie wydatki: {cluster_data['wydatki_roczne'].mean():.0f} z≈Ç")
    print(f"≈örednia liczba zakup√≥w: {cluster_data['liczba_zakupow'].mean():.0f}")
    print(f"≈öredni koszt zakupu: {cluster_data['sredni_koszt_zakupu'].mean():.0f} z≈Ç")
    
    # Nadaj nazwƒô klastrowi na podstawie charakterystyk
    avg_income = cluster_data['roczny_dochod'].mean()
    avg_spending = cluster_data['wydatki_roczne'].mean()
    avg_frequency = cluster_data['liczba_zakupow'].mean()
    
    if avg_income > 60000 and avg_spending > 30000:
        cluster_name = "üåü PREMIUM CUSTOMERS"
    elif avg_frequency < 10 and avg_spending < 10000:
        cluster_name = "üò¥ SPORADYCZNI KLIENCI"
    elif avg_spending / avg_income < 0.3 and avg_frequency > 20:
        cluster_name = "üí∞ BARGAIN HUNTERS"
    else:
        cluster_name = "üîÑ ≈öREDNI KLIENCI"
    
    print(f"Typ: {cluster_name}")

# Przygotuj wizualizacjƒô
plt.figure(figsize=(12, 8))

# Wykres 1: Doch√≥d vs Wydatki
plt.subplot(2, 2, 1)
scatter = plt.scatter(data['roczny_dochod'], data['wydatki_roczne'], 
                     c=data['klaster'], cmap='viridis', alpha=0.6)
plt.xlabel('Roczny doch√≥d')
plt.ylabel('Wydatki roczne')
plt.title('Doch√≥d vs Wydatki')
plt.colorbar(scatter)

# Wykres 2: Wiek vs Liczba zakup√≥w
plt.subplot(2, 2, 2)
scatter2 = plt.scatter(data['wiek'], data['liczba_zakupow'], 
                      c=data['klaster'], cmap='viridis', alpha=0.6)
plt.xlabel('Wiek')
plt.ylabel('Liczba zakup√≥w')
plt.title('Wiek vs Liczba zakup√≥w')
plt.colorbar(scatter2)

# Wykres 3: ≈öredni koszt vs Liczba zakup√≥w
plt.subplot(2, 2, 3)
scatter3 = plt.scatter(data['sredni_koszt_zakupu'], data['liczba_zakupow'], 
                      c=data['klaster'], cmap='viridis', alpha=0.6)
plt.xlabel('≈öredni koszt zakupu')
plt.ylabel('Liczba zakup√≥w')
plt.title('Koszt vs Czƒôstotliwo≈õƒá')
plt.colorbar(scatter3)

# Wykres 4: Rozk≈Çad klastr√≥w
plt.subplot(2, 2, 4)
cluster_counts = data['klaster'].value_counts().sort_index()
plt.bar(range(len(cluster_counts)), cluster_counts.values, 
        color=plt.cm.viridis(np.linspace(0, 1, len(cluster_counts))))
plt.xlabel('Klaster')
plt.ylabel('Liczba klient√≥w')
plt.title('Rozmiary klastr√≥w')
plt.xticks(range(len(cluster_counts)))

plt.tight_layout()
plt.close()

# Por√≥wnanie z rzeczywistymi typami klient√≥w
if 'typ_klienta' in data.columns:
    print("\n" + "="*50)
    print("POR√ìWNANIE Z RZECZYWISTYMI TYPAMI:")
    comparison = pd.crosstab(data['klaster'], data['typ_klienta'])
    print(comparison)

# Odtw√≥rz wykresy
plt.figure(figsize=(12, 8))

plt.subplot(2, 2, 1)
scatter = plt.scatter(data['roczny_dochod'], data['wydatki_roczne'], 
                     c=data['klaster'], cmap='viridis', alpha=0.6)
plt.xlabel('Roczny doch√≥d')
plt.ylabel('Wydatki roczne')
plt.title('Doch√≥d vs Wydatki')
plt.colorbar(scatter)

plt.subplot(2, 2, 2)
scatter2 = plt.scatter(data['wiek'], data['liczba_zakupow'], 
                      c=data['klaster'], cmap='viridis', alpha=0.6)
plt.xlabel('Wiek')
plt.ylabel('Liczba zakup√≥w')
plt.title('Wiek vs Liczba zakup√≥w')
plt.colorbar(scatter2)

plt.subplot(2, 2, 3)
scatter3 = plt.scatter(data['sredni_koszt_zakupu'], data['liczba_zakupow'], 
                      c=data['klaster'], cmap='viridis', alpha=0.6)
plt.xlabel('≈öredni koszt zakupu')
plt.ylabel('Liczba zakup√≥w')
plt.title('Koszt vs Czƒôstotliwo≈õƒá')
plt.colorbar(scatter3)

plt.subplot(2, 2, 4)
cluster_counts = data['klaster'].value_counts().sort_index()
plt.bar(range(len(cluster_counts)), cluster_counts.values, 
        color=plt.cm.viridis(np.linspace(0, 1, len(cluster_counts))))
plt.xlabel('Klaster')
plt.ylabel('Liczba klient√≥w')
plt.title('Rozmiary klastr√≥w')
plt.xticks(range(len(cluster_counts)))

plt.tight_layout()
plt.show()
```

::: {.callout-note collapse="true"}
## Poka≈º analizƒô klastr√≥w i wizualizacje

```{python}
#| echo: false
#| fig-cap: "Analiza klastr√≥w klient√≥w"
#| fig-width: 12
#| fig-height: 8

print("CHARAKTERYSTYKI KLASTR√ìW:")
print("=" * 50)

for cluster_id in range(best_k):
    print(f"\nKLASTER {cluster_id}:")
    cluster_data = data[data['klaster'] == cluster_id]
    
    print(f"Liczba klient√≥w: {len(cluster_data)} ({len(cluster_data)/len(data)*100:.1f}%)")
    print(f"≈öredni wiek: {cluster_data['wiek'].mean():.0f} lat")
    print(f"≈öredni doch√≥d: {cluster_data['roczny_dochod'].mean():.0f} z≈Ç")
    print(f"≈örednie wydatki: {cluster_data['wydatki_roczne'].mean():.0f} z≈Ç")
    print(f"≈örednia liczba zakup√≥w: {cluster_data['liczba_zakupow'].mean():.0f}")
    print(f"≈öredni koszt zakupu: {cluster_data['sredni_koszt_zakupu'].mean():.0f} z≈Ç")
    
    # Nadaj nazwƒô klastrowi na podstawie charakterystyk
    avg_income = cluster_data['roczny_dochod'].mean()
    avg_spending = cluster_data['wydatki_roczne'].mean()
    avg_frequency = cluster_data['liczba_zakupow'].mean()
    
    if avg_income > 60000 and avg_spending > 30000:
        cluster_name = "üåü PREMIUM CUSTOMERS"
    elif avg_frequency < 10 and avg_spending < 10000:
        cluster_name = "üò¥ SPORADYCZNI KLIENCI"
    elif avg_spending / avg_income < 0.3 and avg_frequency > 20:
        cluster_name = "üí∞ BARGAIN HUNTERS"
    else:
        cluster_name = "üîÑ ≈öREDNI KLIENCI"
    
    print(f"Typ: {cluster_name}")

if 'typ_klienta' in data.columns:
    print("\n" + "="*50)
    print("POR√ìWNANIE Z RZECZYWISTYMI TYPAMI:")
    comparison = pd.crosstab(data['klaster'], data['typ_klienta'])
    print(comparison)

# Odtw√≥rz wykresy
plt.figure(figsize=(12, 8))

plt.subplot(2, 2, 1)
scatter = plt.scatter(data['roczny_dochod'], data['wydatki_roczne'], 
                     c=data['klaster'], cmap='viridis', alpha=0.6)
plt.xlabel('Roczny doch√≥d')
plt.ylabel('Wydatki roczne')
plt.title('Doch√≥d vs Wydatki')
plt.colorbar(scatter)

plt.subplot(2, 2, 2)
scatter2 = plt.scatter(data['wiek'], data['liczba_zakupow'], 
                      c=data['klaster'], cmap='viridis', alpha=0.6)
plt.xlabel('Wiek')
plt.ylabel('Liczba zakup√≥w')
plt.title('Wiek vs Liczba zakup√≥w')
plt.colorbar(scatter2)

plt.subplot(2, 2, 3)
scatter3 = plt.scatter(data['sredni_koszt_zakupu'], data['liczba_zakupow'], 
                      c=data['klaster'], cmap='viridis', alpha=0.6)
plt.xlabel('≈öredni koszt zakupu')
plt.ylabel('Liczba zakup√≥w')
plt.title('Koszt vs Czƒôstotliwo≈õƒá')
plt.colorbar(scatter3)

plt.subplot(2, 2, 4)
cluster_counts = data['klaster'].value_counts().sort_index()
plt.bar(range(len(cluster_counts)), cluster_counts.values, 
        color=plt.cm.viridis(np.linspace(0, 1, len(cluster_counts))))
plt.xlabel('Klaster')
plt.ylabel('Liczba klient√≥w')
plt.title('Rozmiary klastr√≥w')
plt.xticks(range(len(cluster_counts)))

plt.tight_layout()
plt.show()
```
:::

---

## üéØ Praktyczne zastosowania K-Means

### 1) **Marketing - personalizacja kampanii**
```{python}
#| echo: true
#| output: false

# Strategia marketingowa na podstawie klastr√≥w
def marketing_strategy(cluster_id, cluster_data):
    avg_income = cluster_data['roczny_dochod'].mean()
    avg_spending = cluster_data['wydatki_roczne'].mean()
    avg_frequency = cluster_data['liczba_zakupow'].mean()
    
    if avg_income > 60000 and avg_spending > 30000:
        return {
            'segment': 'Premium Customers',
            'strategy': 'Produkty luksusowe, VIP program, personal shopping',
            'budget_allocation': '40%',
            'channels': 'Email premium, personal calls, exclusive events'
        }
    elif avg_frequency < 10 and avg_spending < 10000:
        return {
            'segment': 'Sporadic Customers', 
            'strategy': 'Reaktywacja, oferty specjalne, przypomnienia',
            'budget_allocation': '15%',
            'channels': 'SMS, push notifications, retargeting ads'
        }
    elif avg_spending / avg_income < 0.3 and avg_frequency > 20:
        return {
            'segment': 'Bargain Hunters',
            'strategy': 'Promocje, wyprzeda≈ºe, programy lojalno≈õciowe',
            'budget_allocation': '25%', 
            'channels': 'Newsletter z promocjami, social media deals'
        }
    else:
        return {
            'segment': 'Average Customers',
            'strategy': 'Standardowe produkty, cross-selling, up-selling',
            'budget_allocation': '20%',
            'channels': 'Email marketing, social media, display ads'
        }

print("STRATEGIA MARKETINGOWA DLA KA≈ªDEGO KLASTRA:")
print("=" * 60)

for cluster_id in range(best_k):
    cluster_data = data[data['klaster'] == cluster_id]
    strategy = marketing_strategy(cluster_id, cluster_data)
    
    print(f"\nKLASTER {cluster_id}: {strategy['segment']}")
    print(f"Strategia: {strategy['strategy']}")
    print(f"Bud≈ºet: {strategy['budget_allocation']}")
    print(f"Kana≈Çy: {strategy['channels']}")
```

::: {.callout-note collapse="true"}
## Poka≈º strategiƒô marketingowƒÖ

```{python}
#| echo: false

print("STRATEGIA MARKETINGOWA DLA KA≈ªDEGO KLASTRA:")
print("=" * 60)

for cluster_id in range(best_k):
    cluster_data = data[data['klaster'] == cluster_id]
    strategy = marketing_strategy(cluster_id, cluster_data)
    
    print(f"\nKLASTER {cluster_id}: {strategy['segment']}")
    print(f"Strategia: {strategy['strategy']}")
    print(f"Bud≈ºet: {strategy['budget_allocation']}")
    print(f"Kana≈Çy: {strategy['channels']}")
```
:::

### 2) **Inne bran≈ºe**

```{python}
# Healthcare - grupowanie pacjent√≥w
healthcare_features = ['wiek', 'BMI', 'ci≈õnienie', 'cholesterol', 'aktywno≈õƒá_fizyczna']
# Wynik: programy profilaktyczne dostosowane do grup ryzyka

# Finanse - portfolio management  
finance_features = ['doch√≥d', 'tolerancja_ryzyka', 'horyzont_inwestycji', 'do≈õwiadczenie']
# Wynik: personalizowane porady inwestycyjne

# Retail - optymalizacja sklep√≥w
retail_features = ['lokalizacja', 'demografia', 'konkurencja', 'ruch_pieszy']  
# Wynik: optymalne rozmieszczenie produkt√≥w w r√≥≈ºnych lokalizacjach
```

---

## ‚öôÔ∏è Tuning parametr√≥w K-Means

```{python}
#| echo: true
#| output: false

from sklearn.cluster import KMeans

# 1) R√≥≈ºne metody inicjalizacji
init_methods = ['k-means++', 'random']
results = {}

for init_method in init_methods:
    kmeans = KMeans(n_clusters=best_k, init=init_method, n_init=10, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    sil_score = silhouette_score(X_scaled, labels)
    results[init_method] = sil_score

print("Por√≥wnanie metod inicjalizacji:")
for method, score in results.items():
    print(f"{method}: Silhouette = {score:.3f}")

# 2) Wp≈Çyw liczby inicjalizacji
n_init_values = [1, 5, 10, 20]
for n_init in n_init_values:
    kmeans = KMeans(n_clusters=best_k, n_init=n_init, random_state=42)
    start_time = pd.Timestamp.now()
    kmeans.fit(X_scaled)
    end_time = pd.Timestamp.now()
    duration = (end_time - start_time).total_seconds()
    sil_score = silhouette_score(X_scaled, kmeans.labels_)
    print(f"n_init={n_init}: Silhouette={sil_score:.3f}, Czas={duration:.2f}s")

# 3) R√≥≈ºne algorytmy
algorithms = ['lloyd', 'elkan']  # 'auto' wybiera automatycznie
for algorithm in algorithms:
    try:
        kmeans = KMeans(n_clusters=best_k, algorithm=algorithm, random_state=42)
        start_time = pd.Timestamp.now()
        kmeans.fit(X_scaled)
        end_time = pd.Timestamp.now()
        duration = (end_time - start_time).total_seconds()
        print(f"Algorytm {algorithm}: Czas={duration:.2f}s")
    except:
        print(f"Algorytm {algorithm}: Niedostƒôpny w tej wersji")
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki tuningu

```{python}
#| echo: false

print("Por√≥wnanie metod inicjalizacji:")
for method, score in results.items():
    print(f"{method}: Silhouette = {score:.3f}")

n_init_values = [1, 5, 10, 20]
for n_init in n_init_values:
    kmeans = KMeans(n_clusters=best_k, n_init=n_init, random_state=42)
    start_time = pd.Timestamp.now()
    kmeans.fit(X_scaled)
    end_time = pd.Timestamp.now()
    duration = (end_time - start_time).total_seconds()
    sil_score = silhouette_score(X_scaled, kmeans.labels_)
    print(f"n_init={n_init}: Silhouette={sil_score:.3f}, Czas={duration:.2f}s")
```
:::

---

## ‚ö†Ô∏è Pu≈Çapki i rozwiƒÖzania

### 1) **Brak standaryzacji danych**
```{python}
#| echo: true
#| output: false

# Problem: r√≥≈ºne skale zmiennych
print("PROBLEM: Bez standaryzacji")
print("Doch√≥d (tysiƒÖce): 20-80")  
print("Wiek (lata): 18-70")
print("‚Üí K-Means bƒôdzie skupiaƒá siƒô g≈Ç√≥wnie na dochodzie!")

# Demonstracja
kmeans_unscaled = KMeans(n_clusters=3, random_state=42)
labels_unscaled = kmeans_unscaled.fit_predict(X[['roczny_dochod', 'wiek']])

kmeans_scaled = KMeans(n_clusters=3, random_state=42)  
X_subset_scaled = scaler.fit_transform(X[['roczny_dochod', 'wiek']])
labels_scaled = kmeans_scaled.fit_predict(X_subset_scaled)

sil_unscaled = silhouette_score(X[['roczny_dochod', 'wiek']], labels_unscaled)
sil_scaled = silhouette_score(X_subset_scaled, labels_scaled)

print(f"\nWyniki:")
print(f"Bez standaryzacji: Silhouette = {sil_unscaled:.3f}")
print(f"Ze standaryzacjƒÖ: Silhouette = {sil_scaled:.3f}")
print("‚úÖ Standaryzacja ZAWSZE poprawia wyniki!")
```

::: {.callout-note collapse="true"}
## Poka≈º problem standaryzacji

```{python}
#| echo: false

print(f"\nWyniki:")
print(f"Bez standaryzacji: Silhouette = {sil_unscaled:.3f}")
print(f"Ze standaryzacjƒÖ: Silhouette = {sil_scaled:.3f}")
print("‚úÖ Standaryzacja ZAWSZE poprawia wyniki!")
```
:::

### 2) **Outliers - warto≈õci odstajƒÖce**
```{python}
#| echo: true
#| output: false

# Problem: outliers zak≈Ç√≥cajƒÖ centroidy klastr√≥w
from sklearn.preprocessing import RobustScaler

# Dodaj kilka outlier√≥w
data_with_outliers = data.copy()
data_with_outliers.loc[0, 'roczny_dochod'] = 500000  # milioner!
data_with_outliers.loc[1, 'wydatki_roczne'] = 200000  # mega wydatki

X_outliers = data_with_outliers[features]

# Por√≥wnaj r√≥≈ºne skalery
scalers = {
    'StandardScaler': StandardScaler(),
    'RobustScaler': RobustScaler()  # odporny na outliers
}

print("Wp≈Çyw outliers:")
for scaler_name, scaler_obj in scalers.items():
    X_scaled_comp = scaler_obj.fit_transform(X_outliers)
    kmeans = KMeans(n_clusters=3, random_state=42)
    labels = kmeans.fit_predict(X_scaled_comp)
    sil_score = silhouette_score(X_scaled_comp, labels)
    print(f"{scaler_name}: Silhouette = {sil_score:.3f}")

print("\n‚úÖ RobustScaler lepiej radzi sobie z outliers!")
```

::: {.callout-note collapse="true"}
## Poka≈º wp≈Çyw outliers

```{python}
#| echo: false

print("Wp≈Çyw outliers:")
for scaler_name, scaler_obj in scalers.items():
    X_scaled_comp = scaler_obj.fit_transform(X_outliers)
    kmeans = KMeans(n_clusters=3, random_state=42)
    labels = kmeans.fit_predict(X_scaled_comp)
    sil_score = silhouette_score(X_scaled_comp, labels)
    print(f"{scaler_name}: Silhouette = {sil_score:.3f}")

print("\n‚úÖ RobustScaler lepiej radzi sobie z outliers!")
```
:::

### 3) **Curse of dimensionality**
```{python}
#| echo: true
#| output: false

# Problem: za du≈ºo wymiar√≥w
from sklearn.decomposition import PCA

print("Problem wielu wymiar√≥w:")
dimensions = [2, 5, 10, 20, 50]

for n_dims in dimensions:
    if n_dims <= X_scaled.shape[1]:
        X_subset = X_scaled[:, :n_dims]
    else:
        # Dodaj sztuczne wymiary
        extra_dims = np.random.randn(X_scaled.shape[0], n_dims - X_scaled.shape[1])
        X_subset = np.hstack([X_scaled, extra_dims])
    
    kmeans = KMeans(n_clusters=3, random_state=42)
    labels = kmeans.fit_predict(X_subset)
    
    if len(np.unique(labels)) > 1:  # sprawd≈∫ czy sƒÖ r√≥≈ºne klastry
        sil_score = silhouette_score(X_subset, labels)
        print(f"{n_dims} wymiar√≥w: Silhouette = {sil_score:.3f}")
    else:
        print(f"{n_dims} wymiar√≥w: Wszystkie punkty w jednym klastrze!")

print("\nRozwiƒÖzanie: PCA dimensionality reduction")
pca = PCA(n_components=0.95)  # zachowaj 95% wariancji
X_pca = pca.fit_transform(X_scaled)
print(f"Redukcja z {X_scaled.shape[1]} do {X_pca.shape[1]} wymiar√≥w")

kmeans_pca = KMeans(n_clusters=3, random_state=42)
labels_pca = kmeans_pca.fit_predict(X_pca)
sil_pca = silhouette_score(X_pca, labels_pca)
print(f"Po PCA: Silhouette = {sil_pca:.3f}")
```

::: {.callout-note collapse="true"}
## Poka≈º problem wymiarowo≈õci

```{python}
#| echo: false

print("Problem wielu wymiar√≥w:")
dimensions = [2, 5, 10, 20, 50]

for n_dims in dimensions:
    if n_dims <= X_scaled.shape[1]:
        X_subset = X_scaled[:, :n_dims]
    else:
        # Dodaj sztuczne wymiary
        extra_dims = np.random.randn(X_scaled.shape[0], n_dims - X_scaled.shape[1])
        X_subset = np.hstack([X_scaled, extra_dims])
    
    kmeans = KMeans(n_clusters=3, random_state=42)
    labels = kmeans.fit_predict(X_subset)
    
    if len(np.unique(labels)) > 1:  # sprawd≈∫ czy sƒÖ r√≥≈ºne klastry
        sil_score = silhouette_score(X_subset, labels)
        print(f"{n_dims} wymiar√≥w: Silhouette = {sil_score:.3f}")
    else:
        print(f"{n_dims} wymiar√≥w: Wszystkie punkty w jednym klastrze!")

print("\nRozwiƒÖzanie: PCA dimensionality reduction")
print(f"Redukcja z {X_scaled.shape[1]} do {X_pca.shape[1]} wymiar√≥w")
print(f"Po PCA: Silhouette = {sil_pca:.3f}")
```
:::

---

## üåç Real-world przypadki u≈ºycia

1. **E-commerce:** Segmentacja klient√≥w, personalizacja, dynamic pricing
2. **Marketing:** Customer personas, campaign optimization, market research  
3. **Finance:** Portfolio optimization, risk assessment, fraud detection
4. **Healthcare:** Patient stratification, treatment personalization, drug discovery
5. **Operations:** Supply chain optimization, demand forecasting, quality control

::: {.callout-tip}
## üí° Kiedy u≈ºywaƒá K-Means?

**‚úÖ U≈ªYJ GDY:**

- Chcesz odkryƒá ukryte grupy w danych bez etykiet
- Dane majƒÖ podobne gƒôsto≈õci i sƒÖ "okrƒÖg≈Çe" (sferyczne klastry)
- Potrzebujesz szybkiego i skalowalnego algorytmu
- Wiesz w przybli≈ºeniu ile mo≈ºe byƒá grup (K)
- Chcesz segmentowaƒá klient√≥w, produkty, rynki

**‚ùå NIE U≈ªYWAJ GDY:**

- Klastry majƒÖ r√≥≈ºne rozmiary lub gƒôsto≈õci (u≈ºyj DBSCAN)
- Klastry majƒÖ nieregularne kszta≈Çty (u≈ºyj Hierarchical Clustering)
- Nie wiesz wcale ile mo≈ºe byƒá grup (u≈ºyj DBSCAN/HDBSCAN)
- Masz kategoryczne zmienne (u≈ºyj K-Modes)
- Dane majƒÖ du≈ºo outliers (u≈ºyj DBSCAN)
:::

**Nastƒôpna ≈õciƒÖgawka:** Support Vector Machines - znajdowanie optymalnych granic! üéØ
