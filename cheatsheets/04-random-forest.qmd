---
title: "Random Forest ‚Äî las drzew decyzyjnych"
format:
  html:
    code-tools: true
---

## üå≤ Czym jest Random Forest?

**Random Forest** to zesp√≥≈Ç wielu drzew decyzyjnych, kt√≥re **g≈ÇosujƒÖ razem** nad ko≈ÑcowƒÖ decyzjƒÖ. Jeden Decision Tree mo≈ºe siƒô myliƒá, ale gdy masz 100 drzew i wiƒôkszo≈õƒá m√≥wi "TAK" - to prawdopodobnie dobra odpowied≈∫!

::: {.callout-note}
## üí° Intuicja
Wyobra≈∫ sobie konsylium lekarskim: jeden lekarz mo≈ºe siƒô pomyliƒá w diagnozie, ale gdy 7 z 10 ekspert√≥w zgadza siƒô - diagnoza jest znacznie bardziej pewna. Random Forest dzia≈Ça dok≈Çadnie tak samo!
:::

---

## üéØ Praktyczny przyk≈Çad: przewidywanie sukcesu produktu

Czy nowy produkt odniesie sukces na rynku?

```{python}
#| echo: true
#| output: false

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt

# Tworzenie realistycznych danych produkt√≥w
np.random.seed(42)
n_products = 2000

data = pd.DataFrame({
    'cena': np.random.lognormal(6, 1, n_products),  # log-normal dla cen
    'jakosc': np.random.randint(1, 11, n_products),  # 1-10 skala jako≈õci
    'marketing_budget': np.random.exponential(50000, n_products),
    'konkurencja': np.random.randint(1, 21, n_products),  # liczba konkurent√≥w
    'sezon': np.random.choice(['wiosna', 'lato', 'jesien', 'zima'], n_products),
    'kategoria': np.random.choice(['elektronika', 'moda', 'dom', 'sport', 'ksiazki'], n_products),
    'ocena_testowa': np.random.normal(7, 2, n_products),  # oceny focus group
    'innowacyjnosc': np.random.randint(1, 11, n_products),
    'marka_znana': np.random.choice([0, 1], n_products, p=[0.3, 0.7])  # czy znana marka
})

# Realistyczna logika sukcesu produktu
def czy_sukces(row):
    score = 0
    
    # Cena (sweet spot 50-500)
    if 50 <= row['cena'] <= 500:
        score += 2
    elif row['cena'] > 1000:
        score -= 2
    
    # Jako≈õƒá (najwa≈ºniejsze!)
    score += row['jakosc'] * 0.8
    
    # Marketing
    if row['marketing_budget'] > 100000:
        score += 3
    elif row['marketing_budget'] > 50000:
        score += 1
    
    # Konkurencja (mniej = lepiej)
    score -= row['konkurencja'] * 0.2
    
    # Sezon (lato i zima lepsze)
    if row['sezon'] in ['lato', 'zima']:
        score += 1
    
    # Kategoria
    if row['kategoria'] == 'elektronika':
        score += 1
    elif row['kategoria'] == 'ksiazki':
        score -= 1
    
    # Ocena testowa
    score += (row['ocena_testowa'] - 5) * 0.5
    
    # Innowacyjno≈õƒá
    score += row['innowacyjnosc'] * 0.3
    
    # Znana marka
    if row['marka_znana']:
        score += 2
    
    # Ko≈Ñcowa decyzja z odrobinƒÖ losowo≈õci
    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 1)))
    return probability > 0.5

data['sukces'] = data.apply(czy_sukces, axis=1)

print("Statystyki produkt√≥w:")
print(data.describe())
print(f"\nOdsetek udanych produkt√≥w: {data['sukces'].mean():.1%}")
```

::: {.callout-note collapse="true"}
## Poka≈º statystyki produkt√≥w

```{python}
#| echo: false

print("Statystyki produkt√≥w:")
print(data.describe())
print(f"\nOdsetek udanych produkt√≥w: {data['sukces'].mean():.1%}")
```
:::

---

## üîß Budowanie modelu krok po kroku

### 1) Przygotowanie danych

```{python}
#| echo: true
#| output: false

# Encoding kategorycznych zmiennych
from sklearn.preprocessing import LabelEncoder

data_encoded = data.copy()
le_sezon = LabelEncoder()
le_kategoria = LabelEncoder()

data_encoded['sezon_num'] = le_sezon.fit_transform(data['sezon'])
data_encoded['kategoria_num'] = le_kategoria.fit_transform(data['kategoria'])

# Features do modelu
feature_columns = ['cena', 'jakosc', 'marketing_budget', 'konkurencja', 
                  'sezon_num', 'kategoria_num', 'ocena_testowa', 
                  'innowacyjnosc', 'marka_znana']
X = data_encoded[feature_columns]
y = data_encoded['sukces']

# Podzia≈Ç train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Dane treningowe: {len(X_train)} produkt√≥w")
print(f"Dane testowe: {len(X_test)} produkt√≥w")
```

::: {.callout-note collapse="true"}
## Poka≈º podzia≈Ç danych

```{python}
#| echo: false

print(f"Dane treningowe: {len(X_train)} produkt√≥w")
print(f"Dane testowe: {len(X_test)} produkt√≥w")
```
:::

### 2) Trenowanie Random Forest

```{python}
#| echo: true
#| output: false

# Tworzenie modelu Random Forest
rf_model = RandomForestClassifier(
    n_estimators=100,       # liczba drzew w lesie
    max_depth=10,          # maksymalna g≈Çƒôboko≈õƒá ka≈ºdego drzewa
    min_samples_split=20,   # min. pr√≥bek do podzia≈Çu
    min_samples_leaf=10,    # min. pr√≥bek w li≈õciu
    random_state=42,
    n_jobs=-1              # u≈ºyj wszystkie rdzenie CPU
)

# Trenowanie
rf_model.fit(X_train, y_train)

print("Random Forest wytrenowany!")
print(f"Liczba drzew: {rf_model.n_estimators}")
print(f"≈örednia g≈Çƒôboko≈õƒá drzew: {np.mean([tree.tree_.max_depth for tree in rf_model.estimators_]):.1f}")
```

::: {.callout-note collapse="true"}
## Poka≈º parametry wytrenowanego modelu

```{python}
#| echo: false

print("Random Forest wytrenowany!")
print(f"Liczba drzew: {rf_model.n_estimators}")
print(f"≈örednia g≈Çƒôboko≈õƒá drzew: {np.mean([tree.tree_.max_depth for tree in rf_model.estimators_]):.1f}")
```
:::

### 3) Ewaluacja modelu

```{python}
#| echo: true
#| output: false

# Predykcje
y_pred = rf_model.predict(X_test)
y_pred_proba = rf_model.predict_proba(X_test)[:, 1]

# Metryki
accuracy = accuracy_score(y_test, y_pred)
print(f"\nDok≈Çadno≈õƒá Random Forest: {accuracy:.1%}")

# Szczeg√≥≈Çowy raport
print("\nRaport klasyfikacji:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print("Przewidywane:     Pora≈ºka  Sukces")
print(f"Rzeczywiste Pora≈ºka: {cm[0,0]:3d}     {cm[0,1]:3d}")
print(f"Rzeczywiste Sukces:  {cm[1,0]:3d}     {cm[1,1]:3d}")
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki Random Forest

```{python}
#| echo: false

print(f"\nDok≈Çadno≈õƒá Random Forest: {accuracy:.1%}")

cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print("Przewidywane:     Pora≈ºka  Sukces")
print(f"Rzeczywiste Pora≈ºka: {cm[0,0]:3d}     {cm[0,1]:3d}")
print(f"Rzeczywiste Sukces:  {cm[1,0]:3d}     {cm[1,1]:3d}")
```
:::

---

## üìä Analiza wa≈ºno≈õci cech (Feature Importance)

```{python}
#| echo: true
#| output: false

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt

# Przygotuj dane (powtarzamy dla kompletno≈õci)
np.random.seed(42)
n_products = 2000

data = pd.DataFrame({
    'cena': np.random.lognormal(6, 1, n_products),
    'jakosc': np.random.randint(1, 11, n_products),
    'marketing_budget': np.random.exponential(50000, n_products),
    'konkurencja': np.random.randint(1, 21, n_products),
    'sezon': np.random.choice(['wiosna', 'lato', 'jesien', 'zima'], n_products),
    'kategoria': np.random.choice(['elektronika', 'moda', 'dom', 'sport', 'ksiazki'], n_products),
    'ocena_testowa': np.random.normal(7, 2, n_products),
    'innowacyjnosc': np.random.randint(1, 11, n_products),
    'marka_znana': np.random.choice([0, 1], n_products, p=[0.3, 0.7])
})

def czy_sukces(row):
    score = 0
    if 50 <= row['cena'] <= 500:
        score += 2
    elif row['cena'] > 1000:
        score -= 2
    score += row['jakosc'] * 0.8
    if row['marketing_budget'] > 100000:
        score += 3
    elif row['marketing_budget'] > 50000:
        score += 1
    score -= row['konkurencja'] * 0.2
    if row['sezon'] in ['lato', 'zima']:
        score += 1
    if row['kategoria'] == 'elektronika':
        score += 1
    elif row['kategoria'] == 'ksiazki':
        score -= 1
    score += (row['ocena_testowa'] - 5) * 0.5
    score += row['innowacyjnosc'] * 0.3
    if row['marka_znana']:
        score += 2
    probability = 1 / (1 + np.exp(-score + np.random.normal(0, 1)))
    return probability > 0.5

data['sukces'] = data.apply(czy_sukces, axis=1)

# Encoding i model
data_encoded = data.copy()
le_sezon = LabelEncoder()
le_kategoria = LabelEncoder()
data_encoded['sezon_num'] = le_sezon.fit_transform(data['sezon'])
data_encoded['kategoria_num'] = le_kategoria.fit_transform(data['kategoria'])

feature_columns = ['cena', 'jakosc', 'marketing_budget', 'konkurencja', 
                  'sezon_num', 'kategoria_num', 'ocena_testowa', 
                  'innowacyjnosc', 'marka_znana']
X = data_encoded[feature_columns]
y = data_encoded['sukces']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Trenuj model
rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)

# Ewaluacja
y_pred = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

# Wa≈ºno≈õƒá cech
feature_names = ['cena', 'jako≈õƒá', 'marketing_budget', 'konkurencja', 
                'sezon', 'kategoria', 'ocena_testowa', 'innowacyjno≈õƒá', 'marka_znana']
importance = rf_model.feature_importances_
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'importance': importance
}).sort_values('importance', ascending=False)

# Wykres wa≈ºno≈õci cech
plt.figure(figsize=(10, 6))
plt.barh(feature_importance['feature'], feature_importance['importance'])
plt.xlabel('Wa≈ºno≈õƒá cechy')
plt.title('Wa≈ºno≈õƒá cech w przewidywaniu sukcesu produktu')
plt.gca().invert_yaxis()
plt.close()

print(f"Wyniki Random Forest:")
print(f"Dok≈Çadno≈õƒá modelu: {accuracy:.1%}")
print(f"Liczba drzew: {rf_model.n_estimators}")

print("\nConfusion Matrix:")
print("Przewidywane:     Pora≈ºka  Sukces")
print(f"Rzeczywiste Pora≈ºka: {cm[0,0]:3d}     {cm[0,1]:3d}")
print(f"Rzeczywiste Sukces:  {cm[1,0]:3d}     {cm[1,1]:3d}")

print("\nWa≈ºno≈õƒá cech:")
for _, row in feature_importance.iterrows():
    print(f"{row['feature']}: {row['importance']:.3f}")

# Odtw√≥rz wykres
plt.figure(figsize=(10, 6))
plt.barh(feature_importance['feature'], feature_importance['importance'])
plt.xlabel('Wa≈ºno≈õƒá cechy')
plt.title('Wa≈ºno≈õƒá cech w przewidywaniu sukcesu produktu')
plt.gca().invert_yaxis()
plt.show()
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki i wykres wa≈ºno≈õci cech

```{python}
#| echo: false
#| fig-cap: "Wa≈ºno≈õƒá cech w Random Forest"
#| fig-width: 10
#| fig-height: 6

print(f"Wyniki Random Forest:")
print(f"Dok≈Çadno≈õƒá modelu: {accuracy:.1%}")
print(f"Liczba drzew: {rf_model.n_estimators}")

print("\nConfusion Matrix:")
print("Przewidywane:     Pora≈ºka  Sukces")
print(f"Rzeczywiste Pora≈ºka: {cm[0,0]:3d}     {cm[0,1]:3d}")
print(f"Rzeczywiste Sukces:  {cm[1,0]:3d}     {cm[1,1]:3d}")

print("\nWa≈ºno≈õƒá cech:")
for _, row in feature_importance.iterrows():
    print(f"{row['feature']}: {row['importance']:.3f}")

# Odtw√≥rz wykres
plt.figure(figsize=(10, 6))
plt.barh(feature_importance['feature'], feature_importance['importance'])
plt.xlabel('Wa≈ºno≈õƒá cechy')
plt.title('Wa≈ºno≈õƒá cech w przewidywaniu sukcesu produktu')
plt.gca().invert_yaxis()
plt.show()
```
:::

---

## üîç Random Forest vs Decision Tree - por√≥wnanie

```{python}
#| echo: true
#| output: false

from sklearn.tree import DecisionTreeClassifier

# Por√≥wnanie z pojedynczym Decision Tree
dt_model = DecisionTreeClassifier(max_depth=10, random_state=42)
dt_model.fit(X_train, y_train)

dt_pred = dt_model.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_pred)

print("POR√ìWNANIE MODELI:")
print(f"Decision Tree:    {dt_accuracy:.1%}")
print(f"Random Forest:    {accuracy:.1%}")
print(f"Poprawa:          +{(accuracy - dt_accuracy)*100:.1f} punkt√≥w procentowych")

# Test stabilno≈õci - r√≥≈ºne random_state
dt_results = []
rf_results = []

for rs in range(10):
    # Decision Tree
    dt_temp = DecisionTreeClassifier(max_depth=10, random_state=rs)
    dt_temp.fit(X_train, y_train)
    dt_results.append(accuracy_score(y_test, dt_temp.predict(X_test)))
    
    # Random Forest  
    rf_temp = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=rs)
    rf_temp.fit(X_train, y_train)
    rf_results.append(accuracy_score(y_test, rf_temp.predict(X_test)))

print(f"\nSTABILNO≈öƒÜ (10 r√≥≈ºnych random_state):")
print(f"Decision Tree rozrzut: {max(dt_results)-min(dt_results):.1%}")
print(f"Random Forest rozrzut: {max(rf_results)-min(rf_results):.1%}")
print("Random Forest jest znacznie bardziej stabilny!")
```

::: {.callout-note collapse="true"}
## Poka≈º por√≥wnanie modeli

```{python}
#| echo: false

print("POR√ìWNANIE MODELI:")
print(f"Decision Tree:    {dt_accuracy:.1%}")
print(f"Random Forest:    {accuracy:.1%}")
print(f"Poprawa:          +{(accuracy - dt_accuracy)*100:.1f} punkt√≥w procentowych")

print(f"\nSTABILNO≈öƒÜ (10 r√≥≈ºnych random_state):")
print(f"Decision Tree rozrzut: {max(dt_results)-min(dt_results):.1%}")
print(f"Random Forest rozrzut: {max(rf_results)-min(rf_results):.1%}")
print("Random Forest jest znacznie bardziej stabilny!")
```
:::

---

## üéØ Zastosowania Random Forest

### 1) **E-commerce - rekomendacje produkt√≥w**
```{python}
# Przewidywanie czy u≈ºytkownik kupi produkt
ecommerce_features = ['historia_zakup√≥w', 'czas_na_stronie', 'kategoria_preferowana', 
                     'cena_produktu', 'oceny_produktu', 'sezon']
# Target: 'kupi_produkt' (tak/nie)

ecommerce_rf = RandomForestClassifier(n_estimators=200)
# Wynik: system rekomendacji uwzglƒôdniajƒÖcy kompleksowe zachowania u≈ºytkownik√≥w
```

### 2) **Finanse - wykrywanie fraud√≥w**
```{python}
# Identyfikacja podejrzanych transakcji
fraud_features = ['kwota', 'godzina', 'lokalizacja', 'typ_karty', 
                 'historia_klienta', 'czƒôstotliwo≈õƒá_transakcji']
# Target: 'fraud' (tak/nie)

fraud_rf = RandomForestClassifier(n_estimators=500, class_weight='balanced')
# Wynik: system antyfaudowy z wysokƒÖ dok≈Çadno≈õciƒÖ
```

### 3) **HR - przewidywanie odej≈õƒá pracownik√≥w**
```{python}
# Employee churn prediction
hr_features = ['satisfaction', 'last_evaluation', 'projects', 'salary', 
              'time_company', 'work_accident', 'promotion']
# Target: 'left_company' (tak/nie)

hr_rf = RandomForestClassifier(n_estimators=300)
# Wynik: wczesne ostrze≈ºenia przed odej≈õciami kluczowych pracownik√≥w
```

---

## ‚öôÔ∏è Tuning parametr√≥w Random Forest

```{python}
#| echo: true
#| output: false

from sklearn.model_selection import GridSearchCV

# Najwa≈ºniejsze parametry do tuningu
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [10, 20, 50],
    'min_samples_leaf': [5, 10, 20],
    'max_features': ['sqrt', 'log2', None]
}

# Grid search z cross-validation (uwaga: mo≈ºe potrwaƒá!)
print("Rozpoczynam Grid Search - mo≈ºe potrwaƒá kilka minut...")
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=3,  # redukcja CV dla szybko≈õci
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

print("Najlepsze parametry:")
print(grid_search.best_params_)
print(f"Najlepsza dok≈Çadno≈õƒá CV: {grid_search.best_score_:.1%}")

# Model z najlepszymi parametrami
best_rf = grid_search.best_estimator_
best_accuracy = accuracy_score(y_test, best_rf.predict(X_test))
print(f"Dok≈Çadno≈õƒá na test set: {best_accuracy:.1%}")
```

::: {.callout-note collapse="true"}
## Poka≈º najlepsze parametry

```{python}
#| echo: false

print("Najlepsze parametry:")
print(grid_search.best_params_)
print(f"Najlepsza dok≈Çadno≈õƒá CV: {grid_search.best_score_:.1%}")
print(f"Dok≈Çadno≈õƒá na test set: {best_accuracy:.1%}")
```
:::

---

## ‚ö†Ô∏è Pu≈Çapki i rozwiƒÖzania

### 1) **Overfitting mimo ensemble**
```{python}
#| echo: true
#| output: false

# Problem: za g≈Çƒôbokie drzewa mogƒÖ nadal prowadziƒá do overfittingu
overfitted_rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,    # bez limitu g≈Çƒôboko≈õci!
    min_samples_leaf=1  # pozw√≥l na li≈õcie z 1 pr√≥bkƒÖ!
)
overfitted_rf.fit(X_train, y_train)

train_acc_over = accuracy_score(y_train, overfitted_rf.predict(X_train))
test_acc_over = accuracy_score(y_test, overfitted_rf.predict(X_test))

print(f"Overfitted Random Forest:")
print(f"Train accuracy: {train_acc_over:.1%}")
print(f"Test accuracy: {test_acc_over:.1%}")
print(f"R√≥≈ºnica: {train_acc_over - test_acc_over:.1%}")

# RozwiƒÖzanie: odpowiednie parametry
balanced_rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,      # ogranicz g≈Çƒôboko≈õƒá
    min_samples_leaf=10  # wymu≈õ wiƒôcej pr√≥bek w li≈õciach
)
balanced_rf.fit(X_train, y_train)

train_acc_bal = accuracy_score(y_train, balanced_rf.predict(X_train))
test_acc_bal = accuracy_score(y_test, balanced_rf.predict(X_test))

print(f"\nBalanced Random Forest:")
print(f"Train accuracy: {train_acc_bal:.1%}")
print(f"Test accuracy: {test_acc_bal:.1%}")
print(f"R√≥≈ºnica: {train_acc_bal - test_acc_bal:.1%} - znacznie lepiej!")
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki overfittingu

```{python}
#| echo: false

print(f"Overfitted Random Forest:")
print(f"Train accuracy: {train_acc_over:.1%}")
print(f"Test accuracy: {test_acc_over:.1%}")
print(f"R√≥≈ºnica: {train_acc_over - test_acc_over:.1%}")

print(f"\nBalanced Random Forest:")
print(f"Train accuracy: {train_acc_bal:.1%}")
print(f"Test accuracy: {test_acc_bal:.1%}")
print(f"R√≥≈ºnica: {train_acc_bal - test_acc_bal:.1%} - znacznie lepiej!")
```
:::

### 2) **Niezbalansowane klasy**
```{python}
#| echo: true
#| output: false

# Problem: gdy jedna klasa jest rzadka (np. 5% fraud√≥w, 95% normalnych transakcji)
from sklearn.utils.class_weight import compute_class_weight

# Sprawd≈∫ balans klas w naszych danych
print(f"Rozk≈Çad klas:")
print(f"Pora≈ºka: {(~y_train).sum()} ({(~y_train).mean():.1%})")
print(f"Sukces: {y_train.sum()} ({y_train.mean():.1%})")

# RozwiƒÖzanie 1: class_weight='balanced'
balanced_class_rf = RandomForestClassifier(
    n_estimators=100,
    class_weight='balanced',  # automatyczne wa≈ºenie klas
    random_state=42
)
balanced_class_rf.fit(X_train, y_train)

# RozwiƒÖzanie 2: bootstrap sampling
balanced_rf = RandomForestClassifier(
    n_estimators=100,
    max_samples=0.8,  # u≈ºyj tylko 80% pr√≥bek do ka≈ºdego drzewa
    random_state=42
)
balanced_rf.fit(X_train, y_train)

print("RozwiƒÖzania zosta≈Çy zaimplementowane!")
```

::: {.callout-note collapse="true"}
## Poka≈º rozk≈Çad klas

```{python}
#| echo: false

print(f"Rozk≈Çad klas:")
print(f"Pora≈ºka: {(~y_train).sum()} ({(~y_train).mean():.1%})")
print(f"Sukces: {y_train.sum()} ({y_train.mean():.1%})")
print("RozwiƒÖzania zosta≈Çy zaimplementowane!")
```
:::

---

## üåç Real-world przypadki u≈ºycia

1. **Finanse:** Credit scoring, wykrywanie prania pieniƒôdzy, trading algorytmiczny
2. **E-commerce:** Systemy rekomendacji, dynamic pricing, churn prediction
3. **Healthcare:** Diagnoza medyczna, drug discovery, analiza obraz√≥w medycznych
4. **Marketing:** Customer segmentation, campaign optimization, A/B testing
5. **Cybersecurity:** Intrusion detection, malware classification, anomaly detection

::: {.callout-tip}
## üí° Kiedy u≈ºywaƒá Random Forest?

**‚úÖ U≈ªYJ GDY:**

- Potrzebujesz wysokiej dok≈Çadno≈õci z interpretowalnym modelem
- Masz mixed features (liczbowe + kategoryczne)
- Dane zawierajƒÖ missing values (RF radzi sobie z nimi dobrze)
- Chcesz feature importance bez complex feature engineering
- Potrzebujesz stabilnego modelu (ma≈Ça wariancja)

**‚ùå NIE U≈ªYWAJ GDY:**

- Masz bardzo du≈ºe datasety (u≈ºyj XGBoost/LightGBM)
- Potrzebujesz bardzo prostego modelu (u≈ºyj Decision Tree)
- Target jest continuous i potrzebujesz liniowej interpretacji (u≈ºyj Linear Regression)
- Masz bardzo wysokowymiarowe dane (u≈ºyj SVM lub Neural Networks)
:::

**Nastƒôpna ≈õciƒÖgawka:** K-Means Clustering - grupowanie bez etykiet! üéØ
