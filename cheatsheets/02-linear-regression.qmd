---
title: "Linear Regression ‚Äî przewiduj warto≈õci liczbowe"
format:
  html:
    code-tools: true
---

## üìà Czym jest Linear Regression?

**Linear Regression** to jeden z najprostszych i najczƒô≈õciej u≈ºywanych algorytm√≥w ML do **przewidywania warto≈õci liczbowych** (np. ceny, temperatury, sprzeda≈ºy). Szuka najlepszej linii prostej, kt√≥ra opisuje zale≈ºno≈õƒá miƒôdzy zmiennymi.

::: {.callout-note}
## üí° Intuicja matematyczna
Szukamy takiej linii `y = ax + b`, kt√≥ra najlepiej "przechodzi" przez nasze punkty danych. Algorytm automatycznie dobiera optymalne warto≈õci `a` (slope) i `b` (intercept).
:::

---

## üè† Praktyczny przyk≈Çad: predykcja cen mieszka≈Ñ

```{python}
#| echo: true
#| output: false

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import matplotlib.pyplot as plt

# Tworzenie przyk≈Çadowych danych mieszka≈Ñ
np.random.seed(42)
n_mieszkan = 1000

data = pd.DataFrame({
    'powierzchnia': np.random.normal(70, 25, n_mieszkan),
    'liczba_pokoi': np.random.randint(1, 5, n_mieszkan),
    'wiek_budynku': np.random.randint(0, 50, n_mieszkan),
    'odleglosc_centrum': np.random.exponential(5, n_mieszkan)
})

# Realistyczna formu≈Ça ceny (z szumem)
data['cena'] = (
    data['powierzchnia'] * 8000 +           # 8k za m¬≤
    data['liczba_pokoi'] * 15000 +          # 15k za pok√≥j
    (50 - data['wiek_budynku']) * 2000 +    # starsze = ta≈Ñsze
    data['odleglosc_centrum'] * (-3000) +   # dalej = ta≈Ñsze
    np.random.normal(0, 50000, n_mieszkan)  # szum losowy
)

print("Podstawowe statystyki:")
print(data.describe())
```

::: {.callout-note collapse="true"}
## Poka≈º statystyki

```{python}
#| echo: false

print("Podstawowe statystyki:")
print(data.describe())
```
:::

---

## üîß Trenowanie modelu krok po kroku

### 1) Przygotowanie danych

```{python}
#| echo: true
#| output: false

# Sprawdzenie korelacji - kt√≥re zmienne sƒÖ najwa≈ºniejsze?
correlation = data.corr()['cena'].sort_values(ascending=False)

# Podzia≈Ç na features (X) i target (y)
X = data[['powierzchnia', 'liczba_pokoi', 'wiek_budynku', 'odleglosc_centrum']]
y = data['cena']

# Podzia≈Ç train/test (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Korelacja z cenƒÖ:")
print(correlation)

print(f"Dane treningowe: {X_train.shape[0]} mieszka≈Ñ")
print(f"Dane testowe: {X_test.shape[0]} mieszka≈Ñ")
```

::: {.callout-note collapse="true"}
## Poka≈º korelacje oraz dane treningowe

```{python}
#| echo: false

print("Korelacja z cenƒÖ:")
print(correlation)

print(f"Dane treningowe: {X_train.shape[0]} mieszka≈Ñ")
print(f"Dane testowe: {X_test.shape[0]} mieszka≈Ñ")
```
:::

### 2) Trenowanie modelu

```{python}
#| echo: true
#| output: false

# Utworzenie i trenowanie modelu
model = LinearRegression()
model.fit(X_train, y_train)

# Sprawdzenie wsp√≥≈Çczynnik√≥w
print("Wsp√≥≈Çczynniki modelu:")
for feature, coef in zip(X.columns, model.coef_):
    print(f"{feature}: {coef:.0f} z≈Ç")
print(f"Intercept (sta≈Ça): {model.intercept_:.0f} z≈Ç")
```

::: {.callout-note collapse="true"}
## Poka≈º wsp√≥≈Çczynniki

```{python}
#| echo: false

# Sprawdzenie wsp√≥≈Çczynnik√≥w
print("Wsp√≥≈Çczynniki modelu:")
for feature, coef in zip(X.columns, model.coef_):
    print(f"{feature}: {coef:.0f} z≈Ç")
print(f"Intercept (sta≈Ça): {model.intercept_:.0f} z≈Ç")
```
:::

### 3) Ewaluacja wynik√≥w

```{python}
#| echo: true
#| output: false

# Predykcje na zbiorze testowym
y_pred = model.predict(X_test)

# Metryki
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"\nWyniki modelu:")
print(f"Mean Absolute Error: {mae:.0f} z≈Ç")
print(f"R¬≤ Score: {r2:.3f}")
print(f"≈öredni b≈ÇƒÖd to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania")

# Przyk≈Çadowe predykcje
for i in range(5):
    rzeczywista = y_test.iloc[i]
    przewidywana = y_pred[i]
    b≈ÇƒÖd = abs(rzeczywista - przewidywana)
    print(f"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}z≈Ç, "
          f"przewidywana={przewidywana:.0f}z≈Ç, b≈ÇƒÖd={b≈ÇƒÖd:.0f}z≈Ç")
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki modelu

```{python}
#| echo: false

print(f"\nWyniki modelu:")
print(f"Mean Absolute Error: {mae:.0f} z≈Ç")
print(f"R¬≤ Score: {r2:.3f}")
print(f"≈öredni b≈ÇƒÖd to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania")

for i in range(5):
    rzeczywista = y_test.iloc[i]
    przewidywana = y_pred[i]
    b≈ÇƒÖd = abs(rzeczywista - przewidywana)
    print(f"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}z≈Ç, "
          f"przewidywana={przewidywana:.0f}z≈Ç, b≈ÇƒÖd={b≈ÇƒÖd:.0f}z≈Ç")
```
:::

---

## üìä Wizualizacja wynik√≥w

```{python}
#| echo: true
#| output: false

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import matplotlib.pyplot as plt

# Przyk≈Çadowe dane mieszka≈Ñ (powtarzamy dla demonstracji)
np.random.seed(42)
n_mieszkan = 1000

data = pd.DataFrame({
    'powierzchnia': np.random.normal(70, 25, n_mieszkan),
    'liczba_pokoi': np.random.randint(1, 5, n_mieszkan),
    'wiek_budynku': np.random.randint(0, 50, n_mieszkan),
    'odleglosc_centrum': np.random.exponential(5, n_mieszkan)
})

data['cena'] = (
    data['powierzchnia'] * 8000 +
    data['liczba_pokoi'] * 15000 +
    (50 - data['wiek_budynku']) * 2000 +
    data['odleglosc_centrum'] * (-3000) +
    np.random.normal(0, 50000, n_mieszkan)
)

# Przygotowanie i trenowanie modelu
X = data[['powierzchnia', 'liczba_pokoi', 'wiek_budynku', 'odleglosc_centrum']]
y = data['cena']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Oblicz metryki
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Przygotuj wykresy
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_test, y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Rzeczywiste ceny')
plt.ylabel('Przewidywane ceny')
plt.title('Rzeczywiste vs Przewidywane')

plt.subplot(1, 2, 2)
residuals = y_test - y_pred
plt.scatter(y_pred, residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Przewidywane ceny')
plt.ylabel('Residuals (b≈Çƒôdy)')
plt.title('Analiza residuals')

plt.tight_layout()

# Zapisz wykres do zmiennej
import io
import base64
buf = io.BytesIO()
plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')
buf.seek(0)
plt.close()

# Przygotuj przyk≈Çadowe predykcje
przykladowe_predykcje = []
for i in range(3):
    rzeczywista = y_test.iloc[i]
    przewidywana = y_pred[i]
    blad = abs(rzeczywista - przewidywana)
    przykladowe_predykcje.append(f"Mieszkanie {i+1}: rzeczywista={rzeczywista:.0f}z≈Ç, przewidywana={przewidywana:.0f}z≈Ç, b≈ÇƒÖd={blad:.0f}z≈Ç")

print(f"Wyniki modelu Linear Regression:")
print(f"Mean Absolute Error: {mae:.0f} z≈Ç")
print(f"R¬≤ Score: {r2:.3f}")
print(f"≈öredni b≈ÇƒÖd to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania")
print(f"\nPrzyk≈Çadowe predykcje:")
for pred in przykladowe_predykcje:
    print(pred)

# Odtw√≥rz wykres
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_test, y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Rzeczywiste ceny')
plt.ylabel('Przewidywane ceny')
plt.title('Rzeczywiste vs Przewidywane')

plt.subplot(1, 2, 2)
residuals = y_test - y_pred
plt.scatter(y_pred, residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Przewidywane ceny')
plt.ylabel('Residuals (b≈Çƒôdy)')
plt.title('Analiza residuals')

plt.tight_layout()
plt.show()
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki i wykresy analizy

```{python}
#| echo: false
#| fig-cap: "Analiza wynik√≥w modelu Linear Regression"
#| fig-width: 10
#| fig-height: 6

print(f"Wyniki modelu Linear Regression:")
print(f"Mean Absolute Error: {mae:.0f} z≈Ç")
print(f"R¬≤ Score: {r2:.3f}")
print(f"≈öredni b≈ÇƒÖd to {mae/data['cena'].mean()*100:.1f}% ceny mieszkania")
print(f"\nPrzyk≈Çadowe predykcje:")
for pred in przykladowe_predykcje:
    print(pred)

# Odtw√≥rz wykres
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_test, y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Rzeczywiste ceny')
plt.ylabel('Przewidywane ceny')
plt.title('Rzeczywiste vs Przewidywane')

plt.subplot(1, 2, 2)
residuals = y_test - y_pred
plt.scatter(y_pred, residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Przewidywane ceny')
plt.ylabel('Residuals (b≈Çƒôdy)')
plt.title('Analiza residuals')

plt.tight_layout()
plt.show()
```
:::

---

## üéØ Praktyczne zastosowania Linear Regression

### 1) **Biznesowe**
```{python}
#| echo: true
#| output: false

# Przewidywanie sprzeda≈ºy na podstawie bud≈ºetu marketingowego
sales_data = pd.DataFrame({
    'marketing_budget': [10000, 15000, 20000, 25000, 30000],
    'sales': [100000, 140000, 180000, 220000, 260000]
})

model_sales = LinearRegression()
model_sales.fit(sales_data[['marketing_budget']], sales_data['sales'])

# "Je≈õli zwiƒôkszƒô bud≈ºet do 35k, sprzeda≈º wzro≈õnie do:"
new_sales = model_sales.predict([[35000]])
print(f"Przewidywana sprzeda≈º przy bud≈ºecie 35k: {new_sales[0]:.0f}")
```

::: {.callout-note collapse="true"}
## Poka≈º przewidywanƒÖ sprzeda≈º

```{python}
#| echo: false

print(f"Przewidywana sprzeda≈º przy bud≈ºecie 35k: {new_sales[0]:.0f}")
```
:::

### 2) **Analizy finansowe**
```{python}
#| echo: true
#| output: false

# Relacja miƒôdzy PKB a konsumpcjƒÖ
economics_data = pd.DataFrame({
    'pkb_per_capita': [25000, 30000, 35000, 40000, 45000],
    'konsumpcja': [18000, 21000, 24000, 27000, 30000]
})

model_econ = LinearRegression()
model_econ.fit(economics_data[['pkb_per_capita']], economics_data['konsumpcja'])

# Wsp√≥≈Çczynnik sk≈Çonno≈õci do konsumpcji
print(f"Na ka≈ºde dodatkowe 1000z≈Ç PKB, konsumpcja ro≈õnie o: {model_econ.coef_[0]:.0f}z≈Ç")
```

::: {.callout-note collapse="true"}
## Poka≈º wsp√≥≈Çczynnik sk≈Çonno≈õci do konsumpcji

```{python}
#| echo: false

print(f"Na ka≈ºde dodatkowe 1000z≈Ç PKB, konsumpcja ro≈õnie o: {model_econ.coef_[0]:.0f}z≈Ç")
```
:::

---

## ‚ö†Ô∏è Najczƒôstsze pu≈Çapki i jak ich unikaƒá

### 1) **Overfitting z wieloma zmiennymi**
```{python}
#| echo: true
#| output: false

# Z≈ÅO: za du≈ºo features wzglƒôdem danych
# Je≈õli masz 100 mieszka≈Ñ, nie u≈ºywaj 50 features!

# DOBRZE: zasada kciuka
def check_features_ratio(X, y):
    ratio = len(y) / X.shape[1]
    if ratio < 10:
        print(f"‚ö†Ô∏è Uwaga: masz tylko {ratio:.1f} obserwacji na feature!")
        print("Rozwa≈º: wiƒôcej danych lub mniej features")
    else:
        print(f"‚úÖ OK: {ratio:.1f} obserwacji na feature")

check_features_ratio(X_train, y_train)
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki

```{python}
#| echo: false

def check_features_ratio(X, y):
    ratio = len(y) / X.shape[1]
    if ratio < 10:
        print(f"‚ö†Ô∏è Uwaga: masz tylko {ratio:.1f} obserwacji na feature!")
        print("Rozwa≈º: wiƒôcej danych lub mniej features")
    else:
        print(f"‚úÖ OK: {ratio:.1f} obserwacji na feature")

check_features_ratio(X_train, y_train)
```
:::

### 2) **Sprawdzanie za≈Ço≈ºe≈Ñ linearno≈õci**
```{python}
#| echo: true
#| output: false

# Sprawd≈∫ czy zale≈ºno≈õci sƒÖ rzeczywi≈õcie liniowe
from scipy import stats

for col in X.columns:
    correlation, p_value = stats.pearsonr(data[col], data['cena'])
    print(f"{col}: korelacja={correlation:.3f}, p-value={p_value:.3f}")
    
    if abs(correlation) < 0.1:
        print(f"‚ö†Ô∏è {col} ma s≈ÇabƒÖ korelacjƒô - mo≈ºe nie byƒá przydatna")
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki korelacji

```{python}
#| echo: false

from scipy import stats

for col in X.columns:
    correlation, p_value = stats.pearsonr(data[col], data['cena'])
    print(f"{col}: korelacja={correlation:.3f}, p-value={p_value:.3f}")
    
    if abs(correlation) < 0.1:
        print(f"‚ö†Ô∏è {col} ma s≈ÇabƒÖ korelacjƒô - mo≈ºe nie byƒá przydatna")
```
:::

### 3) **Wielokoliniowo≈õƒá** (features korelujƒÖ miƒôdzy sobƒÖ)

```{python}
#| echo: true
#| output: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Przyk≈Çadowe dane features (demonstracja)
np.random.seed(42)
n_samples = 1000

X = pd.DataFrame({
    'powierzchnia': np.random.normal(70, 25, n_samples),
    'liczba_pokoi': np.random.randint(1, 5, n_samples),
    'wiek_budynku': np.random.randint(0, 50, n_samples),
    'odleglosc_centrum': np.random.exponential(5, n_samples)
})

# Oblicz korelacje miƒôdzy features
correlation_matrix = X.corr()

# Przygotuj wykres
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Korelacje miƒôdzy features')
plt.close()

print("Macierz korelacji miƒôdzy zmiennymi:")
print(correlation_matrix)
print("\nInterpretacja:")
print("‚Ä¢ Warto≈õci blisko 1.0 = silna korelacja pozytywna")
print("‚Ä¢ Warto≈õci blisko -1.0 = silna korelacja negatywna") 
print("‚Ä¢ Warto≈õci blisko 0.0 = brak korelacji")
print("\nJe≈õli korelacja miƒôdzy features > 0.8, usu≈Ñ jednƒÖ z nich (multicollinearity)!")

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Korelacje miƒôdzy features')
plt.show()
```

::: {.callout-note collapse="true"}
## Poka≈º mapƒô korelacji

```{python}
#| echo: false
#| fig-cap: "Mapa korelacji miƒôdzy zmiennymi"
#| fig-width: 8
#| fig-height: 6

print("Macierz korelacji miƒôdzy zmiennymi:")
print(correlation_matrix)
print("\nInterpretacja:")
print("‚Ä¢ Warto≈õci blisko 1.0 = silna korelacja pozytywna")
print("‚Ä¢ Warto≈õci blisko -1.0 = silna korelacja negatywna") 
print("‚Ä¢ Warto≈õci blisko 0.0 = brak korelacji")
print("\nJe≈õli korelacja miƒôdzy features > 0.8, usu≈Ñ jednƒÖ z nich (multicollinearity)!")

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Korelacje miƒôdzy features')
plt.show()
```
:::

# Je≈õli korelacja miƒôdzy features > 0.8, usu≈Ñ jednƒÖ z nich!

---

## üîß Parametry do tuningu

```{python}
#| echo: true
#| output: false

from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline

# 1) Standardization - gdy features majƒÖ r√≥≈ºne skale
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)

# 2) Polynomial Features - dla nieliniowych zale≈ºno≈õci
poly_model = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('linear', LinearRegression())
])
poly_model.fit(X_train, y_train)

poly_pred = poly_model.predict(X_test)
poly_r2 = r2_score(y_test, poly_pred)
print(f"Polynomial Regression R¬≤: {poly_r2:.3f}")

# 3) Regularization - Ridge i Lasso
from sklearn.linear_model import Ridge, Lasso

ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
ridge_r2 = r2_score(y_test, ridge.predict(X_test))

lasso = Lasso(alpha=1.0)
lasso.fit(X_train, y_train)
lasso_r2 = r2_score(y_test, lasso.predict(X_test))

print(f"Ridge R¬≤: {ridge_r2:.3f}")
print(f"Lasso R¬≤: {lasso_r2:.3f}")
```

::: {.callout-note collapse="true"}
## Poka≈º wyniki predykcji cen

```{python}
#| echo: false
print(f"Polynomial Regression R¬≤: {poly_r2:.3f}")
print(f"Ridge R¬≤: {ridge_r2:.3f}")
print(f"Lasso R¬≤: {lasso_r2:.3f}")
```
:::

---

## üåç Real-world przyk≈Çady zastosowa≈Ñ

1. **E-commerce:** Przewidywanie warto≈õci ≈ºyciowej klienta (LTV)
2. **Nieruchomo≈õci:** Automatyczna wycena dom√≥w (Zillow)
3. **Finanse:** Scoring kredytowy, przewidywanie cen akcji
4. **Marketing:** ROI kampanii reklamowych
5. **Supply Chain:** Prognozowanie popytu na produkty

::: {.callout-tip}
## üí° Kiedy u≈ºywaƒá Linear Regression?

**‚úÖ U≈ªYJ GDY:**

- Przewidujesz warto≈õci liczbowe (continuous target)
- Zale≈ºno≈õci wydajƒÖ siƒô liniowe
- Chcesz interpretowalny model
- Masz stosunkowo ma≈Ço features

**‚ùå NIE U≈ªYWAJ GDY:**

- Target jest kategoryczny (u≈ºyj klasyfikacji)
- Zale≈ºno≈õci sƒÖ bardzo nieliniowe (u≈ºyj Random Forest, XGBoost)
- Masz tysiƒÖce features (u≈ºyj regularization)
:::

**Nastƒôpna ≈õciƒÖgawka:** [Decision Trees - klasyfikacja](03-decision-trees.qmd) üå≥
