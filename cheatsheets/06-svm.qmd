---
title: "Support Vector Machines â€” optymalne granice decyzyjne"
format:
  html:
    code-tools: true
---

## âš”ï¸ Czym jest Support Vector Machine (SVM)?

**SVM** to algorytm, ktÃ³ry znajduje **optymalnÄ… granicÄ™ decyzyjnÄ…** miÄ™dzy klasami. Zamiast szukaÄ‡ "jakiejkolwiek" linii podziaÅ‚u, SVM znajduje tÄ…, ktÃ³ra **maksymalizuje margines** - odlegÅ‚oÅ›Ä‡ do najbliÅ¼szych punktÃ³w z kaÅ¼dej klasy.

::: {.callout-note}
## ğŸ’¡ Intuicja
WyobraÅº sobie sÄ™dziego, ktÃ³ry musi wyznaczyÄ‡ granicÄ™ miÄ™dzy dwoma druÅ¼ynami. SVM nie tylko znajdzie granicÄ™, ale postawi jÄ… tak, Å¼eby byÅ‚a jak najdalej od najbliÅ¼szych zawodnikÃ³w z obu stron - dla maksymalnego bezpieczeÅ„stwa!
:::

---

## ğŸ¯ Praktyczny przykÅ‚ad: klasyfikacja emaili spam/ham

Czy email to spam czy prawdziwa wiadomoÅ›Ä‡?

```{python}
#| echo: true
#| output: false

import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt

# Tworzenie realistycznych danych emaili
np.random.seed(42)
n_emails = 2000

# Generuj rÃ³Å¼ne typy emaili
email_types = np.random.choice(['spam', 'ham'], n_emails, p=[0.3, 0.7])

data = pd.DataFrame({
    'dlugosc_tematu': np.random.randint(5, 100, n_emails),
    'liczba_wielkich_liter': np.random.randint(0, 50, n_emails),
    'liczba_wykrzyknikow': np.random.randint(0, 10, n_emails),
    'liczba_linkow': np.random.randint(0, 20, n_emails),
    'slowa_promocyjne': np.random.randint(0, 15, n_emails),
    'dlugosc_wiadomosci': np.random.randint(50, 5000, n_emails),
    'godzina_wyslania': np.random.randint(0, 24, n_emails),
    'typ': email_types
})

# Realistyczne wzorce dla spam vs ham
for i, typ in enumerate(data['typ']):
    if typ == 'spam':
        # Spam charakterystyki
        data.loc[i, 'dlugosc_tematu'] = np.random.randint(20, 80)  # dÅ‚uÅ¼sze tematy
        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(10, 50)  # DUÅ»O WIELKIMI
        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(3, 10)  # wiÄ™cej !!!
        data.loc[i, 'liczba_linkow'] = np.random.randint(5, 20)  # duÅ¼o linkÃ³w
        data.loc[i, 'slowa_promocyjne'] = np.random.randint(5, 15)  # PROMOCJA, GRATIS
        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(200, 1000)  # Å›rednie dÅ‚ugoÅ›ci
        data.loc[i, 'godzina_wyslania'] = np.random.choice([2, 3, 22, 23, 1])  # dziwne godziny
    else:
        # Ham (prawdziwe) charakterystyki  
        data.loc[i, 'dlugosc_tematu'] = np.random.randint(5, 40)  # krÃ³tsze tematy
        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(0, 8)  # normalne uÅ¼ywanie
        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(0, 3)  # rzadko !
        data.loc[i, 'liczba_linkow'] = np.random.randint(0, 3)  # maÅ‚o linkÃ³w
        data.loc[i, 'slowa_promocyjne'] = np.random.randint(0, 2)  # prawie brak
        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(50, 3000)  # rÃ³Å¼ne dÅ‚ugoÅ›ci
        data.loc[i, 'godzina_wyslania'] = np.random.randint(8, 22)  # normalne godziny

# Konwertuj target na binary
data['is_spam'] = (data['typ'] == 'spam').astype(int)

print("Statystyki emaili:")
print(data.describe())
print(f"\nRozkÅ‚ad spam/ham:")
print(data['typ'].value_counts())
print(f"Procent spam: {data['is_spam'].mean():.1%}")
```

::: {.callout-note collapse="true"}
## PokaÅ¼ statystyki emaili

```{python}
#| echo: false

print("Statystyki emaili:")
print(data.describe())
print(f"\nRozkÅ‚ad spam/ham:")
print(data['typ'].value_counts())
print(f"Procent spam: {data['is_spam'].mean():.1%}")
```
:::

---

## ğŸ”§ Budowanie modelu krok po kroku

### 1) Przygotowanie danych

```{python}
#| echo: true
#| output: false

# Features do modelu
features = ['dlugosc_tematu', 'liczba_wielkich_liter', 'liczba_wykrzyknikow', 
           'liczba_linkow', 'slowa_promocyjne', 'dlugosc_wiadomosci', 'godzina_wyslania']
X = data[features]
y = data['is_spam']

# PodziaÅ‚ train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standaryzacja - KLUCZOWA dla SVM!
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Dane treningowe: {len(X_train)} emaili")
print(f"Dane testowe: {len(X_test)} emaili")
print(f"Spam w train: {y_train.mean():.1%}")
print(f"Spam w test: {y_test.mean():.1%}")

print("\nPrzykÅ‚ad standaryzacji:")
print("Przed:", X_train.iloc[0].values)
print("Po:", X_train_scaled[0])
```

::: {.callout-note collapse="true"}
## PokaÅ¼ podziaÅ‚ danych

```{python}
#| echo: false

print(f"Dane treningowe: {len(X_train)} emaili")
print(f"Dane testowe: {len(X_test)} emaili")
print(f"Spam w train: {y_train.mean():.1%}")
print(f"Spam w test: {y_test.mean():.1%}")

print("\nPrzykÅ‚ad standaryzacji:")
print("Przed:", X_train.iloc[0].values)
print("Po:", X_train_scaled[0])
```
:::

### 2) Trenowanie rÃ³Å¼nych kerneli SVM

```{python}
#| echo: true
#| output: false

# PorÃ³wnanie rÃ³Å¼nych kerneli
kernels = ['linear', 'rbf', 'poly']
svm_results = {}

for kernel in kernels:
    print(f"\nTrenowanie SVM z kernelem: {kernel}")
    
    if kernel == 'poly':
        svm = SVC(kernel=kernel, degree=3, random_state=42)
    else:
        svm = SVC(kernel=kernel, random_state=42)
    
    # Trenowanie
    svm.fit(X_train_scaled, y_train)
    
    # Predykcje
    y_pred = svm.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    
    svm_results[kernel] = {
        'model': svm,
        'accuracy': accuracy,
        'predictions': y_pred
    }
    
    print(f"DokÅ‚adnoÅ›Ä‡: {accuracy:.1%}")
    print(f"Liczba support vectors: {svm.n_support_}")

# ZnajdÅº najlepszy kernel
best_kernel = max(svm_results.keys(), key=lambda k: svm_results[k]['accuracy'])
best_model = svm_results[best_kernel]['model']

print(f"\nğŸ† Najlepszy kernel: {best_kernel}")
print(f"Najlepsza dokÅ‚adnoÅ›Ä‡: {svm_results[best_kernel]['accuracy']:.1%}")
```

::: {.callout-note collapse="true"}
## PokaÅ¼ wyniki rÃ³Å¼nych kerneli

```{python}
#| echo: false

for kernel in kernels:
    svm_results_display = svm_results[kernel]
    print(f"\nKernel {kernel}:")
    print(f"DokÅ‚adnoÅ›Ä‡: {svm_results_display['accuracy']:.1%}")
    print(f"Liczba support vectors: {svm_results_display['model'].n_support_}")

print(f"\nğŸ† Najlepszy kernel: {best_kernel}")
print(f"Najlepsza dokÅ‚adnoÅ›Ä‡: {svm_results[best_kernel]['accuracy']:.1%}")
```
:::

### 3) Ewaluacja najlepszego modelu

```{python}
#| echo: true
#| output: false

# SzczegÃ³Å‚owa analiza najlepszego modelu
best_predictions = svm_results[best_kernel]['predictions']

print(f"SZCZEGÃ“ÅOWA ANALIZA - SVM {best_kernel.upper()}")
print("=" * 50)

# Classification report
print("\nRaport klasyfikacji:")
print(classification_report(y_test, best_predictions, target_names=['Ham', 'Spam']))

# Confusion Matrix
cm = confusion_matrix(y_test, best_predictions)
print("\nConfusion Matrix:")
print("Przewidywane:     Ham    Spam")
print(f"Rzeczywiste Ham:  {cm[0,0]:3d}    {cm[0,1]:3d}")
print(f"Rzeczywiste Spam: {cm[1,0]:3d}    {cm[1,1]:3d}")

# Analiza bÅ‚Ä™dÃ³w
false_positives = np.where((y_test == 0) & (best_predictions == 1))[0]
false_negatives = np.where((y_test == 1) & (best_predictions == 0))[0]

print(f"\nAnaliza bÅ‚Ä™dÃ³w:")
print(f"False Positives (Ham â†’ Spam): {len(false_positives)}")
print(f"False Negatives (Spam â†’ Ham): {len(false_negatives)}")

if len(false_positives) > 0:
    print(f"\nPrzykÅ‚ad bÅ‚Ä™dnie sklasyfikowanego Ham jako Spam:")
    fp_idx = false_positives[0]
    print(f"Email: {X_test.iloc[fp_idx].to_dict()}")
```

::: {.callout-note collapse="true"}
## PokaÅ¼ szczegÃ³Å‚owÄ… analizÄ™

```{python}
#| echo: false

print(f"SZCZEGÃ“ÅOWA ANALIZA - SVM {best_kernel.upper()}")
print("=" * 50)

print("\nRaport klasyfikacji:")
print(classification_report(y_test, best_predictions, target_names=['Ham', 'Spam']))

cm = confusion_matrix(y_test, best_predictions)
print("\nConfusion Matrix:")
print("Przewidywane:     Ham    Spam")
print(f"Rzeczywiste Ham:  {cm[0,0]:3d}    {cm[0,1]:3d}")
print(f"Rzeczywiste Spam: {cm[1,0]:3d}    {cm[1,1]:3d}")

false_positives = np.where((y_test == 0) & (best_predictions == 1))[0]
false_negatives = np.where((y_test == 1) & (best_predictions == 0))[0]

print(f"\nAnaliza bÅ‚Ä™dÃ³w:")
print(f"False Positives (Ham â†’ Spam): {len(false_positives)}")
print(f"False Negatives (Spam â†’ Ham): {len(false_negatives)}")

if len(false_positives) > 0:
    print(f"\nPrzykÅ‚ad bÅ‚Ä™dnie sklasyfikowanego Ham jako Spam:")
    fp_idx = false_positives[0]
    print(f"Email: {X_test.iloc[fp_idx].to_dict()}")
```
:::

---

## ğŸ“Š Wizualizacja granic decyzyjnych

```{python}
#| echo: true
#| output: false

import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt

# Przygotuj dane (powtarzamy dla kompletnoÅ›ci)
np.random.seed(42)
n_emails = 2000

email_types = np.random.choice(['spam', 'ham'], n_emails, p=[0.3, 0.7])

data = pd.DataFrame({
    'dlugosc_tematu': np.random.randint(5, 100, n_emails),
    'liczba_wielkich_liter': np.random.randint(0, 50, n_emails),
    'liczba_wykrzyknikow': np.random.randint(0, 10, n_emails),
    'liczba_linkow': np.random.randint(0, 20, n_emails),
    'slowa_promocyjne': np.random.randint(0, 15, n_emails),
    'dlugosc_wiadomosci': np.random.randint(50, 5000, n_emails),
    'godzina_wyslania': np.random.randint(0, 24, n_emails),
    'typ': email_types
})

# Popraw charakterystyki
for i, typ in enumerate(data['typ']):
    if typ == 'spam':
        data.loc[i, 'dlugosc_tematu'] = np.random.randint(20, 80)
        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(10, 50)
        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(3, 10)
        data.loc[i, 'liczba_linkow'] = np.random.randint(5, 20)
        data.loc[i, 'slowa_promocyjne'] = np.random.randint(5, 15)
        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(200, 1000)
        data.loc[i, 'godzina_wyslania'] = np.random.choice([2, 3, 22, 23, 1])
    else:
        data.loc[i, 'dlugosc_tematu'] = np.random.randint(5, 40)
        data.loc[i, 'liczba_wielkich_liter'] = np.random.randint(0, 8)
        data.loc[i, 'liczba_wykrzyknikow'] = np.random.randint(0, 3)
        data.loc[i, 'liczba_linkow'] = np.random.randint(0, 3)
        data.loc[i, 'slowa_promocyjne'] = np.random.randint(0, 2)
        data.loc[i, 'dlugosc_wiadomosci'] = np.random.randint(50, 3000)
        data.loc[i, 'godzina_wyslania'] = np.random.randint(8, 22)

data['is_spam'] = (data['typ'] == 'spam').astype(int)

# Wybierz tylko 2 features dla wizualizacji 2D
features_2d = ['liczba_wielkich_liter', 'liczba_wykrzyknikow']
X_2d = data[features_2d]
y = data['is_spam']

X_train_2d, X_test_2d, y_train, y_test = train_test_split(X_2d, y, test_size=0.2, random_state=42)

scaler_2d = StandardScaler()
X_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)
X_test_2d_scaled = scaler_2d.transform(X_test_2d)

# Trenuj modele z rÃ³Å¼nymi kernelami (na 2D)
kernels = ['linear', 'rbf', 'poly']
plt.figure(figsize=(15, 5))

for i, kernel in enumerate(kernels):
    plt.subplot(1, 3, i+1)
    
    # Trenuj model
    if kernel == 'poly':
        svm = SVC(kernel=kernel, degree=3, random_state=42)
    else:
        svm = SVC(kernel=kernel, random_state=42)
    
    svm.fit(X_train_2d_scaled, y_train)
    
    # StwÃ³rz mesh do wizualizacji granic
    h = 0.02
    x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1
    y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    # Predykcje na mesh
    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # Narysuj granice i punkty
    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)
    scatter = plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], 
                         c=y_train, cmap=plt.cm.RdYlBu, edgecolors='black')
    
    # PodÅ›wietl support vectors
    if hasattr(svm, 'support_'):
        plt.scatter(X_train_2d_scaled[svm.support_, 0], 
                   X_train_2d_scaled[svm.support_, 1],
                   s=100, facecolors='none', edgecolors='black', linewidth=2)
    
    plt.xlabel('Liczba wielkich liter (scaled)')
    plt.ylabel('Liczba wykrzyknikÃ³w (scaled)')
    plt.title(f'SVM - {kernel.upper()} kernel')
    
    # DokÅ‚adnoÅ›Ä‡
    accuracy = accuracy_score(y_test, svm.predict(X_test_2d_scaled))
    plt.text(0.02, 0.98, f'Accuracy: {accuracy:.1%}', 
             transform=plt.gca().transAxes, 
             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
             verticalalignment='top')

plt.tight_layout()
plt.close()

# Przygotuj dane dla peÅ‚nego modelu
features = ['dlugosc_tematu', 'liczba_wielkich_liter', 'liczba_wykrzyknikow', 
           'liczba_linkow', 'slowa_promocyjne', 'dlugosc_wiadomosci', 'godzina_wyslania']
X_full = data[features]
X_train_full, X_test_full, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42)

scaler_full = StandardScaler()
X_train_full_scaled = scaler_full.fit_transform(X_train_full)
X_test_full_scaled = scaler_full.transform(X_test_full)

# PorÃ³wnaj wyniki
print("PORÃ“WNANIE KERNELI (2D vs 7D features):")
print("=" * 50)

for kernel in kernels:
    # 2D model
    if kernel == 'poly':
        svm_2d = SVC(kernel=kernel, degree=3, random_state=42)
    else:
        svm_2d = SVC(kernel=kernel, random_state=42)
    svm_2d.fit(X_train_2d_scaled, y_train)
    acc_2d = accuracy_score(y_test, svm_2d.predict(X_test_2d_scaled))
    
    # 7D model
    if kernel == 'poly':
        svm_7d = SVC(kernel=kernel, degree=3, random_state=42)
    else:
        svm_7d = SVC(kernel=kernel, random_state=42)
    svm_7d.fit(X_train_full_scaled, y_train)
    acc_7d = accuracy_score(y_test, svm_7d.predict(X_test_full_scaled))
    
    print(f"{kernel.upper():6} - 2D: {acc_2d:.1%}, 7D: {acc_7d:.1%}, Poprawa: +{(acc_7d-acc_2d)*100:.1f}pp")

# OdtwÃ³rz wizualizacjÄ™
plt.figure(figsize=(15, 5))

for i, kernel in enumerate(kernels):
    plt.subplot(1, 3, i+1)
    
    if kernel == 'poly':
        svm = SVC(kernel=kernel, degree=3, random_state=42)
    else:
        svm = SVC(kernel=kernel, random_state=42)
    
    svm.fit(X_train_2d_scaled, y_train)
    
    h = 0.02
    x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1
    y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    
    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)
    scatter = plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], 
                         c=y_train, cmap=plt.cm.RdYlBu, edgecolors='black')
    
    if hasattr(svm, 'support_'):
        plt.scatter(X_train_2d_scaled[svm.support_, 0], 
                   X_train_2d_scaled[svm.support_, 1],
                   s=100, facecolors='none', edgecolors='black', linewidth=2)
    
    plt.xlabel('Liczba wielkich liter (scaled)')
    plt.ylabel('Liczba wykrzyknikÃ³w (scaled)')
    plt.title(f'SVM - {kernel.upper()} kernel')
    
    accuracy = accuracy_score(y_test, svm.predict(X_test_2d_scaled))
    plt.text(0.02, 0.98, f'Accuracy: {accuracy:.1%}', 
             transform=plt.gca().transAxes, 
             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
             verticalalignment='top')

plt.tight_layout()
plt.show()
```

::: {.callout-note collapse="true"}
## PokaÅ¼ wizualizacjÄ™ granic i porÃ³wnanie

```{python}
#| echo: false
#| fig-cap: "Wizualizacja granic decyzyjnych SVM dla rÃ³Å¼nych kerneli"
#| fig-width: 15
#| fig-height: 5

print("PORÃ“WNANIE KERNELI (2D vs 7D features):")
print("=" * 50)

for kernel in kernels:
    # 2D model
    if kernel == 'poly':
        svm_2d = SVC(kernel=kernel, degree=3, random_state=42)
    else:
        svm_2d = SVC(kernel=kernel, random_state=42)
    svm_2d.fit(X_train_2d_scaled, y_train)
    acc_2d = accuracy_score(y_test, svm_2d.predict(X_test_2d_scaled))
    
    # 7D model
    if kernel == 'poly':
        svm_7d = SVC(kernel=kernel, degree=3, random_state=42)
    else:
        svm_7d = SVC(kernel=kernel, random_state=42)
    svm_7d.fit(X_train_full_scaled, y_train)
    acc_7d = accuracy_score(y_test, svm_7d.predict(X_test_full_scaled))
    
    print(f"{kernel.upper():6} - 2D: {acc_2d:.1%}, 7D: {acc_7d:.1%}, Poprawa: +{(acc_7d-acc_2d)*100:.1f}pp")

# OdtwÃ³rz wizualizacjÄ™
plt.figure(figsize=(15, 5))

for i, kernel in enumerate(kernels):
    plt.subplot(1, 3, i+1)
    
    if kernel == 'poly':
        svm = SVC(kernel=kernel, degree=3, random_state=42)
    else:
        svm = SVC(kernel=kernel, random_state=42)
    
    svm.fit(X_train_2d_scaled, y_train)
    
    h = 0.02
    x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1
    y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    
    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)
    scatter = plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], 
                         c=y_train, cmap=plt.cm.RdYlBu, edgecolors='black')
    
    if hasattr(svm, 'support_'):
        plt.scatter(X_train_2d_scaled[svm.support_, 0], 
                   X_train_2d_scaled[svm.support_, 1],
                   s=100, facecolors='none', edgecolors='black', linewidth=2)
    
    plt.xlabel('Liczba wielkich liter (scaled)')
    plt.ylabel('Liczba wykrzyknikÃ³w (scaled)')
    plt.title(f'SVM - {kernel.upper()} kernel')
    
    accuracy = accuracy_score(y_test, svm.predict(X_test_2d_scaled))
    plt.text(0.02, 0.98, f'Accuracy: {accuracy:.1%}', 
             transform=plt.gca().transAxes, 
             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
             verticalalignment='top')

plt.tight_layout()
plt.show()
```
:::

---

## ğŸ¯ Rodzaje kerneli i ich zastosowania

### 1) **Linear Kernel - proste granice**
```{python}
#| echo: true
#| output: false

# UÅ¼yj gdy:
# - Dane sÄ… liniowo separowalne
# - Masz duÅ¼o features (high-dimensional)
# - Potrzebujesz szybkiego modelu
# - Chcesz interpretowalnoÅ›Ä‡

linear_use_cases = [
    "Text classification (high-dimensional sparse data)",
    "Gene expression analysis", 
    "Document categorization",
    "Sentiment analysis z bag-of-words"
]

print("LINEAR KERNEL - najlepsze zastosowania:")
for i, use_case in enumerate(linear_use_cases, 1):
    print(f"{i}. {use_case}")

# Demo: Linear SVM na prostych danych
X_simple = np.random.randn(100, 2)
y_simple = (X_simple[:, 0] + X_simple[:, 1] > 0).astype(int)

linear_svm = SVC(kernel='linear')
linear_svm.fit(X_simple, y_simple)
print(f"\nLinear SVM na prostych danych: {linear_svm.score(X_simple, y_simple):.1%}")
```

::: {.callout-note collapse="true"}
## PokaÅ¼ zastosowania Linear Kernel

```{python}
#| echo: false

print("LINEAR KERNEL - najlepsze zastosowania:")
for i, use_case in enumerate(linear_use_cases, 1):
    print(f"{i}. {use_case}")

print(f"\nLinear SVM na prostych danych: {linear_svm.score(X_simple, y_simple):.1%}")
```
:::

### 2) **RBF Kernel - uniwersalny wybÃ³r**
```{python}
#| echo: true
#| output: false

# RBF (Radial Basis Function) - najczÄ™Å›ciej uÅ¼ywany
# MoÅ¼e aproksymowaÄ‡ dowolnÄ… granicÄ™ decyzyjnÄ…!

rbf_use_cases = [
    "Image classification",
    "Medical diagnosis", 
    "Fraud detection",
    "Customer segmentation",
    "General classification tasks"
]

print("RBF KERNEL - najlepsze zastosowania:")
for i, use_case in enumerate(rbf_use_cases, 1):
    print(f"{i}. {use_case}")

# Demo: wpÅ‚yw parametru gamma
gammas = [0.1, 1.0, 10.0]
print(f"\nWpÅ‚yw parametru gamma w RBF:")

for gamma in gammas:
    rbf_svm = SVC(kernel='rbf', gamma=gamma, random_state=42)
    rbf_svm.fit(X_train_full_scaled, y_train)
    accuracy = rbf_svm.score(X_test_full_scaled, y_test)
    print(f"Gamma {gamma:4.1f}: Accuracy = {accuracy:.1%}, Support Vectors = {rbf_svm.n_support_.sum()}")

print("\nğŸ’¡ Gamma wysoka = bardziej skomplikowane granice")
print("ğŸ’¡ Gamma niska = gÅ‚adsze granice")
```

::: {.callout-note collapse="true"}
## PokaÅ¼ zastosowania RBF Kernel

```{python}
#| echo: false

print("RBF KERNEL - najlepsze zastosowania:")
for i, use_case in enumerate(rbf_use_cases, 1):
    print(f"{i}. {use_case}")

print(f"\nWpÅ‚yw parametru gamma w RBF:")

for gamma in gammas:
    rbf_svm = SVC(kernel='rbf', gamma=gamma, random_state=42)
    rbf_svm.fit(X_train_full_scaled, y_train)
    accuracy = rbf_svm.score(X_test_full_scaled, y_test)
    print(f"Gamma {gamma:4.1f}: Accuracy = {accuracy:.1%}, Support Vectors = {rbf_svm.n_support_.sum()}")

print("\nğŸ’¡ Gamma wysoka = bardziej skomplikowane granice")
print("ğŸ’¡ Gamma niska = gÅ‚adsze granice")
```
:::

### 3) **Polynomial Kernel - dla zÅ‚oÅ¼onych wzorcÃ³w**
```{python}
#| echo: true
#| output: false

poly_use_cases = [
    "Computer vision (shape recognition)",
    "Bioinformatics (protein classification)",
    "Natural language processing",
    "Pattern recognition"
]

print("POLYNOMIAL KERNEL - najlepsze zastosowania:")
for i, use_case in enumerate(poly_use_cases, 1):
    print(f"{i}. {use_case}")

# Demo: wpÅ‚yw stopnia wielomianu
degrees = [2, 3, 4, 5]
print(f"\nWpÅ‚yw stopnia wielomianu:")

for degree in degrees:
    poly_svm = SVC(kernel='poly', degree=degree, random_state=42)
    poly_svm.fit(X_train_full_scaled, y_train)
    accuracy = poly_svm.score(X_test_full_scaled, y_test)
    print(f"Degree {degree}: Accuracy = {accuracy:.1%}")

print("\nâš ï¸ Uwaga: wyÅ¼szie stopnie = ryzyko overfittingu!")
```

::: {.callout-note collapse="true"}
## PokaÅ¼ zastosowania Polynomial Kernel

```{python}
#| echo: false

print("POLYNOMIAL KERNEL - najlepsze zastosowania:")
for i, use_case in enumerate(poly_use_cases, 1):
    print(f"{i}. {use_case}")

print(f"\nWpÅ‚yw stopnia wielomianu:")

for degree in degrees:
    poly_svm = SVC(kernel='poly', degree=degree, random_state=42)
    poly_svm.fit(X_train_full_scaled, y_train)
    accuracy = poly_svm.score(X_test_full_scaled, y_test)
    print(f"Degree {degree}: Accuracy = {accuracy:.1%}")

print("\nâš ï¸ Uwaga: wyÅ¼szie stopnie = ryzyko overfittingu!")
```
:::

---

## âš™ï¸ Tuning parametrÃ³w SVM

```{python}
#| echo: true
#| output: false

from sklearn.model_selection import GridSearchCV

# Grid Search dla RBF kernel
param_grid_rbf = {
    'C': [0.1, 1, 10, 100],           # regularization strength
    'gamma': [0.001, 0.01, 0.1, 1]   # kernel coefficient
}

print("Rozpoczynam Grid Search dla RBF SVM...")
grid_search = GridSearchCV(
    SVC(kernel='rbf', random_state=42),
    param_grid_rbf,
    cv=3,  # 3-fold CV dla szybkoÅ›ci
    scoring='accuracy',
    n_jobs=-1,
    verbose=0
)

grid_search.fit(X_train_full_scaled, y_train)

print("Najlepsze parametry RBF:")
print(grid_search.best_params_)
print(f"Najlepsza dokÅ‚adnoÅ›Ä‡ CV: {grid_search.best_score_:.1%}")

# Test na zbiorze testowym
best_svm = grid_search.best_estimator_
test_accuracy = best_svm.score(X_test_full_scaled, y_test)
print(f"DokÅ‚adnoÅ›Ä‡ na test set: {test_accuracy:.1%}")

# PorÃ³wnanie przed/po tuningu
baseline_svm = SVC(kernel='rbf', random_state=42)
baseline_svm.fit(X_train_full_scaled, y_train)
baseline_accuracy = baseline_svm.score(X_test_full_scaled, y_test)

print(f"\nPorÃ³wnanie:")
print(f"Baseline RBF:      {baseline_accuracy:.1%}")
print(f"Tuned RBF:         {test_accuracy:.1%}")
print(f"Poprawa:           +{(test_accuracy - baseline_accuracy)*100:.1f}pp")
```

::: {.callout-note collapse="true"}
## PokaÅ¼ wyniki tuningu

```{python}
#| echo: false

print("Najlepsze parametry RBF:")
print(grid_search.best_params_)
print(f"Najlepsza dokÅ‚adnoÅ›Ä‡ CV: {grid_search.best_score_:.1%}")
print(f"DokÅ‚adnoÅ›Ä‡ na test set: {test_accuracy:.1%}")

print(f"\nPorÃ³wnanie:")
print(f"Baseline RBF:      {baseline_accuracy:.1%}")
print(f"Tuned RBF:         {test_accuracy:.1%}")
print(f"Poprawa:           +{(test_accuracy - baseline_accuracy)*100:.1f}pp")
```
:::

---

## âš ï¸ PuÅ‚apki i rozwiÄ…zania

### 1) **Brak standaryzacji**
```{python}
#| echo: true
#| output: false

# Problem: SVM jest bardzo wraÅ¼liwy na skale danych
print("DEMONSTRACJA: wpÅ‚yw standaryzacji")

# Bez standaryzacji
svm_unscaled = SVC(kernel='rbf', random_state=42)
svm_unscaled.fit(X_train_full, y_train)  # raw data!
acc_unscaled = svm_unscaled.score(X_test_full, y_test)

# Ze standaryzacjÄ…
svm_scaled = SVC(kernel='rbf', random_state=42)
svm_scaled.fit(X_train_full_scaled, y_train)  # scaled data!
acc_scaled = svm_scaled.score(X_test_full_scaled, y_test)

print(f"Bez standaryzacji: {acc_unscaled:.1%}")
print(f"Ze standaryzacjÄ…:  {acc_scaled:.1%}")
print(f"Poprawa:           +{(acc_scaled - acc_unscaled)*100:.1f}pp")
print("\nâœ… ZAWSZE standaryzuj dane przed SVM!")
```

::: {.callout-note collapse="true"}
## PokaÅ¼ wpÅ‚yw standaryzacji

```{python}
#| echo: false

print(f"Bez standaryzacji: {acc_unscaled:.1%}")
print(f"Ze standaryzacjÄ…:  {acc_scaled:.1%}")
print(f"Poprawa:           +{(acc_scaled - acc_unscaled)*100:.1f}pp")
print("\nâœ… ZAWSZE standaryzuj dane przed SVM!")
```
:::

### 2) **Niezbalansowane klasy**
```{python}
#| echo: true
#| output: false

# Problem: gdy jedna klasa jest bardzo rzadka
print("PROBLEM: niezbalansowane klasy")

# SprawdÅº balans w naszych danych
print(f"Balans klas: Spam {y_train.mean():.1%}, Ham {(1-y_train.mean()):.1%}")

# RozwiÄ…zanie 1: class_weight='balanced'
svm_balanced = SVC(kernel='rbf', class_weight='balanced', random_state=42)
svm_balanced.fit(X_train_full_scaled, y_train)

# RozwiÄ…zanie 2: manual class weights
spam_weight = len(y_train) / (2 * y_train.sum())  # 1/frequency
ham_weight = len(y_train) / (2 * (len(y_train) - y_train.sum()))
class_weights = {0: ham_weight, 1: spam_weight}

svm_manual = SVC(kernel='rbf', class_weight=class_weights, random_state=42)
svm_manual.fit(X_train_full_scaled, y_train)

# PorÃ³wnanie
svm_normal = SVC(kernel='rbf', random_state=42)
svm_normal.fit(X_train_full_scaled, y_train)

models = {
    'Normal': svm_normal,
    'Balanced': svm_balanced, 
    'Manual weights': svm_manual
}

print("\nPorÃ³wnanie podejÅ›Ä‡:")
for name, model in models.items():
    pred = model.predict(X_test_full_scaled)
    accuracy = accuracy_score(y_test, pred)
    
    # SprawdÅº recall dla spam (waÅ¼ne!)
    spam_indices = y_test == 1
    spam_recall = np.mean(pred[spam_indices] == 1) if spam_indices.sum() > 0 else 0
    
    print(f"{name:12}: Accuracy={accuracy:.1%}, Spam Recall={spam_recall:.1%}")
```

::: {.callout-note collapse="true"}
## PokaÅ¼ rozwiÄ…zania dla niezbalansowanych klas

```{python}
#| echo: false

print(f"Balans klas: Spam {y_train.mean():.1%}, Ham {(1-y_train.mean()):.1%}")

print("\nPorÃ³wnanie podejÅ›Ä‡:")
for name, model in models.items():
    pred = model.predict(X_test_full_scaled)
    accuracy = accuracy_score(y_test, pred)
    
    # SprawdÅº recall dla spam (waÅ¼ne!)
    spam_indices = y_test == 1
    spam_recall = np.mean(pred[spam_indices] == 1) if spam_indices.sum() > 0 else 0
    
    print(f"{name:12}: Accuracy={accuracy:.1%}, Spam Recall={spam_recall:.1%}")
```
:::

### 3) **Overfitting z wysokim C**
```{python}
#| echo: true
#| output: false

# Problem: zbyt wysoka wartoÅ›Ä‡ C prowadzi do overfittingu
C_values = [0.01, 0.1, 1, 10, 100, 1000]
results = []

print("WpÅ‚yw parametru C (regularization):")
print("C      | Train Acc | Test Acc | Difference | Support Vectors")
print("-" * 60)

for C in C_values:
    svm = SVC(kernel='rbf', C=C, random_state=42)
    svm.fit(X_train_full_scaled, y_train)
    
    train_acc = svm.score(X_train_full_scaled, y_train)
    test_acc = svm.score(X_test_full_scaled, y_test)
    diff = train_acc - test_acc
    n_sv = svm.n_support_.sum()
    
    print(f"{C:6.2f} | {train_acc:8.1%} | {test_acc:7.1%} | {diff:9.1%} | {n_sv:14d}")
    results.append((C, train_acc, test_acc, diff, n_sv))

print("\nğŸ’¡ Optimalne C: wysokie test accuracy, maÅ‚a rÃ³Å¼nica train-test")
print("ğŸ’¡ Zbyt wysokie C: perfect train accuracy, sÅ‚abe test accuracy")
```

::: {.callout-note collapse="true"}
## PokaÅ¼ wpÅ‚yw parametru C

```{python}
#| echo: false

print("WpÅ‚yw parametru C (regularization):")
print("C      | Train Acc | Test Acc | Difference | Support Vectors")
print("-" * 60)

for C, train_acc, test_acc, diff, n_sv in results:
    print(f"{C:6.2f} | {train_acc:8.1%} | {test_acc:7.1%} | {diff:9.1%} | {n_sv:14d}")

print("\nğŸ’¡ Optimalne C: wysokie test accuracy, maÅ‚a rÃ³Å¼nica train-test")
print("ğŸ’¡ Zbyt wysokie C: perfect train accuracy, sÅ‚abe test accuracy")
```
:::

---

## ğŸŒ Real-world przypadki uÅ¼ycia

1. **Text Classification:** Spam detection, sentiment analysis, document categorization
2. **Computer Vision:** Image classification, face recognition, medical image analysis
3. **Finance:** Credit scoring, algorithmic trading, fraud detection
4. **Healthcare:** Disease diagnosis, drug discovery, medical image analysis
5. **Cybersecurity:** Intrusion detection, malware classification, network security

::: {.callout-tip}
## ğŸ’¡ Kiedy uÅ¼ywaÄ‡ SVM?

**âœ… UÅ»YJ GDY:**

- Potrzebujesz wysokiej dokÅ‚adnoÅ›ci klasyfikacji
- Masz Å›redniej wielkoÅ›ci dataset (1K-100K samples)
- Dane majÄ… clear margins miÄ™dzy klasami
- Nie potrzebujesz probabilistic outputs
- Memory efficiency jest waÅ¼na

**âŒ NIE UÅ»YWAJ GDY:**

- Masz bardzo duÅ¼e datasety (>100K samples - uÅ¼yj Neural Networks)
- Potrzebujesz probabilistic predictions (uÅ¼yj Logistic Regression)
- Dane majÄ… duÅ¼o noise'u (uÅ¼yj Random Forest)
- Target jest continuous (uÅ¼yj SVR - Support Vector Regression)
- Potrzebujesz bardzo szybkiego inference (uÅ¼yj Decision Tree)
:::

**Podsumowanie:** SVM to potÄ™Å¼ny algoritm do klasyfikacji z optymalnÄ… separacjÄ… klas. Wybierz kernel w zaleÅ¼noÅ›ci od zÅ‚oÅ¼onoÅ›ci danych: Linear dla prostych, RBF dla uniwersalnych, Polynomial dla specjalnych przypadkÃ³w.

**NastÄ™pna Å›ciÄ…gawka:** Logistic Regression - klasyfikacja probabilistyczna! ğŸ¯
